{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas.core.api import Series as Series\n",
    "import matplotlib.pyplot as plt\n",
    "from pytrends.request import TrendReq\n",
    "import yfinance as yf\n",
    "import requests\n",
    "import pytz\n",
    "import datetime\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import warnings as wrn\n",
    "import os\n",
    "from enum import Enum\n",
    "from typing import Tuple\n",
    "import itertools\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general_stocks = ['KO', 'PFE', 'WMT', 'PG', 'JNJ', 'DIS', 'PEP', 'MCD', 'T', 'VZ']\n",
    "# tech_stocks = ['AAPL', 'AMZN', 'MSFT', 'GOOGL', 'NVDA', 'TSLA', 'META', 'INTC', 'IBM', 'AMD']\n",
    "# finance_stocks = ['GS', 'BAC', 'WFC', 'USB', 'JPM', 'MA', 'V', 'AXP', 'C', 'BLK']\n",
    "decentralized_currencies = ['BTC', 'ETH', 'ADA', 'SOL', 'XRP', 'XMR', 'LTC', 'DOT', 'LINK', 'XTZ', 'DOGE']\n",
    "\n",
    "# general_stocks_names = ['Coca-Cola', 'Pfizer', 'Walmart', 'Procter & Gamble', 'Johnson & Johnson', 'Disney', 'Pepsi', 'McDonalds', 'AT&T', 'Verizon']\n",
    "# tech_stocks_names = ['Apple', 'Amazon', 'Microsoft', 'Google', 'Nvidia', 'Tesla', 'Meta', 'Intel', 'IBM', 'AMD']\n",
    "# finance_stocks_names = ['Goldman Sachs', 'Bank of America', 'Wells Fargo', 'US Bancorp', 'JPMorgan Chase', 'Mastercard', 'Visa', 'American Express', 'Citigroup', 'BlackRock']\n",
    "decentralized_currencies_names = ['Bitcoin', 'Ethereum', 'Cardano', 'Solana', 'Ripple', 'Monero', 'Litecoin', 'Polkadot', 'Chainlink', 'Tezos', 'Dogecoin']\n",
    "\n",
    "# color_map = {\n",
    "#     'general': 'deepskyblue',\n",
    "#     'tech': 'limegreen',\n",
    "#     'finance': 'darkorchid',\n",
    "#     'crypto': 'red'\n",
    "# }\n",
    "\n",
    "start = '2019-06-30'\n",
    "end = '2024-07-01'\n",
    "max_lags = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and Classes\n",
    "## get_trends_data\n",
    "gets the trend data using pytrends, given a certain timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trends_data(keyword, \n",
    "                    timeframe=datetime.date.today().strftime('%Y-%m-%d') + ' ' + (datetime.date.today() - datetime.timedelta(days = 269)).strftime('%Y-%m-%d'),\n",
    "                    retries=5, backoff_factor=1.0, verbose=True):\n",
    "    pytrends = TrendReq(hl='en-US', tz=360, timeout=(10,25), )\n",
    "    pytrends.build_payload(keyword, cat = 0, timeframe = timeframe, geo='')\n",
    "    \n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            df = pytrends.interest_over_time()\n",
    "            if df is not None and not df.empty:\n",
    "                if verbose:\n",
    "                    print(f\"Trend Data for {keyword[0]} at timeframe {timeframe} retrieved successfully.\")\n",
    "                df.reset_index(inplace = True)\n",
    "                df.rename(columns = {'date': 'Date', keyword[0]: 'Trend'}, inplace = True)\n",
    "                df['Date'] = pd.to_datetime(df['Date'].dt.strftime('%m/%d/%Y'))\n",
    "                return df\n",
    "            else:\n",
    "                print(\"No data retrieved or DataFrame is empty.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            if \"429\" in str(e):\n",
    "                sleep_time = backoff_factor * (2 ** i)\n",
    "                if verbose:\n",
    "                    print(f\"Rate limit exceeded. Retrying in {sleep_time} seconds...\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                raise(f\"An error occurred: {e}\")\n",
    "    print(\"Failed to retrieve data after several retries.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_stock_data\n",
    "gets the prices of a certain stock in a certain timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from binance\n",
    "def make_api_call(base_url, endpoint=\"\", method=\"GET\", **kwargs):\n",
    "    # Construct the full URL\n",
    "    full_url = f'{base_url}{endpoint}'\n",
    "\n",
    "    # Make the API call\n",
    "    response = requests.request(method=method, url=full_url, **kwargs)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        return response\n",
    "    else:\n",
    "        # If the request was not successful, raise an exception with the error message\n",
    "        raise Exception(f'API request failed with status code {response.status_code}: {response.text}')\n",
    "\n",
    "def get_binance_historical_data(symbol, interval= '1d', start_date='2019-06-30', end_date='2024-07-01'):\n",
    "    \n",
    "    # define basic parameters for call\n",
    "    base_url = 'https://fapi.binance.com'\n",
    "    endpoint = '/fapi/v1/klines'\n",
    "    method = 'GET'\n",
    "    \n",
    "    # Set the start time parameter in the params dictionary\n",
    "    params = {\n",
    "        'symbol': symbol,\n",
    "        'interval': interval,\n",
    "        'limit': 1500,\n",
    "        'startTime': int(datetime.datetime.strptime(start_date, \"%Y-%m-%d\").timestamp() * 1000), # Start time in milliseconds,\n",
    "        'endTime': int(datetime.datetime.strptime(end_date, \"%Y-%m-%d\").timestamp() * 1000) # End time in milliseconds\n",
    "    }\n",
    "\n",
    "    # Make initial API call to get candles\n",
    "    response = make_api_call(base_url, endpoint=endpoint, method=method, params=params)\n",
    "\n",
    "    candles_data = []\n",
    "\n",
    "    while len(response.json()) > 0:\n",
    "        # Append the received candles to the list\n",
    "        candles_data.extend(response.json())\n",
    "\n",
    "        # Update the start time for the next API call\n",
    "        params['startTime'] = candles_data[-1][0] + 1 # last candle open_time + 1ms\n",
    "\n",
    "        try:\n",
    "            # Make the next API call\n",
    "            response = make_api_call(base_url, endpoint=endpoint, method=method, params=params)\n",
    "        except Exception as e:\n",
    "            raise Exception(f'{symbol} - {e}')\n",
    "\n",
    "    \n",
    "    # Wrap the candles data as a pandas DataFrame\n",
    "    columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'close_time', 'quote_asset_volume',\n",
    "               'number_of_trades', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore']\n",
    "    dtype={\n",
    "    'Date': 'datetime64[ms, Asia/Jerusalem]',\n",
    "    'Open': 'float64',\n",
    "    'High': 'float64',\n",
    "    'Low': 'float64',\n",
    "    'Close': 'float64',\n",
    "    'Volume': 'float64',\n",
    "    'close_time': 'datetime64[ms, Asia/Jerusalem]',\n",
    "    'quote_asset_volume': 'float64',\n",
    "    'number_of_trades': 'int64',\n",
    "    'taker_buy_base_asset_volume': 'float64',\n",
    "    'taker_buy_quote_asset_volume': 'float64',\n",
    "    'ignore': 'float64'\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(candles_data, columns=columns)\n",
    "    df = df.astype(dtype)\n",
    "\n",
    "    df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    return df.drop(columns=['close_time', 'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from yfinance\n",
    "def get_stock_data(ticker, start, end, verbose = True):\n",
    "    currTicker = yf.Ticker(ticker)\n",
    "    tickerDF = currTicker.history(repair = True, start = start, end = end, auto_adjust = False).drop(columns = ['Dividends', 'Stock Splits', 'Repaired?']).reset_index()\n",
    "    if verbose:\n",
    "        print(f\"Stock Data for {ticker} retrieved successfully.\")\n",
    "    tickerDF['Date'] = pd.to_datetime(tickerDF['Date'].dt.strftime('%m/%d/%Y'))\n",
    "    return tickerDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trend_corr\n",
    "gets the data of the trends and prices of the stock given to it in a certain timeframe and calculates the correlation between the log_returns and the trends delayed by certain delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_corr(stock, days = 60, start = '2023-10-01', end = '2024-06-01', delay = 7):\n",
    "    if not os.path.exists(f\"./Data/{stock}_trends({start} - {end}).csv\"):\n",
    "        if stock in general_stocks:\n",
    "            name = general_stocks_names[general_stocks.index(stock)]\n",
    "        elif stock in tech_stocks:\n",
    "            name = tech_stocks_names[tech_stocks.index(stock)]\n",
    "        elif stock in finance_stocks:\n",
    "            name = finance_stocks_names[finance_stocks.index(stock)]\n",
    "        else:\n",
    "            name = decentralized_currencies_names[decentralized_currencies.index(stock)]\n",
    "        t = get_trends_data([name], timeframe = f\"{start} {end}\")\n",
    "        if t is None:\n",
    "            raise Exception(f'Failed to retrieve Trend Data of {stock}.')\n",
    "        t.to_csv(f\"./Data/{stock}_trends({start} - {end}).csv\")\n",
    "    else:\n",
    "        t = pd.read_csv(f\"./Data/{stock}_trends({start} - {end}).csv\")\n",
    "    if not os.path.exists(f\"./Data/{stock}_Prices({start} - {end}).csv\"):\n",
    "        if stock in decentralized_currencies:\n",
    "            p = get_stock_data(f'{stock}-USD', start = start, end = end)\n",
    "        else:\n",
    "            p = get_stock_data(stock, start = start, end = end)\n",
    "        p.to_csv(f\"./Data/{stock}_Prices({start} - {end}).csv\")\n",
    "    else:\n",
    "        p = pd.read_csv(f\"./Data/{stock}_Prices({start} - {end}).csv\")\n",
    "\n",
    "    t['Date'] = pd.to_datetime(t['Date'])\n",
    "    p['Date'] = pd.to_datetime(p['Date'])\n",
    "\n",
    "    full_data = pd.merge(p, t, on='Date')\n",
    "\n",
    "    full_data['log_returns'] = np.log(full_data.Close / full_data.Close.shift(1))\n",
    "    full_data['Volatility'] = full_data['log_returns'].rolling(window=days).std() * np.sqrt(days)\n",
    "\n",
    "    for i in range(1, 8):\n",
    "        full_data[f'Delay_{i}'] = full_data['Trend'].shift(i)\n",
    "\n",
    "    rho = full_data.corr()\n",
    "    rho_c = rho['Close'][f'Delay_{delay}']\n",
    "    return rho_c, full_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot_stock_data\n",
    "gets the data of the trends and the prices of the stock given to it in a certain timeframe and plots its close and its delayed trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stock_data(stock, days = 60, start = '2023-10-01', end = '2024-06-01', delay = 7, download = False):\n",
    "    if not os.path.exists(f\"./Data/{stock}_trends({start} - {end}).csv\"):\n",
    "        if stock in general_stocks:\n",
    "            name = general_stocks_names[general_stocks.index(stock)]\n",
    "        elif stock in tech_stocks:\n",
    "            name = tech_stocks_names[tech_stocks.index(stock)]\n",
    "        elif stock in finance_stocks:\n",
    "            name = finance_stocks_names[finance_stocks.index(stock)]\n",
    "        else:\n",
    "            name = decentralized_currencies_names[decentralized_currencies.index(stock)]\n",
    "        t = get_trends_data([name], timeframe = f\"{start} {end}\")\n",
    "        t.to_csv(f\"./Data/{stock}_trends({start} - {end}).csv\")\n",
    "    else:\n",
    "        t = pd.read_csv(f\"./Data/{stock}_trends({start} - {end}).csv\")\n",
    "    if not os.path.exists(f\"./Data/{stock}_Prices({start} - {end}).csv\"):\n",
    "        p = get_stock_data(stock, start = start, end = end)\n",
    "        p.to_csv(f\"./Data/{stock}_Prices({start} - {end}).csv\")\n",
    "    else:\n",
    "        p = pd.read_csv(f\"./Data/{stock}_Prices({start} - {end}).csv\")\n",
    "\n",
    "    t['Date'] = pd.to_datetime(t['Date'])\n",
    "    p['Date'] = pd.to_datetime(p['Date'])\n",
    "\n",
    "    full_data = pd.merge(p, t, on='Date')\n",
    "\n",
    "    full_data['log_returns'] = np.log(full_data.Close / full_data.Close.shift(1))\n",
    "    full_data['Volatility'] = full_data['log_returns'].rolling(window=days).std() * np.sqrt(days)\n",
    "\n",
    "    full_data[f'Delay_{delay}'] = full_data.Trend.shift(7)\n",
    "\n",
    "    # Determine the color based on the stock category\n",
    "    if stock in general_stocks:\n",
    "        color = color_map['general']\n",
    "        name = general_stocks_names[general_stocks.index(stock)]\n",
    "    elif stock in tech_stocks:\n",
    "        color = color_map['tech']\n",
    "        name = tech_stocks_names[tech_stocks.index(stock)]\n",
    "    elif stock in finance_stocks:\n",
    "        color = color_map['finance']\n",
    "        name = finance_stocks_names[finance_stocks.index(stock)]\n",
    "    else:\n",
    "        color = color_map['crypto']\n",
    "        name = decentralized_currencies_names[decentralized_currencies.index(stock)]\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "    # Plot Close price\n",
    "    axes[0].plot(full_data['Date'], full_data['Close'], label = 'Close Price', color = color)\n",
    "    axes[0].set_xlabel('Date')\n",
    "    axes[0].set_ylabel('Close Price')\n",
    "    axes[0].set_title(f'{stock}: Close Price')\n",
    "    legend = axes[0].legend(loc='upper left')\n",
    "    legend.get_frame().set_alpha(0.3)\n",
    "\n",
    "    # Plot 7-days delay trend\n",
    "    axes[1].plot(full_data['Date'], full_data[f'Delay_{delay}'], label = f'{delay}-Days Delayed Trend', color = 'black')\n",
    "    axes[1].set_xlabel('Date')\n",
    "    axes[1].set_ylabel(f'{delay}-Days Delayed Trend')\n",
    "    axes[1].set_title(f'{stock}: {delay}-Days Delayed Trend')\n",
    "    legend = axes[1].legend(loc='upper right')\n",
    "    legend.get_frame().set_alpha(0.3)\n",
    "\n",
    "    fig.suptitle(f'{name} ({stock})', fontsize=20, verticalalignment = 'bottom', fontweight = 'bold')\n",
    "    plt.tight_layout(pad=2.0)\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    if download:\n",
    "        plt.savefig(f\"./Plots/{stock}_plot({start} - {end}).png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time_jump\n",
    "adds time in days to a given date that was accepted as string, returns as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the date in string format after a certain number of days\n",
    "def time_jump(start, days = 7 * 38):\n",
    "    return (datetime.datetime.strptime(start, '%Y-%m-%d') + datetime.timedelta(days = days)).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_breakpoints\n",
    "calculates the breakpoints needed for the multiple requests of data to make the data as long as possible for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_breakpoints(start, end, days = 7 * 38):\n",
    "    breakpoints = [start]\n",
    "    while datetime.datetime.strptime(breakpoints[-1], '%Y-%m-%d') < datetime.datetime.strptime(end, '%Y-%m-%d'):\n",
    "        temp = time_jump(breakpoints[-1], days)\n",
    "        if datetime.datetime.strptime(temp, '%Y-%m-%d') < datetime.datetime.strptime(end, '%Y-%m-%d'):\n",
    "            breakpoints.append(temp)\n",
    "        else:\n",
    "            breakpoints.append(end)\n",
    "    return breakpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## connectNnormalizeTrends\n",
    "gets the data of the trends and prices daily in the whole time frame and in parts, and connects them by estimating an eproximation of their absolute amount of searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connectNnormalizeTrends(Dfs, stock):\n",
    "    # setting up the data for step 1\n",
    "    if not os.path.exists(f'./Data/glimpse_{stock}_5Y.csv'):\n",
    "        raise Exception(f'Failed to retrieve Glimpse Data of {stock}.')\n",
    "    glmpsDf = pd.read_csv(f'./Data/glimpse_{stock}_5Y.csv')\n",
    "    glmpsDf.rename(columns={'Time (week of)': 'Date', 'Absolute Google Search Volume': 'Absolute_Volume'}, inplace=True)\n",
    "    glmpsDf['Date'] = pd.to_datetime(glmpsDf['Date'])\n",
    "\n",
    "    df_concat = pd.concat(Dfs).reset_index(drop = True)\n",
    "    df_concat['Date'] = pd.to_datetime(df_concat['Date'])\n",
    "\n",
    "    # calculating the mean trend for each week and merging it into glmpsDf\n",
    "    df_concat['MeanTrend'] = df_concat['Trend'].rolling(window=7, min_periods=1).mean().shift(-6)\n",
    "    glmpsDf = pd.merge(glmpsDf, df_concat, on='Date', how='left').drop(columns = ['Trend'])\n",
    "\n",
    "    # setting up the data for step 2\n",
    "    glmpsDf.rename(columns={'Date': 'Date_Week'}, inplace=True)\n",
    "    df_concat = df_concat.drop(columns = ['MeanTrend'])\n",
    "    df_concat['Date_Week'] = (df_concat['Date'] - pd.to_timedelta((df_concat['Date'].dt.weekday + 1) % 7, unit='d')).dt.strftime('%Y-%m-%d')\n",
    "    df_concat['Date_Week'] = pd.to_datetime(df_concat['Date_Week'])\n",
    "\n",
    "    # calculating the ratio and search volume for each week\n",
    "    df_concat = pd.merge(df_concat, glmpsDf[['Date_Week', 'MeanTrend', 'Absolute_Volume']], on='Date_Week', how='left')\n",
    "    df_concat['Ratio'] = df_concat['Trend'] / (df_concat['MeanTrend'] * 7)\n",
    "    df_concat['Search_Volume'] = df_concat['Ratio'] * df_concat['Absolute_Volume']\n",
    "\n",
    "    # adding to df_concat the check ratio to check validity of the data\n",
    "    df_concat['check_ratio'] = df_concat['Search_Volume'] / df_concat['Trend']\n",
    "\n",
    "    # renormalizing the data\n",
    "    df_concat['Normalized_Searches'] = ((df_concat['Search_Volume'] - df_concat['Search_Volume'].min()) / (df_concat['Search_Volume'].max() - df_concat['Search_Volume'].min())) * 100\n",
    "\n",
    "    # cleaning out unnecessary columns\n",
    "    df_concat = df_concat.drop(columns = ['Trend', 'Date_Week', 'MeanTrend', 'Absolute_Volume', 'Ratio'])\n",
    "\n",
    "    df_concat['Date'] = df_concat['Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getNormalizedData\n",
    "gets the data normalized, and performs validity checks (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNormalizedData(stock, ticker, start = '2019-06-23', end = '2024-06-01', weeks = 38, do_double = True, verbose = False):\n",
    "    if weeks > 38:\n",
    "        wrn.warn('The maximum number of weeks is 38. \\nThe number of weeks will be set to 38.', category=Warning)\n",
    "        weeks = 38\n",
    "    if do_double & (weeks % 2 != 0):\n",
    "        wrn.warn('The number of weeks must be even to use the double method. \\nThe number of weeks will be rounded down to the nearest even number.', category=Warning)\n",
    "        weeks -= 1\n",
    "    breakpoints = get_breakpoints(start, end, days = 7 * weeks)\n",
    "    if do_double:\n",
    "        breakpoints2 = get_breakpoints(start, end, days = 7 * weeks / 2)\n",
    "        breakpoints2 = [x for x in breakpoints2 if x not in breakpoints[1:-1]]\n",
    "    \n",
    "    # initializing the list to store the dataframes\n",
    "    Dfs = []\n",
    "\n",
    "    # extracting the data for each time period\n",
    "    for i in range(len(breakpoints) - 1):\n",
    "        startTemp = breakpoints[i]\n",
    "        endTemp = time_jump(breakpoints[i + 1], days=-1)\n",
    "        # checking if the data is already extracted\n",
    "        if not os.path.exists(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\"):\n",
    "            # extracting the data\n",
    "            t = get_trends_data([stock], timeframe = f\"{startTemp} {endTemp}\", verbose = verbose).drop(columns = ['isPartial'])\n",
    "            if t is None:\n",
    "                raise Exception(f'Failed to retrieve Trend Data of {stock}.')\n",
    "            t.to_csv(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\")\n",
    "        else:\n",
    "            t = pd.read_csv(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\").drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "        Dfs.append(t)\n",
    "\n",
    "    df_concat = connectNnormalizeTrends(Dfs, stock)\n",
    "\n",
    "    if do_double:\n",
    "        # initializing the list to store the dataframes\n",
    "        Dfs2 = []\n",
    "\n",
    "        # extracting the data for each time period\n",
    "        for i in range(len(breakpoints2) - 1):\n",
    "            startTemp = breakpoints2[i]\n",
    "            endTemp = time_jump(breakpoints2[i + 1], days=-1)\n",
    "            # checking if the data is already extracted\n",
    "            if not os.path.exists(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\"):\n",
    "                # extracting the data\n",
    "                t = get_trends_data([stock], timeframe = f\"{startTemp} {endTemp}\", verbose = verbose).drop(columns = ['isPartial'])\n",
    "                if t is None:\n",
    "                    raise Exception(f'Failed to retrieve Trend Data of {stock}.')\n",
    "                t.to_csv(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\")\n",
    "            else:\n",
    "                t = pd.read_csv(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\").drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "            Dfs2.append(t)\n",
    "        \n",
    "        df_concat2 = connectNnormalizeTrends(Dfs2, stock)\n",
    "    \n",
    "        df_concat[\"Normalized_Searches\"] = (df_concat[\"Normalized_Searches\"] + df_concat2[\"Normalized_Searches\"]) / 2\n",
    "    \n",
    "    if not os.path.exists(f\"./Data/{stock}_Prices({start}-{end}).csv\"):\n",
    "        stockData = get_binance_historical_data(f'{ticker}USDT', start_date=start, end_date=end)\n",
    "        stockData.to_csv(f\"./Data/{stock}_Prices({start}-{end}).csv\")\n",
    "    else:\n",
    "        stockData = pd.read_csv(f\"./Data/{stock}_Prices({start}-{end}).csv\").drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "    df_concat[df_concat.Normalized_Searches == 0] = 0.1\n",
    "\n",
    "    df_concat['log_searches'] = np.log(df_concat.Normalized_Searches / df_concat.Normalized_Searches.shift(1))\n",
    "    stockData['log_returns'] = np.log(stockData.Close / stockData.Close.shift(1))\n",
    "\n",
    "    try:\n",
    "        df_concat['Date'] = df_concat['Date'].dt.strftime('%Y-%m-%d')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        stockData['Date'] = stockData['Date'].dt.strftime('%Y-%m-%d')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    finalDf = pd.merge(stockData, df_concat, on='Date', how='left').dropna()\n",
    "\n",
    "    if verbose:\n",
    "        # data normalization validity check\n",
    "        top = 0\n",
    "        bottom = 0\n",
    "        for i in range(len(Dfs)):\n",
    "            top += Dfs[i].shape[0]\n",
    "            print(f'period {i + 1}:\\n=========\\nmean: {df_concat.iloc[bottom:top]['check_ratio'].mean():.4f}\\nsd: {df_concat.iloc[bottom:top][\"check_ratio\"].std():.4f}\\n\\nmean/sd: {df_concat.iloc[bottom:top]['check_ratio'].mean()/df_concat.iloc[bottom:top][\"check_ratio\"].std():.4f}\\n')\n",
    "            bottom += Dfs[i].shape[0]\n",
    "\n",
    "        if do_double:\n",
    "            top = 0\n",
    "            bottom = 0\n",
    "            for i in range(len(Dfs2)):\n",
    "                top += Dfs2[i].shape[0]\n",
    "                print(f'period {i + len(Dfs) + 1}:\\n=========\\nmean: {df_concat.iloc[bottom:top]['check_ratio'].mean():.4f}\\nsd: {df_concat.iloc[bottom:top][\"check_ratio\"].std():.4f}\\n\\nmean/sd: {df_concat.iloc[bottom:top]['check_ratio'].mean()/df_concat.iloc[bottom:top][\"check_ratio\"].std():.4f}\\n')\n",
    "                bottom += Dfs2[i].shape[0]\n",
    "    return finalDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate_strategy\n",
    "prints and returns the evaluation of the strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_total_return(portfolio_values: pd.Series):\n",
    "    return (portfolio_values.iloc[-1] / portfolio_values.iloc[0]) - 1.0\n",
    "\n",
    "def calc_annualized_return(portfolio_values):\n",
    "    portfolio_trading_years = portfolio_values.shape[0] / 365\n",
    "    return (portfolio_values.iloc[-1] / portfolio_values.iloc[0])**(1/portfolio_trading_years) - 1.0\n",
    "\n",
    "def calc_annualized_sharpe(portfolio_values: pd.Series, rf: float=0.0): # rf = risk-free rate\n",
    "    annualized_return = calc_annualized_return(portfolio_values)\n",
    "    annualized_std = portfolio_values.pct_change().std() * np.sqrt(365)\n",
    "    if annualized_std is None or annualized_std == 0:\n",
    "        return 0\n",
    "    return (annualized_return - rf) / annualized_std\n",
    "\n",
    "def calc_downside_deviation(portfolio_values):\n",
    "    porfolio_returns = portfolio_values.pct_change().dropna()\n",
    "    return porfolio_returns[porfolio_returns < 0].std()\n",
    "\n",
    "def calc_sortino(portfolio_values, rf=0.0): # rf = risk-free rate\n",
    "    down_deviation = calc_downside_deviation(portfolio_values) * np.sqrt(365)\n",
    "    annualized_return = calc_annualized_return(portfolio_values)\n",
    "    if down_deviation is None or down_deviation == 0:\n",
    "        return 0\n",
    "    return (annualized_return - rf) / down_deviation\n",
    "\n",
    "def calc_max_drawdown(portfolio_values):\n",
    "    cumulative_max = portfolio_values.cummax()\n",
    "    drawdown = (cumulative_max - portfolio_values) / cumulative_max\n",
    "    return drawdown.max()\n",
    "\n",
    "def calc_calmar(portfolio_values):\n",
    "    max_drawdown = calc_max_drawdown(portfolio_values)\n",
    "    annualized_return = calc_annualized_return(portfolio_values)\n",
    "    return annualized_return / max_drawdown\n",
    "\n",
    "def evaluate_strategy(df, strat_name, rf: float=0.0, returns = True, verbose = True):\n",
    "    \n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    first_date = df['Date'].iloc[0]\n",
    "    df_rest = df[df['Date'] != first_date].groupby('Date').last().reset_index()\n",
    "\n",
    "    df = pd.concat([df[df['Date'] == first_date].head(1), df_rest]).reset_index(drop = True)\n",
    "    df = df.sort_values(by = 'Date').reset_index(drop = True)\n",
    "\n",
    "    total_return = calc_total_return(df['portfolio_value'])\n",
    "    annualized_return = calc_annualized_return(df['portfolio_value'])\n",
    "    annualized_sharpe = calc_annualized_sharpe(df['portfolio_value'], rf=rf)\n",
    "    sortino_ratio = calc_sortino(df['portfolio_value'], rf=rf)\n",
    "    max_drawdown = calc_max_drawdown(df['portfolio_value'])\n",
    "    calmar_ratio = calc_calmar(df['portfolio_value'])\n",
    "\n",
    "    if verbose:    \n",
    "        trString = f\"{total_return:.2%}\"\n",
    "        asString = f\"{annualized_return:.2f}\"\n",
    "        mdString = f\"{max_drawdown:.2%}\"\n",
    "        print(f\"Results for {strat_name}:\")\n",
    "        print(f\"  Total Return:            {trString:8s}  Annualized Return: {annualized_return:.2%}\")\n",
    "        print(f\"  Annualized Sharpe Ratio: {asString:8s}  Sortino Ratio:     {sortino_ratio:.2f}\")\n",
    "        print(f\"  Max Drawdown:            {mdString:8s}  Calmar Ratio:      {calmar_ratio:.2f}\")\n",
    "\n",
    "    if returns:\n",
    "        return total_return, annualized_return, annualized_sharpe, sortino_ratio, max_drawdown, calmar_ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting the dataframes\n",
    "so they could be interpeted as one complete investment plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfConnect(DFs: list) -> pd.DataFrame:\n",
    "    combinedDF = pd.concat(DFs).reset_index(drop = True, inplace = False)\n",
    "    combinedDF['TempDate'] = pd.to_datetime(combinedDF['Date'])\n",
    "    combinedDF = combinedDF.sort_values(by = 'TempDate').reset_index(drop = True, inplace = False)\n",
    "    try:\n",
    "        combinedDF = combinedDF.drop(columns = ['TempDate', 'index'])\n",
    "    except:\n",
    "        combinedDF = combinedDF.drop(columns = ['TempDate'])\n",
    "    return combinedDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enums\n",
    "class ActionType(Enum):\n",
    "    BUY = 1\n",
    "    SELL = -1\n",
    "    DONOTHING = 0\n",
    "\n",
    "# our strategy class\n",
    "class Strategy():\n",
    "    def __init__(self, bb_window: int, rsi_window: int, \n",
    "                 bb_window_min: int=None, bb_buy_threshold: float=0.5, bb_sell_threshold: float=0.5, rsi_window_min: int=None, rsi_limits: Tuple[float,float]=[30.0, 70.0], sell_all: bool=False, qty_scale: float=0.1,\n",
    "                 commission_type: object=\"scalar\", slippage_factor=np.inf) -> None:\n",
    "        self.bb_window = bb_window\n",
    "        self.rsi_window = rsi_window\n",
    "        if bb_window_min is None:\n",
    "            bb_window_min = bb_window\n",
    "        else:\n",
    "            if bb_window_min > bb_window:\n",
    "                raise ValueError(\"bb_window_min should be less than or equal to bb_window\")\n",
    "            if bb_window_min <= 0:\n",
    "                raise ValueError(\"bb_window_min should be a value greater than 0\")\n",
    "            self.bb_window_min = bb_window_min\n",
    "        if bb_buy_threshold <= 0 or bb_sell_threshold <= 0:\n",
    "            raise ValueError(\"bb_buy_threshold and bb_sell_threshold should be values greater than 0\")\n",
    "        self.bb_buy_threshold = bb_buy_threshold\n",
    "        self.bb_sell_threshold = bb_sell_threshold\n",
    "        if rsi_window_min is None:\n",
    "            rsi_window_min = rsi_window\n",
    "        else:\n",
    "            if rsi_window_min > rsi_window:\n",
    "                raise ValueError(\"rsi_window_min should be less than or equal to rsi_window\")\n",
    "            if rsi_window_min <= 0:\n",
    "                raise ValueError(\"rsi_window_min should be a value greater than 0\")\n",
    "            self.rsi_window_min = rsi_window_min\n",
    "        if rsi_limits[0] <= 0 or rsi_limits[1] > 100 or rsi_limits[0] >= rsi_limits[1]:\n",
    "            raise ValueError(\"rsi_limit should be a tuple of two values between 0 and 100 where the first value is less than the second\")\n",
    "        self.rsi_limits = rsi_limits\n",
    "        if commission_type not in [\"scalar\", \"precentage\"]:\n",
    "            raise ValueError(\"commision_type should be either 'scalar' or 'precentage'\")\n",
    "        self.commission_type = commission_type\n",
    "        self.sell_all = sell_all\n",
    "        self.slippage_factor = slippage_factor\n",
    "        if qty_scale <= 0:\n",
    "            raise ValueError(\"qty_scale should be a value greater than 0\")\n",
    "        self.qty_scale = qty_scale\n",
    "\n",
    "    def calc_signal(self, data: pd.DataFrame) -> None:\n",
    "        norm_search = data['Normalized_Searches_delayed']\n",
    "        close = data.Close.shift(1)\n",
    "\n",
    "        # MA of search volume as threshold for Bollinger Bands\n",
    "        norm_search_MA = norm_search.rolling(window=self.bb_window, min_periods=self.bb_window_min).mean()\n",
    "        norm_search_sd = norm_search.rolling(window=self.bb_window, min_periods=self.bb_window_min).std()\n",
    "        norm_search_sd = norm_search_sd.fillna(0)\n",
    "        upper_band = norm_search_MA + self.bb_buy_threshold * norm_search_sd\n",
    "        lower_band = norm_search_MA - self.bb_sell_threshold * norm_search_sd\n",
    "\n",
    "        data['norm_search_MA_upper_band'] = upper_band\n",
    "        data['norm_search_MA_lower_band'] = lower_band\n",
    "\n",
    "        # RSI of close price as threshold limiting the Bollinger Bands signal\n",
    "        delta = close.diff()\n",
    "        gains = delta.where(delta > 0, 0)\n",
    "        losses = -delta.where(delta < 0, 0)\n",
    "        avg_gain = gains.rolling(window=self.rsi_window, min_periods=self.rsi_window_min).mean()\n",
    "        avg_loss = losses.rolling(window=self.rsi_window, min_periods=self.rsi_window_min).mean()\n",
    "        rs = avg_gain / avg_loss\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "        data['strategy'] = 0\n",
    "        data.loc[(norm_search < lower_band) & (rsi < self.rsi_limits[0]), 'strategy'] = -1 # Sell signal\n",
    "        data.loc[(norm_search > upper_band) & (rsi > self.rsi_limits[1]), 'strategy'] = 1 # Buy signal\n",
    "\n",
    "    def calc_realistic_price(self, row: pd.Series, action: ActionType) -> float:\n",
    "        slippage_rate = ((row['Close'] - row['Open']) / row['Open']) / self.slippage_factor\n",
    "        slippage_price = row['Open'] + row['Open'] * slippage_rate\n",
    "\n",
    "        if action == ActionType.BUY:\n",
    "            return max(slippage_price, row['Open'])\n",
    "        elif action == ActionType.SELL:\n",
    "            return min(slippage_price, row['Open'])\n",
    "        else:\n",
    "            return slippage_price\n",
    "\n",
    "# comparison strategy class\n",
    "class BuyAndHoldStrategy():\n",
    "    def __init__(self, sell_all: bool=False, qty_scale: float=1.0,\n",
    "                 commission_type: object=\"scalar\", slippage_factor=np.inf) -> None:\n",
    "        if commission_type not in [\"scalar\", \"precentage\"]:\n",
    "            raise ValueError(\"commision_type should be either 'scalar' or 'precentage'\")\n",
    "        self.commission_type = commission_type\n",
    "        self.sell_all = sell_all\n",
    "        self.slippage_factor = slippage_factor\n",
    "        if qty_scale <= 0:\n",
    "            raise ValueError(\"qty_scale should be a value greater than 0\")\n",
    "        self.qty_scale = qty_scale\n",
    "    \n",
    "    def calc_signal(self, data: DataFrame) -> None:\n",
    "        data['strategy'] = 0 # Do nothing\n",
    "        data.iloc[0, data.columns.get_loc('strategy')] = 1 # Buy signal\n",
    "        data.iloc[-1, data.columns.get_loc('strategy')] = -1 # Sell signal\n",
    "\n",
    "    def calc_realistic_price(self, row: Series, action: ActionType) -> float:\n",
    "        slippage_rate = ((row['Close'] - row['Open']) / row['Open']) / self.slippage_factor\n",
    "        slippage_price = row['Open'] + row['Open'] * slippage_rate\n",
    "\n",
    "        if action == ActionType.BUY:\n",
    "            return max(slippage_price, row['Open'])\n",
    "        elif action == ActionType.SELL:\n",
    "            return min(slippage_price, row['Open'])\n",
    "        else:\n",
    "            return slippage_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backtest\n",
    "backtesting function that can be applied on different strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(DFs: list, strategy: Strategy, starting_balance: float, include: list, commission: float=0.0, hoddle: bool=False) -> pd.DataFrame:\n",
    "    for i in range(len(DFs)):\n",
    "        strategy.calc_signal(DFs[i])\n",
    "        DFs[i]['coin'] = decentralized_currencies_names[include[i]]\n",
    "        DFs[i].reset_index(drop=True)\n",
    "\n",
    "    # memory dataframe\n",
    "    mem = pd.DataFrame({'coin': [decentralized_currencies_names[include[i]] for i in range(len(DFs))], \n",
    "                        'qty': 0.0,\n",
    "                        'price': 0.0})\n",
    "\n",
    "    data = dfConnect(DFs)\n",
    "    data['qty'] = 0.0\n",
    "    data['balance'] = 0.0\n",
    "    data['coins_value'] = 0.0\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        # Get the current balance and qty before the action\n",
    "        curr_qty = mem.loc[mem.coin == data.loc[i, 'coin'], 'qty'].values[0]\n",
    "        prev_price = mem.loc[mem.coin == data.loc[i, 'coin'], 'price'].values[0]\n",
    "        new_price = strategy.calc_realistic_price(row, ActionType.BUY if row['strategy'] == 1 else ActionType.DONOTHING if ((row['strategy'] != -1) | (row['Date'] == data.Date.iloc[-1])) else ActionType.SELL)\n",
    "        mem.loc[mem.coin == data.loc[i, 'coin'], 'price'] = new_price\n",
    "        curr_balance = data.loc[i - 1, 'balance'] + (new_price - prev_price) * curr_qty if i > 0 else starting_balance\n",
    "        if curr_balance < 0:\n",
    "            new_price = strategy.calc_realistic_price(row, ActionType.SELL)\n",
    "            curr_balance = data.loc[i - 1, 'balance'] + (new_price - prev_price) * curr_qty if i > 0 else starting_balance\n",
    "        curr_coin_value = data.loc[i - 1, 'coins_value'] + (new_price - prev_price) * curr_qty if i > 0 else 0.0\n",
    "\n",
    "        # Sell signal when strategy says so or at the end of trade and holding any stock\n",
    "        if (curr_qty > 0) & ((row['Date'] == data.Date.iloc[-1]) | (row['strategy'] == -1) | (curr_balance < 0)):\n",
    "            if hoddle:\n",
    "                sell_qty = curr_qty\n",
    "            else:\n",
    "                sell_qty = curr_qty if (strategy.sell_all | (row['Date'] == data.Date.iloc[-1])) else min((row['norm_search_MA_lower_band'] / row['Normalized_Searches_delayed'] - 1) * strategy.qty_scale, curr_qty)\n",
    "            data.loc[i, 'balance'] = curr_balance + (new_price * sell_qty - commission if strategy.commission_type == \"scalar\" else new_price * sell_qty * (1 - commission))\n",
    "            data.loc[i, 'qty'] = 0.0 if (strategy.sell_all | (row['Date'] == data.Date.iloc[-1])) else curr_qty - sell_qty\n",
    "            mem.loc[mem.coin == data.loc[i, 'coin'], 'qty'] = 0.0 if (strategy.sell_all | (row['Date'] == data.Date.iloc[-1])) else curr_qty - sell_qty\n",
    "            data.loc[i, 'coins_value'] = curr_coin_value - ((new_price - prev_price) * sell_qty)\n",
    "\n",
    "        # Buy signal when strategy says so as long not in the end of trade data and not holding any stock\n",
    "        elif (row['Date'] != data.Date.iloc[-1]) & (data.loc[i, 'strategy'] == 1) & (curr_balance > 0):\n",
    "            if hoddle:\n",
    "                buy_qty = (starting_balance / len(DFs)) / new_price\n",
    "            else:\n",
    "                buy_qty = min((row['Normalized_Searches_delayed'] / row['norm_search_MA_upper_band'] - 1) * strategy.qty_scale, (curr_balance - commission) / new_price if strategy.commission_type == \"scalar\" else curr_balance / (new_price * (1 + commission)))\n",
    "            data.loc[i, 'balance'] = curr_balance - (new_price * buy_qty + commission if strategy.commission_type == \"scalar\" else new_price * buy_qty * (1 + commission))\n",
    "            data.loc[i, 'qty'] = curr_qty + buy_qty\n",
    "            mem.loc[mem.coin == data.loc[i, 'coin'], 'qty'] = curr_qty + buy_qty\n",
    "            data.loc[i, 'coins_value'] = curr_coin_value + ((new_price - prev_price) * buy_qty)\n",
    "\n",
    "        # Do nothing\n",
    "        else:\n",
    "            data.loc[i, 'balance'] = curr_balance\n",
    "            data.loc[i, 'qty'] = curr_qty\n",
    "            data.loc[i, 'coins_value'] = curr_coin_value\n",
    "        \n",
    "    data['portfolio_value'] = data['balance'] + data['coins_value']\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrainTestLog(file = './log.txt'):\n",
    "    total_retruns, annualized_returns, annualized_sharpes, sortino_ratios, max_drawdowns, calmar_ratios = [], [], [], [], [], []\n",
    "    first = True\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            if first:\n",
    "                first = False\n",
    "                continue\n",
    "            total_retruns.append(float(line.split()[1][-2]) / 100)\n",
    "            annualized_returns.append(float(line.split()[3][-2]) / 100)\n",
    "            annualized_sharpes.append(float(line.split()[5][-1]))\n",
    "            sortino_ratios.append(float(line.split()[7][-1]))\n",
    "            max_drawdowns.append(float(line.split()[9][-2]) / 100)\n",
    "            calmar_ratios.append(float(line.split()[11][-1]))\n",
    "    return total_retruns, annualized_returns, annualized_sharpes, sortino_ratios, max_drawdowns, calmar_ratios\n",
    "\n",
    "def writeTrainTestLog(total_return, annualized_return, annualized_sharpe, sortino_ratio, max_drawdown, calmar_ratio, filePath = './log.txt'):\n",
    "    if not os.path.exists(filePath):\n",
    "        with open(filePath, 'a') as file:\n",
    "            file.write('total_return annualized_return annualized_sharpe sortino_ratio max_drawdown calmar_ratio\\n')\n",
    "    with open(filePath, 'a') as file:\n",
    "        file.write(f'tr {total_return:.2%} ar {annualized_return:.2%} as {annualized_sharpe:.2f} sr {sortino_ratio:.2f} md {max_drawdown:.2%} cr {calmar_ratio:.2f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Granger Causality Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_granger(DFs: list, max_lags: int = 7, verbose: bool = True) -> Tuple[list, list]:\n",
    "    wrn.filterwarnings('ignore', category=UserWarning)\n",
    "    wrn.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "    lags = []\n",
    "    p_values = []\n",
    "\n",
    "    for i in range(len(DFs)):\n",
    "        try:\n",
    "            cause = grangercausalitytests(DFs[i][['log_returns', 'log_searches']], \n",
    "                                          maxlag = max_lags, \n",
    "                                          verbose = False)\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Failed to perform Granger Causality Test on {decentralized_currencies_names[i]}: {e}')\n",
    "\n",
    "        min_p_value = float('inf')\n",
    "        min_p_lag = None\n",
    "\n",
    "        for lag, result in cause.items():\n",
    "            p_value = result[0]['ssr_ftest'][1]\n",
    "            if p_value < min_p_value:\n",
    "                min_p_value = p_value\n",
    "                min_p_lag = lag\n",
    "\n",
    "        lags.append(min_p_lag)\n",
    "        p_values.append(min_p_value)\n",
    "\n",
    "        if verbose:\n",
    "            print(f'{decentralized_currencies_names[i]}: {min_p_lag} lags, p-value = {min_p_value:.4f}')\n",
    "    \n",
    "    return lags, p_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting & Connecting The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrn.filterwarnings('ignore', category=UserWarning)\n",
    "wrn.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "coinsDFs = []\n",
    "\n",
    "for i in range(len(decentralized_currencies)):\n",
    "    coinsDFs.append(getNormalizedData(decentralized_currencies_names[i], decentralized_currencies[i], \n",
    "                                      start = start, end = end, \n",
    "                                      do_double = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtesting\n",
    "## description\n",
    "### Data Split\n",
    "* 4 years train\n",
    "* 1 year test\n",
    "### hyper-parameters:\n",
    "* bb_window: size of the window of MA of the normalized searches and the bollinger bands on that MA\n",
    "* bb_window_min: minimum size of the window of MA of the normalized searches and the bollinger bands on that MA\n",
    "* bb_threshold: how many sds should be added to/subtracted from the bollinger bands\n",
    "* rsi_window: size of the window of the RSI on the Close\n",
    "* rsi_window_min: minimum size of the window of RSI on the Close\n",
    "* rsi_limits: how sensitive the rsi is, \n",
    "    - the higher the lower value (the first value of rsi_limits) the less the RSI blocks a BUY signal\n",
    "    - the lower the higher value (the second value of rsi_limits) the less the RSI blocks a SELL signal\n",
    "* sell_all: a boolian value that indicates whether the strategy says to sell all of the quantity of the asset when it gives a SELL signal or the strategy says to sell some relative amount of the quantity when it gives a SELL signal\n",
    "* qty_scale: represents the scale of the quantity that is calculated by the strategy\n",
    "### additional parameters:\n",
    "* starting_balance: the starting balance\n",
    "* commission: the commission for every market dealing\n",
    "* commission_type: whether the commission is a set amount or some precentage from the price\n",
    "* slippage_factor: how much slippage exists in the market \n",
    "## Defining Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance = 10000\n",
    "\n",
    "# hyperparameters\n",
    "bb_window = [10, 14, 18]\n",
    "rsi_window = [21, 28, 35]\n",
    "bb_buy_threshold = [0.1, 0.5, 1, 2]\n",
    "bb_sell_threshold = [0.1, 0.5, 1, 2]\n",
    "rsi_limits = [[20.0, 80.0], [30.0, 70.0]]\n",
    "qty_scale = [0.7, 1.2, 1.6]\n",
    "max_p_value = [0.1, 0.15, 0.2]\n",
    "\n",
    "# perm parameters\n",
    "bb_window_min = 1\n",
    "rsi_window_min = 1\n",
    "sell_all = True\n",
    "commission = 0.0001\n",
    "commission_type = \"precentage\"\n",
    "slippage_factor = 1.0\n",
    "max_lags = 7\n",
    "cutoff = '2023-07-01'\n",
    "\n",
    "# permute the hyperparameters\n",
    "params = list(itertools.product(bb_window, rsi_window, bb_buy_threshold, bb_sell_threshold, rsi_limits, qty_scale, max_p_value))\n",
    "\n",
    "amntOparams = len(params)\n",
    "\n",
    "fill_stages = [' ', '\\u2591', '\\u2592', '\\u2593', '\\u2588']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      "Strategy hyperparameters:\n",
      "  bb_window:   10             rsi_window: 21     bb_threshold: [0.1, 0.1]\n",
      "  rsi_limits:  [20.0, 80.0]   qty_scale:  0.7    max_p_value:  0.1\n",
      "==================================================================\n",
      "\n",
      "Results for Train:\n",
      "  Total Return:            -365.32%  Annualized Return: nan%\n",
      "  Annualized Sharpe Ratio: nan       Sortino Ratio:     nan\n",
      "  Max Drawdown:            154.03%   Calmar Ratio:      nan\n",
      "\n",
      "Results for Test:\n",
      "  Total Return:            -0.22%    Annualized Return: -0.22%\n",
      "  Annualized Sharpe Ratio: -0.00     Sortino Ratio:     -0.00\n",
      "  Max Drawdown:            120.73%   Calmar Ratio:      -0.00\n",
      "\n",
      "Coins included (6):\n",
      "=============================================\n",
      "  • Ethereum      • Cardano       • Litecoin  \n",
      "  • Polkadot      • Chainlink     • Dogecoin  \n",
      "\n",
      "Time elapsed: 10.50s, Time from previous run: 10.5s\n",
      "Average run time: 10.5s, Estimated run time left: 7h 33m 25.50s\n",
      "Estimated time to finish: 21:00:07\n",
      "1/2592 = 0.04%\n",
      "[                          ]\n",
      "\n",
      "==================================================================\n",
      "Strategy hyperparameters:\n",
      "  bb_window:   10             rsi_window: 21     bb_threshold: [0.1, 0.1]\n",
      "  rsi_limits:  [20.0, 80.0]   qty_scale:  0.7    max_p_value:  0.15\n",
      "==================================================================\n",
      "\n",
      "Results for Train:\n",
      "  Total Return:            -5881.80%  Annualized Return: nan%\n",
      "  Annualized Sharpe Ratio: nan       Sortino Ratio:     nan\n",
      "  Max Drawdown:            6707.27%  Calmar Ratio:      nan\n",
      "\n",
      "Results for Test:\n",
      "  Total Return:            -1270.87%  Annualized Return: nan%\n",
      "  Annualized Sharpe Ratio: nan       Sortino Ratio:     nan\n",
      "  Max Drawdown:            2474.53%  Calmar Ratio:      nan\n",
      "\n",
      "Coins included (7):\n",
      "=============================================\n",
      "  • Bitcoin       • Ethereum      • Cardano   \n",
      "  • Litecoin      • Polkadot      • Chainlink \n",
      "  • Dogecoin    \n",
      "\n",
      "Time elapsed: 22.60s, Time from previous run: 12.1s\n",
      "Average run time: 11.3s, Estimated run time left: 8h 7m 47.00s\n",
      "Estimated time to finish: 21:34:40\n",
      "2/2592 = 0.08%\n",
      "[                          ]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m stratTrainDf \u001b[38;5;241m=\u001b[39m backtest(coinsTrainDfs\u001b[38;5;241m.\u001b[39mcopy(), strat, balance, include, commission)\n\u001b[0;32m     44\u001b[0m stratTrainDf\u001b[38;5;241m.\u001b[39mdropna(inplace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 45\u001b[0m stratTestDf \u001b[38;5;241m=\u001b[39m \u001b[43mbacktest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoinsTestDfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbalance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommission\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m stratTestDf\u001b[38;5;241m.\u001b[39mdropna(inplace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m==================================================================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStrategy hyperparameters:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  bb_window:   \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbb_window\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m13s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  rsi_window: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrsi_window\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m5s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  bb_threshold: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbb_sell_threshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbb_buy_threshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  rsi_limits:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrsi_limits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m13s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  qty_scale:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mqty_scale\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m5s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  max_p_value:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_p_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m==================================================================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 22\u001b[0m, in \u001b[0;36mbacktest\u001b[1;34m(DFs, strategy, starting_balance, include, commission, hoddle)\u001b[0m\n\u001b[0;32m     20\u001b[0m prev_price \u001b[38;5;241m=\u001b[39m mem\u001b[38;5;241m.\u001b[39mloc[mem\u001b[38;5;241m.\u001b[39mcoin \u001b[38;5;241m==\u001b[39m data\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoin\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     21\u001b[0m new_price \u001b[38;5;241m=\u001b[39m strategy\u001b[38;5;241m.\u001b[39mcalc_realistic_price(row, ActionType\u001b[38;5;241m.\u001b[39mBUY \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrategy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m ActionType\u001b[38;5;241m.\u001b[39mDONOTHING \u001b[38;5;28;01mif\u001b[39;00m ((row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrategy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m|\u001b[39m (row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m data\u001b[38;5;241m.\u001b[39mDate\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;28;01melse\u001b[39;00m ActionType\u001b[38;5;241m.\u001b[39mSELL)\n\u001b[1;32m---> 22\u001b[0m \u001b[43mmem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcoin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m new_price\n\u001b[0;32m     23\u001b[0m curr_balance \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mloc[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalance\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m (new_price \u001b[38;5;241m-\u001b[39m prev_price) \u001b[38;5;241m*\u001b[39m curr_qty \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m starting_balance\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m curr_balance \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\pyVers\\py3.12.2\\Lib\\site-packages\\pandas\\core\\indexing.py:911\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    910\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[1;32m--> 911\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\pyVers\\py3.12.2\\Lib\\site-packages\\pandas\\core\\indexing.py:1942\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1939\u001b[0m \u001b[38;5;66;03m# align and set the values\u001b[39;00m\n\u001b[0;32m   1940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m take_split_path:\n\u001b[0;32m   1941\u001b[0m     \u001b[38;5;66;03m# We have to operate column-wise\u001b[39;00m\n\u001b[1;32m-> 1942\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer_split_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1943\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1944\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[1;32md:\\pyVers\\py3.12.2\\Lib\\site-packages\\pandas\\core\\indexing.py:2035\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_split_path\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   2032\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2033\u001b[0m     \u001b[38;5;66;03m# scalar value\u001b[39;00m\n\u001b[0;32m   2034\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m loc \u001b[38;5;129;01min\u001b[39;00m ilocs:\n\u001b[1;32m-> 2035\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_single_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\pyVers\\py3.12.2\\Lib\\site-packages\\pandas\\core\\indexing.py:2175\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_single_column\u001b[1;34m(self, loc, value, plane_indexer)\u001b[0m\n\u001b[0;32m   2165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mvoid:\n\u001b[0;32m   2166\u001b[0m         \u001b[38;5;66;03m# This means we're expanding, with multiple columns, e.g.\u001b[39;00m\n\u001b[0;32m   2167\u001b[0m         \u001b[38;5;66;03m#     df = pd.DataFrame({'A': [1,2,3], 'B': [4,5,6]})\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2170\u001b[0m         \u001b[38;5;66;03m# Here, we replace those temporary `np.void` columns with\u001b[39;00m\n\u001b[0;32m   2171\u001b[0m         \u001b[38;5;66;03m# columns of the appropriate dtype, based on `value`.\u001b[39;00m\n\u001b[0;32m   2172\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc[:, loc] \u001b[38;5;241m=\u001b[39m construct_1d_array_from_inferred_fill_value(\n\u001b[0;32m   2173\u001b[0m             value, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   2174\u001b[0m         )\n\u001b[1;32m-> 2175\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_setitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplane_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[1;32md:\\pyVers\\py3.12.2\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1338\u001b[0m, in \u001b[0;36mBlockManager.column_setitem\u001b[1;34m(self, loc, idx, value, inplace_only)\u001b[0m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1337\u001b[0m     new_mgr \u001b[38;5;241m=\u001b[39m col_mgr\u001b[38;5;241m.\u001b[39msetitem((idx,), value)\n\u001b[1;32m-> 1338\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_block\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m needs_to_warn:\n\u001b[0;32m   1341\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1342\u001b[0m         COW_WARNING_GENERAL_MSG,\n\u001b[0;32m   1343\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m   1344\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1345\u001b[0m     )\n",
      "File \u001b[1;32md:\\pyVers\\py3.12.2\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1133\u001b[0m, in \u001b[0;36mBlockManager.iset\u001b[1;34m(self, loc, value, inplace, refs)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Accessing public blknos ensures the public versions are initialized\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m blknos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblknos[loc]\n\u001b[1;32m-> 1133\u001b[0m blklocs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblklocs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1135\u001b[0m unfit_mgr_locs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1136\u001b[0m unfit_val_locs \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wrn.filterwarnings('ignore')\n",
    "\n",
    "start_time = time.time()\n",
    "prev_time = start_time\n",
    "\n",
    "if os.path.exists('./trainLog.txt'):\n",
    "    train_total_returns, train_annualized_returns, train_annualized_sharpes, train_sortino_ratios, train_max_drawdowns, train_calmar_ratios = readTrainTestLog('./trainLog.txt')\n",
    "    test_total_returns, test_annualized_returns, test_annualized_sharpes, test_sortino_ratios, test_max_drawdowns, test_calmar_ratios = readTrainTestLog('./testLog.txt')\n",
    "else:\n",
    "    train_total_returns, train_annualized_returns, train_annualized_sharpes, train_sortino_ratios, train_max_drawdowns, train_calmar_ratios = [], [], [], [], [], []\n",
    "    test_total_returns, test_annualized_returns, test_annualized_sharpes, test_sortino_ratios, test_max_drawdowns, test_calmar_ratios = [], [], [], [], [], []\n",
    "\n",
    "starting_pos = len(train_total_returns)\n",
    "\n",
    "runTimeList = []\n",
    "\n",
    "for i, param in enumerate(params[starting_pos:]):\n",
    "    bb_window, rsi_window, bb_buy_threshold, bb_sell_threshold, rsi_limits, qty_scale, max_p_value = param\n",
    "\n",
    "    coinsTrainDfs = []\n",
    "    coinsTestDfs = []\n",
    "\n",
    "    for j, df in enumerate(coinsDFs):\n",
    "        df['temp_date'] = pd.to_datetime(df['Date'])\n",
    "        coinsTrainDfs.append(df[df['temp_date'] <= pd.to_datetime(cutoff)])\n",
    "        coinsTestDfs.append(df[df['temp_date'] > pd.to_datetime(cutoff)])\n",
    "        df.drop(columns = ['temp_date'], inplace = True)\n",
    "\n",
    "    lags, p_values = do_granger(coinsTrainDfs.copy(), max_lags=max_lags, verbose=False)\n",
    "\n",
    "    for j in range(len(coinsTrainDfs)):\n",
    "        coinsTrainDfs[j]['Normalized_Searches_delayed'] = coinsTrainDfs[j]['Normalized_Searches'].shift(lags[j])\n",
    "        coinsTrainDfs[j].dropna(inplace = True)\n",
    "        coinsTestDfs[j]['Normalized_Searches_delayed'] = coinsTestDfs[j]['Normalized_Searches'].shift(lags[j])\n",
    "        coinsTestDfs[j].dropna(inplace = True)\n",
    "\n",
    "    include = [j for j, r in enumerate(np.array(p_values) < max_p_value)if r]\n",
    "    coinsIncluded = [decentralized_currencies_names[j] for j in include]\n",
    "    coinsTrainDfs = [coinsTrainDfs[j] for j in include]\n",
    "    coinsTestDfs = [coinsTestDfs[j] for j in include]\n",
    "\n",
    "    strat = Strategy(bb_window=bb_window, rsi_window=rsi_window, bb_window_min=bb_window_min, bb_buy_threshold=bb_buy_threshold, bb_sell_threshold=bb_sell_threshold, rsi_window_min=rsi_window_min, rsi_limits=rsi_limits, sell_all=sell_all, qty_scale=qty_scale, commission_type=commission_type, slippage_factor=slippage_factor)\n",
    "    stratTrainDf = backtest(coinsTrainDfs.copy(), strat, balance, include, commission)\n",
    "    stratTrainDf.dropna(inplace = True)\n",
    "    stratTestDf = backtest(coinsTestDfs.copy(), strat, balance, include, commission)\n",
    "    stratTestDf.dropna(inplace = True)\n",
    "\n",
    "    print(f'==================================================================\\nStrategy hyperparameters:\\n  bb_window:   {f'{bb_window}':13s}  rsi_window: {f'{rsi_window}':5s}  bb_threshold: [{bb_sell_threshold}, {bb_buy_threshold}]\\n  rsi_limits:  {f'{rsi_limits}':13s}  qty_scale:  {f'{qty_scale}':5s}  max_p_value:  {max_p_value}\\n==================================================================\\n')\n",
    "\n",
    "    train_total_return, train_annualized_return, train_annualized_sharpe, train_sortino_ratio, train_max_drawdown, train_calmar_ratio = evaluate_strategy(stratTrainDf.copy(), 'Train', rf=0.052)\n",
    "    print()\n",
    "    test_total_return, test_annualized_return, test_annualized_sharpe, test_sortino_ratio, test_max_drawdown, test_calmar_ratio = evaluate_strategy(stratTestDf.copy(), 'Test', rf=0.052)\n",
    "    print()\n",
    "\n",
    "    train_total_returns.append(train_total_return)\n",
    "    train_annualized_returns.append(train_annualized_return)\n",
    "    train_annualized_sharpes.append(train_annualized_sharpe)\n",
    "    train_sortino_ratios.append(train_sortino_ratio)\n",
    "    train_max_drawdowns.append(train_max_drawdown)\n",
    "    train_calmar_ratios.append(train_calmar_ratio)\n",
    "\n",
    "    test_total_returns.append(test_total_return)\n",
    "    test_annualized_returns.append(test_annualized_return)\n",
    "    test_annualized_sharpes.append(test_annualized_sharpe)\n",
    "    test_sortino_ratios.append(test_sortino_ratio)\n",
    "    test_max_drawdowns.append(test_max_drawdown)\n",
    "    test_calmar_ratios.append(test_calmar_ratio)\n",
    "\n",
    "    writeTrainTestLog(train_total_return, train_annualized_return, train_annualized_sharpe, train_sortino_ratio, train_max_drawdown, train_calmar_ratio, filePath = './trainLog.txt')\n",
    "    writeTrainTestLog(test_total_return, test_annualized_return, test_annualized_sharpe, test_sortino_ratio, test_max_drawdown, test_calmar_ratio, filePath = './testLog.txt')\n",
    "\n",
    "    print(f'Coins included ({len(coinsIncluded)}):\\n=============================================')\n",
    "    for j in range(len(coinsIncluded)):\n",
    "        print(f'  \\u2022 {coinsIncluded[j]:10s}', end = '\\n' if (j + 1) % 3 == 0 else '  ')\n",
    "\n",
    "    if len(coinsIncluded) % 3 != 0:\n",
    "        print()\n",
    "\n",
    "    curr_time = time.time()\n",
    "    elapsed = round(curr_time - start_time, 2)\n",
    "    if elapsed > 60:\n",
    "        elapsed_mins = elapsed // 60\n",
    "        elapsed_secs = elapsed % 60\n",
    "        if elapsed_mins > 60:\n",
    "            elapsed_hours = elapsed_mins // 60\n",
    "            elapsed_mins = elapsed_mins % 60\n",
    "            time_elapsed = f'{elapsed_hours:.0f}h {elapsed_mins:.0f}m {elapsed_secs:.2f}s'\n",
    "        else:\n",
    "            time_elapsed = f'{elapsed_mins:.0f}m {elapsed_secs:.2f}s'\n",
    "    else:\n",
    "        time_elapsed = f'{elapsed:.2f}s'\n",
    "    from_last = round(curr_time - prev_time, 2)\n",
    "\n",
    "    runTimeList.append(from_last)\n",
    "\n",
    "    avgRunTime = np.array(runTimeList).mean().round(2)\n",
    "\n",
    "    estimated = round(avgRunTime * (amntOparams - i - 1 - starting_pos), 2)\n",
    "\n",
    "    if estimated > 60:\n",
    "        estimated_mins = estimated // 60\n",
    "        estimated_secs = estimated % 60\n",
    "        if estimated_mins > 60:\n",
    "            estimated_hours = estimated_mins // 60\n",
    "            estimated_mins = estimated_mins % 60\n",
    "            estimatedTime = f'{estimated_hours:.0f}h {estimated_mins:.0f}m {estimated_secs:.2f}s'\n",
    "        else:\n",
    "            estimatedTime = f'{estimated_mins:.0f}m {estimated_secs:.2f}s'\n",
    "    else:\n",
    "        estimatedTime = f'{estimated:.2f}s'\n",
    "\n",
    "    estimatedFinishTime = time.strftime('%H:%M:%S', time.localtime(curr_time + estimated))\n",
    "\n",
    "    print(f'\\nTime elapsed: {time_elapsed}, Time from previous run: {from_last}s\\nAverage run time: {avgRunTime}s, Estimated run time left: {estimatedTime}\\nEstimated time to finish: {estimatedFinishTime}\\n{i + starting_pos + 1}/{amntOparams} = {((i + starting_pos + 1) / amntOparams):.2%}\\n[{fill_stages[4] * (math.floor(((i + starting_pos + 1) / amntOparams) * 100) // 4)}{fill_stages[(math.ceil(((i + starting_pos + 1) / amntOparams) * 100) - 1) % 4]}{' ' * (25 - math.ceil(((i + starting_pos + 1) / amntOparams) * 100) // 4)}]\\n')\n",
    "    prev_time = curr_time\n",
    "\n",
    "bb_windows, rsi_windows, bb_buy_thresholds, bb_sell_thresholds, rsi_limitss, qty_scales, max_p_values = zip(*params)\n",
    "trainResultsDf = pd.DataFrame({'total_return': train_total_returns, 'annualized_return': train_annualized_returns, 'annualized_sharpe': train_annualized_sharpes, 'sortino_ratio': train_sortino_ratios, 'max_drawdown': train_max_drawdowns, 'calmar_ratio': train_calmar_ratios, 'bb_window': bb_windows, 'rsi_window': rsi_windows, 'bb_buy_threshold': bb_buy_thresholds, 'bb_sell_threshold': bb_sell_thresholds, 'rsi_limits': rsi_limitss, 'qty_scale': qty_scales, 'max_p_value': max_p_values})\n",
    "testResultsDf = pd.DataFrame({'total_return': test_total_returns, 'annualized_return': test_annualized_returns, 'annualized_sharpe': test_annualized_sharpes, 'sortino_ratio': test_sortino_ratios, 'max_drawdown': test_max_drawdowns, 'calmar_ratio': test_calmar_ratios, 'bb_window': bb_windows, 'rsi_window': rsi_windows, 'bb_buy_threshold': bb_buy_thresholds, 'bb_sell_threshold': bb_sell_thresholds, 'rsi_limits': rsi_limitss, 'qty_scale': qty_scales, 'max_p_value': max_p_values})\n",
    "overallResultsDf = pd.DataFrame({'train_total_return': train_total_returns, 'test_total_return': test_total_returns, 'train_annualized_return': train_annualized_returns, 'test_annualized_return': test_annualized_returns, 'train_annualized_sharpe': train_annualized_sharpes, 'test_annualized_sharpe': test_annualized_sharpes, 'train_sortino_ratio': train_sortino_ratios, 'test_sortino_ratio': test_sortino_ratios, 'train_max_drawdown': train_max_drawdowns, 'test_max_drawdown': test_max_drawdowns, 'train_calmar_ratio': train_calmar_ratios, 'test_calmar_ratio': test_calmar_ratios, 'bb_window': bb_windows, 'rsi_window': rsi_windows, 'bb_buy_threshold': bb_buy_thresholds, 'bb_sell_threshold': bb_sell_thresholds, 'rsi_limits': rsi_limitss, 'qty_scale': qty_scales, 'max_p_value': max_p_values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>log_returns</th>\n",
       "      <th>Search_Volume</th>\n",
       "      <th>check_ratio</th>\n",
       "      <th>Normalized_Searches</th>\n",
       "      <th>log_searches</th>\n",
       "      <th>temp_date</th>\n",
       "      <th>Normalized_Searches_delayed</th>\n",
       "      <th>norm_search_MA_upper_band</th>\n",
       "      <th>norm_search_MA_lower_band</th>\n",
       "      <th>strategy</th>\n",
       "      <th>coin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-09-16</td>\n",
       "      <td>10302.00</td>\n",
       "      <td>10353.81</td>\n",
       "      <td>10080.70</td>\n",
       "      <td>10249.27</td>\n",
       "      <td>20185.637</td>\n",
       "      <td>-0.005153</td>\n",
       "      <td>194325.846154</td>\n",
       "      <td>6940.208791</td>\n",
       "      <td>2.540160</td>\n",
       "      <td>0.660290</td>\n",
       "      <td>2019-09-16</td>\n",
       "      <td>2.262653</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Bitcoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-09-17</td>\n",
       "      <td>10257.30</td>\n",
       "      <td>10270.63</td>\n",
       "      <td>10136.77</td>\n",
       "      <td>10186.52</td>\n",
       "      <td>22519.607</td>\n",
       "      <td>-0.006141</td>\n",
       "      <td>187385.637363</td>\n",
       "      <td>6940.208791</td>\n",
       "      <td>2.152395</td>\n",
       "      <td>-0.165646</td>\n",
       "      <td>2019-09-17</td>\n",
       "      <td>2.185797</td>\n",
       "      <td>2.229659</td>\n",
       "      <td>2.218790</td>\n",
       "      <td>0</td>\n",
       "      <td>Bitcoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-09-18</td>\n",
       "      <td>10183.87</td>\n",
       "      <td>10256.65</td>\n",
       "      <td>10097.92</td>\n",
       "      <td>10155.16</td>\n",
       "      <td>20832.895</td>\n",
       "      <td>-0.003083</td>\n",
       "      <td>173505.219780</td>\n",
       "      <td>6940.208791</td>\n",
       "      <td>1.619418</td>\n",
       "      <td>-0.284514</td>\n",
       "      <td>2019-09-18</td>\n",
       "      <td>2.479341</td>\n",
       "      <td>2.324486</td>\n",
       "      <td>2.294041</td>\n",
       "      <td>0</td>\n",
       "      <td>Bitcoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-09-19</td>\n",
       "      <td>10154.70</td>\n",
       "      <td>10331.62</td>\n",
       "      <td>9530.02</td>\n",
       "      <td>10245.30</td>\n",
       "      <td>26228.754</td>\n",
       "      <td>0.008837</td>\n",
       "      <td>194325.846154</td>\n",
       "      <td>6940.208791</td>\n",
       "      <td>2.459308</td>\n",
       "      <td>0.417813</td>\n",
       "      <td>2019-09-19</td>\n",
       "      <td>2.108941</td>\n",
       "      <td>2.275145</td>\n",
       "      <td>2.243220</td>\n",
       "      <td>-1</td>\n",
       "      <td>Bitcoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019-09-20</td>\n",
       "      <td>10247.70</td>\n",
       "      <td>10279.30</td>\n",
       "      <td>10070.50</td>\n",
       "      <td>10166.35</td>\n",
       "      <td>23265.910</td>\n",
       "      <td>-0.007736</td>\n",
       "      <td>187385.637363</td>\n",
       "      <td>6940.208791</td>\n",
       "      <td>2.233246</td>\n",
       "      <td>-0.096424</td>\n",
       "      <td>2019-09-20</td>\n",
       "      <td>1.598708</td>\n",
       "      <td>2.159700</td>\n",
       "      <td>2.094476</td>\n",
       "      <td>0</td>\n",
       "      <td>Bitcoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>2023-06-27</td>\n",
       "      <td>30260.10</td>\n",
       "      <td>30998.50</td>\n",
       "      <td>30213.00</td>\n",
       "      <td>30683.20</td>\n",
       "      <td>406073.348</td>\n",
       "      <td>0.013885</td>\n",
       "      <td>299868.177326</td>\n",
       "      <td>6119.758721</td>\n",
       "      <td>6.828976</td>\n",
       "      <td>-0.118790</td>\n",
       "      <td>2023-06-27</td>\n",
       "      <td>6.882143</td>\n",
       "      <td>5.803998</td>\n",
       "      <td>5.672020</td>\n",
       "      <td>0</td>\n",
       "      <td>Bitcoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>2023-06-28</td>\n",
       "      <td>30683.20</td>\n",
       "      <td>30701.80</td>\n",
       "      <td>29804.60</td>\n",
       "      <td>30066.30</td>\n",
       "      <td>416656.434</td>\n",
       "      <td>-0.020310</td>\n",
       "      <td>305987.936047</td>\n",
       "      <td>6119.758721</td>\n",
       "      <td>7.116097</td>\n",
       "      <td>0.041185</td>\n",
       "      <td>2023-06-28</td>\n",
       "      <td>10.957388</td>\n",
       "      <td>6.439785</td>\n",
       "      <td>6.084421</td>\n",
       "      <td>0</td>\n",
       "      <td>Bitcoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>30066.30</td>\n",
       "      <td>30837.00</td>\n",
       "      <td>30036.10</td>\n",
       "      <td>30439.90</td>\n",
       "      <td>382072.846</td>\n",
       "      <td>0.012349</td>\n",
       "      <td>293748.418605</td>\n",
       "      <td>6119.758721</td>\n",
       "      <td>6.700931</td>\n",
       "      <td>-0.060113</td>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>10.651810</td>\n",
       "      <td>6.979121</td>\n",
       "      <td>6.532156</td>\n",
       "      <td>0</td>\n",
       "      <td>Bitcoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>2023-06-30</td>\n",
       "      <td>30439.90</td>\n",
       "      <td>31300.00</td>\n",
       "      <td>29500.00</td>\n",
       "      <td>30460.20</td>\n",
       "      <td>809331.305</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>318227.453488</td>\n",
       "      <td>6119.758721</td>\n",
       "      <td>7.690338</td>\n",
       "      <td>0.137718</td>\n",
       "      <td>2023-06-30</td>\n",
       "      <td>9.904552</td>\n",
       "      <td>7.505760</td>\n",
       "      <td>7.043898</td>\n",
       "      <td>0</td>\n",
       "      <td>Bitcoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>30460.20</td>\n",
       "      <td>30668.20</td>\n",
       "      <td>30311.30</td>\n",
       "      <td>30573.60</td>\n",
       "      <td>135520.246</td>\n",
       "      <td>0.003716</td>\n",
       "      <td>269269.383721</td>\n",
       "      <td>6119.758721</td>\n",
       "      <td>5.552449</td>\n",
       "      <td>-0.325726</td>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>8.240558</td>\n",
       "      <td>7.698932</td>\n",
       "      <td>7.238980</td>\n",
       "      <td>0</td>\n",
       "      <td>Bitcoin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1385 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date      Open      High       Low     Close      Volume  \\\n",
       "8     2019-09-16  10302.00  10353.81  10080.70  10249.27   20185.637   \n",
       "9     2019-09-17  10257.30  10270.63  10136.77  10186.52   22519.607   \n",
       "10    2019-09-18  10183.87  10256.65  10097.92  10155.16   20832.895   \n",
       "11    2019-09-19  10154.70  10331.62   9530.02  10245.30   26228.754   \n",
       "12    2019-09-20  10247.70  10279.30  10070.50  10166.35   23265.910   \n",
       "...          ...       ...       ...       ...       ...         ...   \n",
       "1388  2023-06-27  30260.10  30998.50  30213.00  30683.20  406073.348   \n",
       "1389  2023-06-28  30683.20  30701.80  29804.60  30066.30  416656.434   \n",
       "1390  2023-06-29  30066.30  30837.00  30036.10  30439.90  382072.846   \n",
       "1391  2023-06-30  30439.90  31300.00  29500.00  30460.20  809331.305   \n",
       "1392  2023-07-01  30460.20  30668.20  30311.30  30573.60  135520.246   \n",
       "\n",
       "      log_returns  Search_Volume  check_ratio  Normalized_Searches  \\\n",
       "8       -0.005153  194325.846154  6940.208791             2.540160   \n",
       "9       -0.006141  187385.637363  6940.208791             2.152395   \n",
       "10      -0.003083  173505.219780  6940.208791             1.619418   \n",
       "11       0.008837  194325.846154  6940.208791             2.459308   \n",
       "12      -0.007736  187385.637363  6940.208791             2.233246   \n",
       "...           ...            ...          ...                  ...   \n",
       "1388     0.013885  299868.177326  6119.758721             6.828976   \n",
       "1389    -0.020310  305987.936047  6119.758721             7.116097   \n",
       "1390     0.012349  293748.418605  6119.758721             6.700931   \n",
       "1391     0.000667  318227.453488  6119.758721             7.690338   \n",
       "1392     0.003716  269269.383721  6119.758721             5.552449   \n",
       "\n",
       "      log_searches  temp_date  Normalized_Searches_delayed  \\\n",
       "8         0.660290 2019-09-16                     2.262653   \n",
       "9        -0.165646 2019-09-17                     2.185797   \n",
       "10       -0.284514 2019-09-18                     2.479341   \n",
       "11        0.417813 2019-09-19                     2.108941   \n",
       "12       -0.096424 2019-09-20                     1.598708   \n",
       "...            ...        ...                          ...   \n",
       "1388     -0.118790 2023-06-27                     6.882143   \n",
       "1389      0.041185 2023-06-28                    10.957388   \n",
       "1390     -0.060113 2023-06-29                    10.651810   \n",
       "1391      0.137718 2023-06-30                     9.904552   \n",
       "1392     -0.325726 2023-07-01                     8.240558   \n",
       "\n",
       "      norm_search_MA_upper_band  norm_search_MA_lower_band  strategy     coin  \n",
       "8                           NaN                        NaN         0  Bitcoin  \n",
       "9                      2.229659                   2.218790         0  Bitcoin  \n",
       "10                     2.324486                   2.294041         0  Bitcoin  \n",
       "11                     2.275145                   2.243220        -1  Bitcoin  \n",
       "12                     2.159700                   2.094476         0  Bitcoin  \n",
       "...                         ...                        ...       ...      ...  \n",
       "1388                   5.803998                   5.672020         0  Bitcoin  \n",
       "1389                   6.439785                   6.084421         0  Bitcoin  \n",
       "1390                   6.979121                   6.532156         0  Bitcoin  \n",
       "1391                   7.505760                   7.043898         0  Bitcoin  \n",
       "1392                   7.698932                   7.238980         0  Bitcoin  \n",
       "\n",
       "[1385 rows x 17 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coinsTrainDfs[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtesting End & it's Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      "Our Strategy hyperparameters:\n",
      "  bb_window:   18             rsi_window: 21     bb_threshold: [0.5, 2]\n",
      "  rsi_limits:  [30.0, 70.0]   qty_scale:  1.6    max_p_value:  0.1\n",
      "==================================================================\n",
      "\n",
      "Coins included (6):\n",
      "=============================================\n",
      "  • Ethereum      • Cardano       • Litecoin  \n",
      "  • Polkadot      • Chainlink     • Dogecoin  \n",
      "\n",
      "Our Strategy:\n",
      "Results for Train:\n",
      "  Total Return:            2623.92%  Annualized Return: 150.94%\n",
      "  Annualized Sharpe Ratio: 1.51      Sortino Ratio:     1.21\n",
      "  Max Drawdown:            86.06%    Calmar Ratio:      1.75\n",
      "Results for Test:\n",
      "  Total Return:            174.31%   Annualized Return: 175.83%\n",
      "  Annualized Sharpe Ratio: 1.76      Sortino Ratio:     1.13\n",
      "  Max Drawdown:            75.72%    Calmar Ratio:      2.32\n",
      "\n",
      "Comparison Strategy (Buy & Hold):\n",
      "Results for Train:\n",
      "  Total Return:            2031.42%  Annualized Return: 134.38%\n",
      "  Annualized Sharpe Ratio: 1.34      Sortino Ratio:     1.51\n",
      "  Max Drawdown:            89.29%    Calmar Ratio:      1.50\n",
      "Results for Test:\n",
      "  Total Return:            161.26%   Annualized Return: 162.65%\n",
      "  Annualized Sharpe Ratio: 1.63      Sortino Ratio:     3.60\n",
      "  Max Drawdown:            35.66%    Calmar Ratio:      4.56\n"
     ]
    }
   ],
   "source": [
    "bestIdx = np.argmax(train_calmar_ratios)\n",
    "bb_window, rsi_window, bb_buy_threshold, bb_sell_threshold, rsi_limits, qty_scale, max_p_value = params[bestIdx]\n",
    "\n",
    "coinsTrainDfs = []\n",
    "coinsTestDfs = []\n",
    "\n",
    "if os.path.exists('./trainLog.txt'):\n",
    "    os.remove('./trainLog.txt')\n",
    "    os.remove('./testLog.txt')\n",
    "\n",
    "for j, df in enumerate(coinsDFs):\n",
    "    df['temp_date'] = pd.to_datetime(df['Date'])\n",
    "    coinsTrainDfs.append(df[df['temp_date'] <= pd.to_datetime(cutoff)])\n",
    "    coinsTestDfs.append(df[df['temp_date'] > pd.to_datetime(cutoff)])\n",
    "    df.drop(columns = ['temp_date'], inplace = True)\n",
    "\n",
    "lags, p_values = do_granger(coinsTrainDfs.copy(), max_lags=max_lags, verbose=False)\n",
    "\n",
    "for j in range(len(coinsTrainDfs)):\n",
    "    coinsTrainDfs[j]['Normalized_Searches_delayed'] = coinsTrainDfs[j]['Normalized_Searches'].shift(lags[j])\n",
    "    coinsTrainDfs[j].dropna(inplace = True)\n",
    "    coinsTestDfs[j]['Normalized_Searches_delayed'] = coinsTestDfs[j]['Normalized_Searches'].shift(lags[j])\n",
    "    coinsTestDfs[j].dropna(inplace = True)\n",
    "\n",
    "include = [j for j, r in enumerate(np.array(p_values) < max_p_value)if r]\n",
    "coinsIncluded = [decentralized_currencies_names[j] for j in include]\n",
    "coinsTrainDfs = [coinsTrainDfs[j] for j in include]\n",
    "coinsTestDfs = [coinsTestDfs[j] for j in include]\n",
    "\n",
    "strat = Strategy(bb_window=bb_window, rsi_window=rsi_window, bb_window_min=bb_window_min, bb_buy_threshold=bb_buy_threshold, bb_sell_threshold=bb_sell_threshold, rsi_window_min=rsi_window_min, rsi_limits=rsi_limits, sell_all=sell_all, qty_scale=qty_scale, commission_type=commission_type, slippage_factor=slippage_factor)\n",
    "stratTrainDf = backtest(coinsTrainDfs.copy(), strat, balance, include, commission)\n",
    "stratTestDf = backtest(coinsTestDfs.copy(), strat, balance, include, commission)\n",
    "\n",
    "comparisonStrat = BuyAndHoldStrategy(sell_all=sell_all, qty_scale=1.0, commission_type=commission_type, slippage_factor=slippage_factor)\n",
    "stratComparisonTrainDf = backtest(coinsTrainDfs.copy(), comparisonStrat, balance, include, commission, hoddle=True)\n",
    "stratComparisonTestDf = backtest(coinsTestDfs.copy(), comparisonStrat, balance, include, commission, hoddle=True)\n",
    "\n",
    "print(f'=========================================================================\\nOur Strategy hyperparameters:\\n  bb_window:   {f'{bb_window}':13s}  rsi_window: {f'{rsi_window}':5s}  bb_threshold: [{bb_sell_threshold}, {bb_buy_threshold}]\\n  rsi_limits:  {f'{rsi_limits}':13s}  qty_scale:  {f'{qty_scale}':5s}  max_p_value:  {max_p_value}\\n=========================================================================\\n')\n",
    "\n",
    "print(f'Coins included ({len(coinsIncluded)}):\\n=============================================')\n",
    "for j in range(len(coinsIncluded)):\n",
    "    print(f'  \\u2022 {coinsIncluded[j]:10s}', end = '\\n' if (j + 1) % 3 == 0 else '  ')\n",
    "\n",
    "if len(coinsIncluded) % 3 != 0:\n",
    "    print()\n",
    "\n",
    "print('\\nOur Strategy:')\n",
    "evaluate_strategy(stratTrainDf, 'Train', rf=0.052, returns=False)\n",
    "evaluate_strategy(stratTestDf, 'Test', rf=0.052, returns=False)\n",
    "print('\\nComparison Strategy (Buy & Hold):')\n",
    "evaluate_strategy(stratComparisonTrainDf, 'Train', rf=0.052, returns=False)\n",
    "evaluate_strategy(stratComparisonTestDf, 'Test', rf=0.052, returns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      "Our Strategy hyperparameters:\n",
      "  bb_window:   18             rsi_window: 21     bb_threshold: [0.5, 2]\n",
      "  rsi_limits:  [30.0, 70.0]   qty_scale:  1.6    max_p_value:  0.1\n",
      "==================================================================\n",
      "\n",
      "Coins included (6):\n",
      "=============================================\n",
      "  • Ethereum      • Cardano       • Litecoin  \n",
      "  • Polkadot      • Chainlink     • Dogecoin  \n",
      "\n",
      "Our Strategy:\n",
      "Results for Train:\n",
      "  Total Return:            2623.92%  Annualized Return: 150.94%\n",
      "  Annualized Sharpe Ratio: 1.51      Sortino Ratio:     1.21\n",
      "  Max Drawdown:            86.06%    Calmar Ratio:      1.75\n",
      "Results for Test:\n",
      "  Total Return:            174.31%   Annualized Return: 175.83%\n",
      "  Annualized Sharpe Ratio: 1.76      Sortino Ratio:     1.13\n",
      "  Max Drawdown:            75.72%    Calmar Ratio:      2.32\n",
      "\n",
      "Comparison Strategy (Buy & Hold):\n",
      "Results for Train:\n",
      "  Total Return:            2031.42%  Annualized Return: 134.38%\n",
      "  Annualized Sharpe Ratio: 1.34      Sortino Ratio:     1.51\n",
      "  Max Drawdown:            89.29%    Calmar Ratio:      1.50\n",
      "Results for Test:\n",
      "  Total Return:            161.26%   Annualized Return: 162.65%\n",
      "  Annualized Sharpe Ratio: 1.63      Sortino Ratio:     3.60\n",
      "  Max Drawdown:            35.66%    Calmar Ratio:      4.56\n"
     ]
    }
   ],
   "source": [
    "bestIdx = np.argmax(train_sortino_ratios)\n",
    "bb_window, rsi_window, bb_buy_threshold, bb_sell_threshold, rsi_limits, qty_scale, max_p_value = params[bestIdx]\n",
    "\n",
    "coinsTrainDfs = []\n",
    "coinsTestDfs = []\n",
    "\n",
    "if os.path.exists('./trainLog.txt'):\n",
    "    os.remove('./trainLog.txt')\n",
    "    os.remove('./testLog.txt')\n",
    "\n",
    "for j, df in enumerate(coinsDFs):\n",
    "    df['temp_date'] = pd.to_datetime(df['Date'])\n",
    "    coinsTrainDfs.append(df[df['temp_date'] <= pd.to_datetime(cutoff)])\n",
    "    coinsTestDfs.append(df[df['temp_date'] > pd.to_datetime(cutoff)])\n",
    "    df.drop(columns = ['temp_date'], inplace = True)\n",
    "\n",
    "lags, p_values = do_granger(coinsTrainDfs.copy(), max_lags=max_lags, verbose=False)\n",
    "\n",
    "for j in range(len(coinsTrainDfs)):\n",
    "    coinsTrainDfs[j]['Normalized_Searches_delayed'] = coinsTrainDfs[j]['Normalized_Searches'].shift(lags[j])\n",
    "    coinsTrainDfs[j].dropna(inplace = True)\n",
    "    coinsTestDfs[j]['Normalized_Searches_delayed'] = coinsTestDfs[j]['Normalized_Searches'].shift(lags[j])\n",
    "    coinsTestDfs[j].dropna(inplace = True)\n",
    "\n",
    "include = [j for j, r in enumerate(np.array(p_values) < max_p_value)if r]\n",
    "coinsIncluded = [decentralized_currencies_names[j] for j in include]\n",
    "coinsTrainDfs = [coinsTrainDfs[j] for j in include]\n",
    "coinsTestDfs = [coinsTestDfs[j] for j in include]\n",
    "\n",
    "strat = Strategy(bb_window=bb_window, rsi_window=rsi_window, bb_window_min=bb_window_min, bb_buy_threshold=bb_buy_threshold, bb_sell_threshold=bb_sell_threshold, rsi_window_min=rsi_window_min, rsi_limits=rsi_limits, sell_all=sell_all, qty_scale=qty_scale, commission_type=commission_type, slippage_factor=slippage_factor)\n",
    "stratTrainDf = backtest(coinsTrainDfs.copy(), strat, balance, include, commission)\n",
    "stratTestDf = backtest(coinsTestDfs.copy(), strat, balance, include, commission)\n",
    "\n",
    "comparisonStrat = BuyAndHoldStrategy(sell_all=sell_all, qty_scale=1.0, commission_type=commission_type, slippage_factor=slippage_factor)\n",
    "stratComparisonTrainDf = backtest(coinsTrainDfs.copy(), comparisonStrat, balance, include, commission, hoddle=True)\n",
    "stratComparisonTestDf = backtest(coinsTestDfs.copy(), comparisonStrat, balance, include, commission, hoddle=True)\n",
    "\n",
    "print(f'=========================================================================\\nOur Strategy hyperparameters:\\n  bb_window:   {f'{bb_window}':13s}  rsi_window: {f'{rsi_window}':5s}  bb_threshold: [{bb_sell_threshold}, {bb_buy_threshold}]\\n  rsi_limits:  {f'{rsi_limits}':13s}  qty_scale:  {f'{qty_scale}':5s}  max_p_value:  {max_p_value}\\n=========================================================================\\n')\n",
    "\n",
    "print(f'Coins included ({len(coinsIncluded)}):\\n=============================================')\n",
    "for j in range(len(coinsIncluded)):\n",
    "    print(f'  \\u2022 {coinsIncluded[j]:10s}', end = '\\n' if (j + 1) % 3 == 0 else '  ')\n",
    "\n",
    "if len(coinsIncluded) % 3 != 0:\n",
    "    print()\n",
    "\n",
    "print('\\nOur Strategy:')\n",
    "evaluate_strategy(stratTrainDf, 'Train', rf=0.052, returns=False)\n",
    "evaluate_strategy(stratTestDf, 'Test', rf=0.052, returns=False)\n",
    "print('\\nComparison Strategy (Buy & Hold):')\n",
    "evaluate_strategy(stratComparisonTrainDf, 'Train', rf=0.052, returns=False)\n",
    "evaluate_strategy(stratComparisonTestDf, 'Test', rf=0.052, returns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratTrainDf.to_csv('./Results/strat_train_data.csv', index=False)\n",
    "stratTestDf.to_csv('./Results/strat_test_data.csv', index=False)\n",
    "stratComparisonTrainDf.to_csv('./Results/comparison_strat_train_data.csv', index=False)\n",
    "stratComparisonTestDf.to_csv('./Results/comparison_strat_test_data.csv', index=False)\n",
    "trainResultsDf.to_csv('./Results/train_results.csv', index=False)\n",
    "testResultsDf.to_csv('./Results/test_results.csv', index=False)\n",
    "overallResultsDf.to_csv('./Results/overall_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy N hold for Bitcoin:\n",
      "train period (2019-11-29 - 2023-07-01): 1246.09%\n",
      "test period (2023-07-03 - 2024-06-29): 172.77%\n",
      "\n",
      "buy N hold for Ethereum:\n",
      "train period (2020-02-08 - 2023-07-01): 485.54%\n",
      "test period (2023-07-09 - 2024-06-29): 135.05%\n",
      "\n",
      "buy N hold for Cardano:\n",
      "train period (2020-01-11 - 2023-07-01): 217.44%\n",
      "test period (2023-07-03 - 2024-06-29): 69.79%\n",
      "\n",
      "buy N hold for Solana:\n",
      "train period (2020-08-26 - 2023-07-01): 87.79%\n",
      "test period (2023-07-05 - 2024-06-29): 116.83%\n",
      "\n",
      "buy N hold for Ripple:\n",
      "train period (2020-01-19 - 2023-07-01): 247.88%\n",
      "test period (2023-07-03 - 2024-06-29): 204.29%\n",
      "\n",
      "buy N hold for Monero:\n",
      "train period (2020-07-16 - 2023-07-01): 2294.81%\n",
      "test period (2023-07-07 - 2024-06-29): 186.41%\n",
      "\n",
      "buy N hold for Litecoin:\n",
      "train period (2019-09-09 - 2023-07-01): 296.63%\n",
      "test period (2023-07-02 - 2024-06-29): 199.21%\n",
      "\n",
      "buy N hold for Polkadot:\n",
      "train period (2019-11-28 - 2023-07-01): 1278.63%\n",
      "test period (2023-07-02 - 2024-06-29): 174.31%\n",
      "\n",
      "buy N hold for Chainlink:\n",
      "train period (2020-02-01 - 2023-07-01): 519.93%\n",
      "test period (2023-07-02 - 2024-06-29): 131.66%\n",
      "\n",
      "buy N hold for Tezos:\n",
      "train period (2020-09-15 - 2023-07-01): 636.74%\n",
      "test period (2023-07-02 - 2024-06-29): 720.89%\n",
      "\n",
      "buy N hold for Dogecoin:\n",
      "train period (2020-01-07 - 2023-07-01): 221.33%\n",
      "test period (2023-07-02 - 2024-06-29): 97.54%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# change to be by sortino and calmar and all the other ratios\n",
    "\n",
    "for j, df in enumerate(coinsDFs):\n",
    "    df['temp_date'] = pd.to_datetime(df['Date'])\n",
    "    coinsTrainDfs.append(df[df['temp_date'] <= pd.to_datetime(cutoff)])\n",
    "    coinsTestDfs.append(df[df['temp_date'] > pd.to_datetime(cutoff)])\n",
    "    df.drop(columns = ['temp_date'], inplace = True)\n",
    "\n",
    "# calculate the returns for buy and hold strategy on all the coins, each coin separately\n",
    "\n",
    "for j, coin in enumerate(decentralized_currencies_names):\n",
    "    print(f'buy N hold for {coin}:')\n",
    "    print(f'train period ({coinsTrainDfs[j].Date.iloc[0]} - {coinsTrainDfs[j].Date.iloc[-1]}): {coinsTrainDfs[j].Close.iloc[-1]/coinsTrainDfs[j].Close.iloc[0]:.2%}')\n",
    "    print(f'test period ({coinsTestDfs[j].Date.iloc[0]} - {coinsTestDfs[j].Date.iloc[-1]}): {coinsTestDfs[j].Close.iloc[-1]/coinsTestDfs[j].Close.iloc[0]:.2%}\\n')\n",
    "\n",
    "# buyNholdStrat = BuyAndHoldStrategy(sell_all=sell_all, qty_scale=1.0, commission_type=commission_type, slippage_factor=slippage_factor)\n",
    "# buyNholdTrainDfs = []\n",
    "# buyNholdTestDfs = []\n",
    "\n",
    "# for j, df in enumerate(coinsTrainDfs):\n",
    "#     buyNholdTrainDfs.append(backtest([df], buyNholdStrat, balance, [0], commission, hoddle=True))\n",
    "#     buyNholdTestDfs.append(backtest([coinsTestDfs[j]], buyNholdStrat, balance, [0], commission, hoddle=True))\n",
    "\n",
    "# for j, coin in enumerate(decentralized_currencies_names):\n",
    "#     print(f'buy N hold for {coin}:')\n",
    "#     evaluate_strategy(buyNholdTrainDfs[j], 'Train', returns=False)\n",
    "#     evaluate_strategy(buyNholdTestDfs[j], 'Test', returns=False)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what from now\n",
    "\n",
    "- present results in presentation\n",
    "- discuss results over the complete strategy and by coin (by coin only total return/annual return)\n",
    "- add to presentation methodology data aquisition and cleaning in bullets\n",
    "- add to presentation methodology for backtesting in bullets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples for checks code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see data validity checks of a certain stock\n",
    "stock_name = 'Bitcoin' # change to any stock name that is in the list at the top\n",
    "stock_ticker = 'BTC' # change to the corresponding ticker of the stock\n",
    "\n",
    "getNormalizedData(stock_name, stock_ticker, \n",
    "                  start = start, end = end, \n",
    "                  do_double = True, verbose = True)\n",
    "\n",
    "# see the granger causality test results of a certain stock\n",
    "df = getNormalizedData(stock_name, stock_ticker, start = start, end = end, do_double = True)\n",
    "\n",
    "grangercausalitytests(df[['log_returns', 'log_searches']], \n",
    "                      maxlag = max_lags, \n",
    "                      verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previously used chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trend Data for ['Shiba Inu'] retrieved successfully.\n",
      "Stock Data for SHIB-USD retrieved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Calculate correlations for each category\n",
    "volt_del_corr_general = [trend_corr(stock)[0] for stock in general_stocks]\n",
    "volt_del_corr_tech = [trend_corr(stock)[0] for stock in tech_stocks]\n",
    "volt_del_corr_finance = [trend_corr(stock)[0] for stock in finance_stocks]\n",
    "volt_del_corr_crypto = [trend_corr(crypto)[0] for crypto in decentralized_currencies]\n",
    "\n",
    "# Combine the results\n",
    "volt_del_corr = volt_del_corr_general + volt_del_corr_tech + volt_del_corr_finance + volt_del_corr_crypto\n",
    "\n",
    "# Create labels for the scatter plot\n",
    "labels = general_stocks + tech_stocks + finance_stocks + decentralized_currencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scatter plot\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.scatter(range(len(general_stocks)), volt_del_corr_general, color = color_map['general'], label = 'General Stocks')\n",
    "for i in range(len(general_stocks)):\n",
    "    plt.axvline(x = i, color = color_map['general'], linestyle = ':', alpha = 0.3)  # Add vertical lines to separate the stocks\n",
    "plt.scatter(range(len(general_stocks), len(general_stocks) + len(tech_stocks)), volt_del_corr_tech, color = color_map['tech'], label = 'Tech Stocks')\n",
    "for i in range(len(general_stocks), len(general_stocks) + len(tech_stocks)):\n",
    "    plt.axvline(x = i, color = color_map['tech'], linestyle = ':', alpha = 0.3)  # Add vertical lines to separate the stocks\n",
    "plt.scatter(range(len(general_stocks) + len(tech_stocks), len(general_stocks) + len(tech_stocks) + len(finance_stocks)), volt_del_corr_finance, color = color_map['finance'], label = 'Finance Stocks')\n",
    "for i in range(len(general_stocks) + len(tech_stocks), len(general_stocks) + len(tech_stocks) + len(finance_stocks)):\n",
    "    plt.axvline(x = i, color = color_map['finance'], linestyle = ':', alpha = 0.3)  # Add vertical lines to separate the stocks\n",
    "plt.scatter(range(len(general_stocks) + len(tech_stocks) + len(finance_stocks), len(general_stocks) + len(tech_stocks) + len(finance_stocks) + len(decentralized_currencies)), volt_del_corr_crypto, color = color_map['crypto'], label = 'Decentralized Currencies')\n",
    "for i in range(len(general_stocks) + len(tech_stocks) + len(finance_stocks), len(general_stocks) + len(tech_stocks) + len(finance_stocks) + len(decentralized_currencies)):\n",
    "    plt.axvline(x = i, color = color_map['crypto'], linestyle = ':', alpha = 0.3)  # Add vertical lines to separate the stocks\n",
    "plt.axhline(y = 0, color = 'black', linestyle = '--')  # Add a horizontal line at y = 0\n",
    "plt.xlabel('Assets')\n",
    "plt.ylabel('Correlation with 7-Day Delayed Trend')\n",
    "plt.title('Correlation of Close Price and 7-Day Delayed Trend')\n",
    "legend = plt.legend()\n",
    "legend.get_frame().set_alpha(0.3)\n",
    "plt.xticks(range(len(labels)), labels, rotation = 60)\n",
    "plt.tight_layout(pad = 2)\n",
    "plt.savefig('Correlation_Scatter_Plot.png')\n",
    "plt.show()\n",
    "\n",
    "# Print the correlation values and their mean\n",
    "print(volt_del_corr, np.mean(volt_del_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for each stock and decentralized currency\n",
    "for stock in general_stocks + tech_stocks + finance_stocks + decentralized_currencies:\n",
    "    plot_stock_data(stock, download = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
