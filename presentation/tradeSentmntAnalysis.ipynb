{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas.core.api import Series as Series\n",
    "import matplotlib.pyplot as plt\n",
    "from pytrends.request import TrendReq\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import warnings as wrn\n",
    "import os\n",
    "from enum import Enum\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general_stocks = ['KO', 'PFE', 'WMT', 'PG', 'JNJ', 'DIS', 'PEP', 'MCD', 'T', 'VZ']\n",
    "# tech_stocks = ['AAPL', 'AMZN', 'MSFT', 'GOOGL', 'NVDA', 'TSLA', 'META', 'INTC', 'IBM', 'AMD']\n",
    "# finance_stocks = ['GS', 'BAC', 'WFC', 'USB', 'JPM', 'MA', 'V', 'AXP', 'C', 'BLK']\n",
    "decentralized_currencies = ['BTC', 'ETH', 'ADA', 'SOL', 'XRP', 'XMR', 'LTC', 'DOT', 'LINK', 'XTZ', 'DOGE', 'SHIB']\n",
    "\n",
    "# general_stocks_names = ['Coca-Cola', 'Pfizer', 'Walmart', 'Procter & Gamble', 'Johnson & Johnson', 'Disney', 'Pepsi', 'McDonalds', 'AT&T', 'Verizon']\n",
    "# tech_stocks_names = ['Apple', 'Amazon', 'Microsoft', 'Google', 'Nvidia', 'Tesla', 'Meta', 'Intel', 'IBM', 'AMD']\n",
    "# finance_stocks_names = ['Goldman Sachs', 'Bank of America', 'Wells Fargo', 'US Bancorp', 'JPMorgan Chase', 'Mastercard', 'Visa', 'American Express', 'Citigroup', 'BlackRock']\n",
    "decentralized_currencies_names = ['Bitcoin', 'Ethereum', 'Cardano', 'Solana', 'Ripple', 'Monero', 'Litecoin', 'Polkadot', 'Chainlink', 'Tezos', 'Dogecoin', 'Shiba Inu']\n",
    "\n",
    "# color_map = {\n",
    "#     'general': 'deepskyblue',\n",
    "#     'tech': 'limegreen',\n",
    "#     'finance': 'darkorchid',\n",
    "#     'crypto': 'red'\n",
    "# }\n",
    "\n",
    "start = '2019-06-30'\n",
    "end = '2024-07-01'\n",
    "max_lags = 7\n",
    "\n",
    "balance = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "## get_trends_data\n",
    "gets the trend data using pytrends, given a certain timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trends_data(keyword, \n",
    "                    timeframe=datetime.date.today().strftime('%Y-%m-%d') + ' ' + (datetime.date.today() - datetime.timedelta(days = 269)).strftime('%Y-%m-%d'),\n",
    "                    retries=5, \n",
    "                    backoff_factor=1.0,\n",
    "                    verbose=True):\n",
    "    pytrends = TrendReq(hl='en-US', tz=360, timeout=(10,25), )\n",
    "    pytrends.build_payload(keyword, cat = 0, timeframe = timeframe, geo='')\n",
    "    \n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            df = pytrends.interest_over_time()\n",
    "            if df is not None and not df.empty:\n",
    "                if verbose:\n",
    "                    print(f\"Trend Data for {keyword[0]} at timeframe {timeframe} retrieved successfully.\")\n",
    "                df.reset_index(inplace = True)\n",
    "                df.rename(columns = {'date': 'Date', keyword[0]: 'Trend'}, inplace = True)\n",
    "                df['Date'] = pd.to_datetime(df['Date'].dt.strftime('%m/%d/%Y'))\n",
    "                return df\n",
    "            else:\n",
    "                print(\"No data retrieved or DataFrame is empty.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            if \"429\" in str(e):\n",
    "                sleep_time = backoff_factor * (2 ** i)\n",
    "                if verbose:\n",
    "                    print(f\"Rate limit exceeded. Retrying in {sleep_time} seconds...\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                raise(f\"An error occurred: {e}\")\n",
    "    print(\"Failed to retrieve data after several retries.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_stock_data\n",
    "gets the prices of a certain stock in a certain timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(ticker, start, end, verbose = True):\n",
    "    currTicker = yf.Ticker(ticker)\n",
    "    tickerDF = currTicker.history(repair = True, start = start, end = end, auto_adjust = False).drop(columns = ['Dividends', 'Stock Splits', 'Repaired?']).reset_index()\n",
    "    if verbose:\n",
    "        print(f\"Stock Data for {ticker} retrieved successfully.\")\n",
    "    tickerDF['Date'] = pd.to_datetime(tickerDF['Date'].dt.strftime('%m/%d/%Y'))\n",
    "    return tickerDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trend_corr\n",
    "gets the data of the trends and prices of the stock given to it in a certain timeframe and calculates the correlation between the log_returns and the trends delayed by certain delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_corr(stock, days = 60, start = '2023-10-01', end = '2024-06-01', delay = 7):\n",
    "    if not os.path.exists(f\"./Data/{stock}_trends({start} - {end}).csv\"):\n",
    "        if stock in general_stocks:\n",
    "            name = general_stocks_names[general_stocks.index(stock)]\n",
    "        elif stock in tech_stocks:\n",
    "            name = tech_stocks_names[tech_stocks.index(stock)]\n",
    "        elif stock in finance_stocks:\n",
    "            name = finance_stocks_names[finance_stocks.index(stock)]\n",
    "        else:\n",
    "            name = decentralized_currencies_names[decentralized_currencies.index(stock)]\n",
    "        t = get_trends_data([name], timeframe = f\"{start} {end}\")\n",
    "        if t is None:\n",
    "            raise Exception(f'Failed to retrieve Trend Data of {stock}.')\n",
    "        t.to_csv(f\"./Data/{stock}_trends({start} - {end}).csv\")\n",
    "    else:\n",
    "        t = pd.read_csv(f\"./Data/{stock}_trends({start} - {end}).csv\")\n",
    "    if not os.path.exists(f\"./Data/{stock}_Prices({start} - {end}).csv\"):\n",
    "        if stock in decentralized_currencies:\n",
    "            p = get_stock_data(f'{stock}-USD', start = start, end = end)\n",
    "        else:\n",
    "            p = get_stock_data(stock, start = start, end = end)\n",
    "        p.to_csv(f\"./Data/{stock}_Prices({start} - {end}).csv\")\n",
    "    else:\n",
    "        p = pd.read_csv(f\"./Data/{stock}_Prices({start} - {end}).csv\")\n",
    "\n",
    "    t['Date'] = pd.to_datetime(t['Date'])\n",
    "    p['Date'] = pd.to_datetime(p['Date'])\n",
    "\n",
    "    full_data = pd.merge(p, t, on='Date')\n",
    "\n",
    "    full_data['log_returns'] = np.log(full_data.Close / full_data.Close.shift(1))\n",
    "    full_data['Volatility'] = full_data['log_returns'].rolling(window=days).std() * np.sqrt(days)\n",
    "\n",
    "    for i in range(1, 8):\n",
    "        full_data[f'Delay_{i}'] = full_data['Trend'].shift(i)\n",
    "\n",
    "    rho = full_data.corr()\n",
    "    rho_c = rho['Close'][f'Delay_{delay}']\n",
    "    return rho_c, full_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot_stock_data\n",
    "gets the data of the trends and the prices of the stock given to it in a certain timeframe and plots its close and its delayed trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stock_data(stock, days = 60, start = '2023-10-01', end = '2024-06-01', delay = 7, download = False):\n",
    "    if not os.path.exists(f\"./Data/{stock}_trends({start} - {end}).csv\"):\n",
    "        if stock in general_stocks:\n",
    "            name = general_stocks_names[general_stocks.index(stock)]\n",
    "        elif stock in tech_stocks:\n",
    "            name = tech_stocks_names[tech_stocks.index(stock)]\n",
    "        elif stock in finance_stocks:\n",
    "            name = finance_stocks_names[finance_stocks.index(stock)]\n",
    "        else:\n",
    "            name = decentralized_currencies_names[decentralized_currencies.index(stock)]\n",
    "        t = get_trends_data([name], timeframe = f\"{start} {end}\")\n",
    "        t.to_csv(f\"./Data/{stock}_trends({start} - {end}).csv\")\n",
    "    else:\n",
    "        t = pd.read_csv(f\"./Data/{stock}_trends({start} - {end}).csv\")\n",
    "    if not os.path.exists(f\"./Data/{stock}_Prices({start} - {end}).csv\"):\n",
    "        p = get_stock_data(stock, start = start, end = end)\n",
    "        p.to_csv(f\"./Data/{stock}_Prices({start} - {end}).csv\")\n",
    "    else:\n",
    "        p = pd.read_csv(f\"./Data/{stock}_Prices({start} - {end}).csv\")\n",
    "\n",
    "    t['Date'] = pd.to_datetime(t['Date'])\n",
    "    p['Date'] = pd.to_datetime(p['Date'])\n",
    "\n",
    "    full_data = pd.merge(p, t, on='Date')\n",
    "\n",
    "    full_data['log_returns'] = np.log(full_data.Close / full_data.Close.shift(1))\n",
    "    full_data['Volatility'] = full_data['log_returns'].rolling(window=days).std() * np.sqrt(days)\n",
    "\n",
    "    full_data[f'Delay_{delay}'] = full_data.Trend.shift(7)\n",
    "\n",
    "    # Determine the color based on the stock category\n",
    "    if stock in general_stocks:\n",
    "        color = color_map['general']\n",
    "        name = general_stocks_names[general_stocks.index(stock)]\n",
    "    elif stock in tech_stocks:\n",
    "        color = color_map['tech']\n",
    "        name = tech_stocks_names[tech_stocks.index(stock)]\n",
    "    elif stock in finance_stocks:\n",
    "        color = color_map['finance']\n",
    "        name = finance_stocks_names[finance_stocks.index(stock)]\n",
    "    else:\n",
    "        color = color_map['crypto']\n",
    "        name = decentralized_currencies_names[decentralized_currencies.index(stock)]\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "    # Plot Close price\n",
    "    axes[0].plot(full_data['Date'], full_data['Close'], label = 'Close Price', color = color)\n",
    "    axes[0].set_xlabel('Date')\n",
    "    axes[0].set_ylabel('Close Price')\n",
    "    axes[0].set_title(f'{stock}: Close Price')\n",
    "    legend = axes[0].legend(loc='upper left')\n",
    "    legend.get_frame().set_alpha(0.3)\n",
    "\n",
    "    # Plot 7-days delay trend\n",
    "    axes[1].plot(full_data['Date'], full_data[f'Delay_{delay}'], label = f'{delay}-Days Delayed Trend', color = 'black')\n",
    "    axes[1].set_xlabel('Date')\n",
    "    axes[1].set_ylabel(f'{delay}-Days Delayed Trend')\n",
    "    axes[1].set_title(f'{stock}: {delay}-Days Delayed Trend')\n",
    "    legend = axes[1].legend(loc='upper right')\n",
    "    legend.get_frame().set_alpha(0.3)\n",
    "\n",
    "    fig.suptitle(f'{name} ({stock})', fontsize=20, verticalalignment = 'bottom', fontweight = 'bold')\n",
    "    plt.tight_layout(pad=2.0)\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    if download:\n",
    "        plt.savefig(f\"./Plots/{stock}_plot({start} - {end}).png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time_jump\n",
    "adds time in days to a given date that was accepted as string, returns as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the date in string format after a certain number of days\n",
    "def time_jump(start, days = 7 * 38):\n",
    "    return (datetime.datetime.strptime(start, '%Y-%m-%d') + datetime.timedelta(days = days)).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_breakpoints\n",
    "calculates the breakpoints needed for the multiple requests of data to make the data as long as possible for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_breakpoints(start, end, days = 7 * 38):\n",
    "    breakpoints = [start]\n",
    "    while datetime.datetime.strptime(breakpoints[-1], '%Y-%m-%d') < datetime.datetime.strptime(end, '%Y-%m-%d'):\n",
    "        temp = time_jump(breakpoints[-1], days)\n",
    "        if datetime.datetime.strptime(temp, '%Y-%m-%d') < datetime.datetime.strptime(end, '%Y-%m-%d'):\n",
    "            breakpoints.append(temp)\n",
    "        else:\n",
    "            breakpoints.append(end)\n",
    "    return breakpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## connectNnormalizeTrends\n",
    "gets the data of the trends and prices daily in the whole time frame and in parts, and connects them by estimating an eproximation of their absolute amount of searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connectNnormalizeTrends(Dfs, stock):\n",
    "    # setting up the data for step 1\n",
    "    if not os.path.exists(f'./Data/glimpse_{stock}_5Y.csv'):\n",
    "        raise Exception(f'Failed to retrieve Glimpse Data of {stock}.')\n",
    "    glmpsDf = pd.read_csv(f'./Data/glimpse_{stock}_5Y.csv')\n",
    "    glmpsDf.rename(columns={'Time (week of)': 'Date', 'Absolute Google Search Volume': 'Absolute_Volume'}, inplace=True)\n",
    "    glmpsDf['Date'] = pd.to_datetime(glmpsDf['Date'])\n",
    "\n",
    "    df_concat = pd.concat(Dfs).reset_index(drop = True)\n",
    "    df_concat['Date'] = pd.to_datetime(df_concat['Date'])\n",
    "\n",
    "    # calculating the mean trend for each week and merging it into glmpsDf\n",
    "    df_concat['MeanTrend'] = df_concat['Trend'].rolling(window=7, min_periods=1).mean().shift(-6)\n",
    "    glmpsDf = pd.merge(glmpsDf, df_concat, on='Date', how='left').drop(columns = ['Trend'])\n",
    "\n",
    "    # setting up the data for step 2\n",
    "    glmpsDf.rename(columns={'Date': 'Date_Week'}, inplace=True)\n",
    "    df_concat = df_concat.drop(columns = ['MeanTrend'])\n",
    "    df_concat['Date_Week'] = (df_concat['Date'] - pd.to_timedelta((df_concat['Date'].dt.weekday + 1) % 7, unit='d')).dt.strftime('%Y-%m-%d')\n",
    "    df_concat['Date_Week'] = pd.to_datetime(df_concat['Date_Week'])\n",
    "\n",
    "    # calculating the ratio and search volume for each week\n",
    "    df_concat = pd.merge(df_concat, glmpsDf[['Date_Week', 'MeanTrend', 'Absolute_Volume']], on='Date_Week', how='left')\n",
    "    df_concat['Ratio'] = df_concat['Trend'] / (df_concat['MeanTrend'] * 7)\n",
    "    df_concat['Search_Volume'] = df_concat['Ratio'] * df_concat['Absolute_Volume']\n",
    "\n",
    "    # adding to df_concat the check ratio to check validity of the data\n",
    "    df_concat['check_ratio'] = df_concat['Search_Volume'] / df_concat['Trend']\n",
    "\n",
    "    # renormalizing the data\n",
    "    df_concat['Normalized_Searches'] = ((df_concat['Search_Volume'] - df_concat['Search_Volume'].min()) / (df_concat['Search_Volume'].max() - df_concat['Search_Volume'].min())) * 100\n",
    "\n",
    "    # cleaning out unnecessary columns\n",
    "    df_concat = df_concat.drop(columns = ['Trend', 'Date_Week', 'MeanTrend', 'Absolute_Volume', 'Ratio'])\n",
    "\n",
    "    df_concat['Date'] = df_concat['Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getNormalizedData\n",
    "gets the data normalized, and performs validity checks (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNormalizedData(stock, ticker, start = '2019-06-23', end = '2024-06-01', weeks = 38, do_double = True, verbose = False):\n",
    "    if weeks > 38:\n",
    "        wrn.warn('The maximum number of weeks is 38. \\nThe number of weeks will be set to 38.', category=Warning)\n",
    "        weeks = 38\n",
    "    if do_double & (weeks % 2 != 0):\n",
    "        wrn.warn('The number of weeks must be even to use the double method. \\nThe number of weeks will be rounded down to the nearest even number.', category=Warning)\n",
    "        weeks -= 1\n",
    "    breakpoints = get_breakpoints(start, end, days = 7 * weeks)\n",
    "    if do_double:\n",
    "        breakpoints2 = get_breakpoints(start, end, days = 7 * weeks / 2)\n",
    "        breakpoints2 = [x for x in breakpoints2 if x not in breakpoints[1:-1]]\n",
    "    \n",
    "    # initializing the list to store the dataframes\n",
    "    Dfs = []\n",
    "\n",
    "    # extracting the data for each time period\n",
    "    for i in range(len(breakpoints) - 1):\n",
    "        startTemp = breakpoints[i]\n",
    "        endTemp = time_jump(breakpoints[i + 1], days=-1)\n",
    "        # checking if the data is already extracted\n",
    "        if not os.path.exists(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\"):\n",
    "            # extracting the data\n",
    "            t = get_trends_data([stock], timeframe = f\"{startTemp} {endTemp}\", verbose = verbose).drop(columns = ['isPartial'])\n",
    "            if t is None:\n",
    "                raise Exception(f'Failed to retrieve Trend Data of {stock}.')\n",
    "            t.to_csv(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\")\n",
    "        else:\n",
    "            t = pd.read_csv(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\").drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "        Dfs.append(t)\n",
    "\n",
    "    df_concat = connectNnormalizeTrends(Dfs, stock)\n",
    "\n",
    "    if do_double:\n",
    "        # initializing the list to store the dataframes\n",
    "        Dfs2 = []\n",
    "\n",
    "        # extracting the data for each time period\n",
    "        for i in range(len(breakpoints2) - 1):\n",
    "            startTemp = breakpoints2[i]\n",
    "            endTemp = time_jump(breakpoints2[i + 1], days=-1)\n",
    "            # checking if the data is already extracted\n",
    "            if not os.path.exists(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\"):\n",
    "                # extracting the data\n",
    "                t = get_trends_data([stock], timeframe = f\"{startTemp} {endTemp}\", verbose = verbose).drop(columns = ['isPartial'])\n",
    "                if t is None:\n",
    "                    raise Exception(f'Failed to retrieve Trend Data of {stock}.')\n",
    "                t.to_csv(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\")\n",
    "            else:\n",
    "                t = pd.read_csv(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\").drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "            Dfs2.append(t)\n",
    "        \n",
    "        df_concat2 = connectNnormalizeTrends(Dfs2, stock)\n",
    "    \n",
    "        df_concat[\"Normalized_Searches\"] = (df_concat[\"Normalized_Searches\"] + df_concat2[\"Normalized_Searches\"]) / 2\n",
    "    \n",
    "    if not os.path.exists(f\"./Data/{stock}_Prices({start}-{end}).csv\"):\n",
    "        stockData = get_stock_data(f'{ticker}-USD', start = start, end = end, verbose = verbose)\n",
    "        stockData.to_csv(f\"./Data/{stock}_Prices({start}-{end}).csv\")\n",
    "    else:\n",
    "        stockData = pd.read_csv(f\"./Data/{stock}_Prices({start}-{end}).csv\").drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "    df_concat[df_concat.Normalized_Searches == 0] = 0.1\n",
    "\n",
    "    df_concat['log_searches'] = np.log(df_concat.Normalized_Searches / df_concat.Normalized_Searches.shift(1))\n",
    "    stockData['log_returns'] = np.log(stockData.Close / stockData.Close.shift(1))\n",
    "\n",
    "    try:\n",
    "        df_concat['Date'] = df_concat['Date'].dt.strftime('%Y-%m-%d')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        stockData['Date'] = stockData['Date'].dt.strftime('%Y-%m-%d')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    finalDf = pd.merge(stockData, df_concat, on='Date', how='left').dropna()\n",
    "\n",
    "    if verbose:\n",
    "        # data normalization validity check\n",
    "        top = 0\n",
    "        bottom = 0\n",
    "        for i in range(len(Dfs)):\n",
    "            top += Dfs[i].shape[0]\n",
    "            print(f'period {i + 1}:\\n=========\\nmean: {df_concat.iloc[bottom:top]['check_ratio'].mean():.4f}\\nsd: {df_concat.iloc[bottom:top][\"check_ratio\"].std():.4f}\\n\\nmean/sd: {df_concat.iloc[bottom:top]['check_ratio'].mean()/df_concat.iloc[bottom:top][\"check_ratio\"].std():.4f}\\n')\n",
    "            bottom += Dfs[i].shape[0]\n",
    "\n",
    "        if do_double:\n",
    "            top = 0\n",
    "            bottom = 0\n",
    "            for i in range(len(Dfs2)):\n",
    "                top += Dfs2[i].shape[0]\n",
    "                print(f'period {i + len(Dfs) + 1}:\\n=========\\nmean: {df_concat.iloc[bottom:top]['check_ratio'].mean():.4f}\\nsd: {df_concat.iloc[bottom:top][\"check_ratio\"].std():.4f}\\n\\nmean/sd: {df_concat.iloc[bottom:top]['check_ratio'].mean()/df_concat.iloc[bottom:top][\"check_ratio\"].std():.4f}\\n')\n",
    "                bottom += Dfs2[i].shape[0]\n",
    "    return finalDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtesting\n",
    "## Collecting The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrn.filterwarnings('ignore', category=UserWarning)\n",
    "wrn.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "coinsDFs = []\n",
    "\n",
    "for i in range(len(decentralized_currencies)):\n",
    "    coinsDFs.append(getNormalizedData(decentralized_currencies_names[i], decentralized_currencies[i], \n",
    "                                      start = start, end = end, \n",
    "                                      do_double = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Granger Causality Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitcoin: 7 lags, p-value = 0.1163\n",
      "Ethereum: 1 lags, p-value = 0.1807\n",
      "Cardano: 2 lags, p-value = 0.0776\n",
      "Solana: 3 lags, p-value = 0.0782\n",
      "Ripple: 1 lags, p-value = 0.5286\n",
      "Monero: 1 lags, p-value = 0.1025\n",
      "Litecoin: 2 lags, p-value = 0.2933\n",
      "Polkadot: 3 lags, p-value = 0.0056\n",
      "Chainlink: 1 lags, p-value = 0.0093\n",
      "Tezos: 3 lags, p-value = 0.4993\n",
      "Dogecoin: 5 lags, p-value = 0.0005\n",
      "Shiba Inu: 2 lags, p-value = 0.0000\n"
     ]
    }
   ],
   "source": [
    "wrn.filterwarnings('ignore', category=UserWarning)\n",
    "wrn.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "lags = []\n",
    "\n",
    "for i in range(len(decentralized_currencies)):\n",
    "    cause = grangercausalitytests(coinsDFs[i][['log_returns', 'log_searches']], \n",
    "                                  maxlag = max_lags, \n",
    "                                  verbose = False)\n",
    "\n",
    "    min_p_value = float('inf')\n",
    "    min_p_lag = None\n",
    "\n",
    "    for lag, result in cause.items():\n",
    "        p_value = result[0]['ssr_ftest'][1]\n",
    "        if p_value < min_p_value:\n",
    "            min_p_value = p_value\n",
    "            min_p_lag = lag\n",
    "\n",
    "    lags.append(min_p_lag)\n",
    "\n",
    "    coinsDFs[i]['Normalized_Searches_delayed'] = coinsDFs[i]['Normalized_Searches'].shift(min_p_lag)\n",
    "    coinsDFs[i]['log_searches_delayed'] = coinsDFs[i]['log_searches'].shift(min_p_lag)\n",
    "    coinsDFs[i].dropna(inplace = True)\n",
    "\n",
    "    print(f'{decentralized_currencies_names[i]}: {min_p_lag} lags, p-value = {min_p_value:.4f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples for checks code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see data validity checks of a certain stock\n",
    "stock_name = 'Bitcoin' # change to any stock name that is in the list at the top\n",
    "stock_ticker = 'BTC' # change to the corresponding ticker of the stock\n",
    "\n",
    "getNormalizedData(stock_name, stock_ticker, \n",
    "                  start = start, end = end, \n",
    "                  do_double = True, verbose = True)\n",
    "\n",
    "# see the granger causality test results of a certain stock\n",
    "df = getNormalizedData(stock_name, stock_ticker, start = start, end = end, do_double = True)\n",
    "\n",
    "grangercausalitytests(df[['log_returns', 'log_searches']], \n",
    "                      maxlag = max_lags, \n",
    "                      verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtesting\n",
    "## Data Split\n",
    "* 4 years train\n",
    "* 1 year test\n",
    "\n",
    "## Steps\n",
    "1. get the data normalized\n",
    "2. perform granger causality test on the train data (first 1461 samples) to determine the appropriate lag\n",
    "3. calculate the \n",
    "\n",
    "### hyper-parameters:\n",
    "* bb_window: size of the window of MA of the normalized searches and the bollinger bands on that MA\n",
    "* bb_window_min: minimum size of the window of MA of the normalized searches and the bollinger bands on that MA\n",
    "* bb_threshold: how many sds should be added to/subtracted from the bollinger bands\n",
    "* rsi_window: size of the window of the RSI on the Close\n",
    "* rsi_window_min: minimum size of the window of RSI on the Close\n",
    "* rsi_limits: how sensitive the rsi is, \n",
    "    - the higher the lower value (the first value of rsi_limits) the less the RSI blocks a BUY signal\n",
    "    - the lower the higher value (the second value of rsi_limits) the less the RSI blocks a SELL signal\n",
    "* sell_all: a boolian value that indicates whether the strategy says to sell all of the quantity of the asset when it gives a SELL signal or the strategy says to sell some relative amount of the quantity when it gives a SELL signal\n",
    "* qty_scale: represents the scale of the quantity that is calculated by the strategy\n",
    "\n",
    "### additional parameters:\n",
    "* starting_balance: the starting balance\n",
    "* commission: the commission for every market dealing\n",
    "* commission_type: whether the commission is a set amount or some precentage from the price\n",
    "* slippage_factor: how much slippage exists in the market "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enums\n",
    "class ActionType(Enum):\n",
    "    BUY = 1\n",
    "    SELL = -1\n",
    "    DONOTHING = 0\n",
    "\n",
    "# strategy class\n",
    "class Strategy():\n",
    "    def __init__(self, bb_window: int, rsi_window: int, \n",
    "                 bb_window_min: int=None, bb_threshold: float=0.5, rsi_window_min: int=None, rsi_limits: Tuple[float,float]=[30.0, 70.0], sell_all: bool=False, qty_scale: float=0.1,\n",
    "                 commission_type: object=\"scalar\", slippage_factor=np.inf) -> None:\n",
    "        self.bb_window = bb_window\n",
    "        self.rsi_window = rsi_window\n",
    "        if bb_window_min is None:\n",
    "            bb_window_min = bb_window\n",
    "        else:\n",
    "            if bb_window_min > bb_window:\n",
    "                raise ValueError(\"bb_window_min should be less than or equal to bb_window\")\n",
    "            if bb_window_min <= 0:\n",
    "                raise ValueError(\"bb_window_min should be a value greater than 0\")\n",
    "            self.bb_window_min = bb_window_min\n",
    "        if bb_threshold <= 0:\n",
    "            raise ValueError(\"bb_threshold should be a value greater than 0\")\n",
    "        self.bb_threshold = bb_threshold\n",
    "        if rsi_window_min is None:\n",
    "            rsi_window_min = rsi_window\n",
    "        else:\n",
    "            if rsi_window_min > rsi_window:\n",
    "                raise ValueError(\"rsi_window_min should be less than or equal to rsi_window\")\n",
    "            if rsi_window_min <= 0:\n",
    "                raise ValueError(\"rsi_window_min should be a value greater than 0\")\n",
    "            self.rsi_window_min = rsi_window_min\n",
    "        if rsi_limits[0] <= 0 or rsi_limits[1] > 100 or rsi_limits[0] >= rsi_limits[1]:\n",
    "            raise ValueError(\"rsi_limit should be a tuple of two values between 0 and 100 where the first value is less than the second\")\n",
    "        self.rsi_limits = rsi_limits\n",
    "        if commission_type not in [\"scalar\", \"precentage\"]:\n",
    "            raise ValueError(\"commision_type should be either 'scalar' or 'precentage'\")\n",
    "        self.commission_type = commission_type\n",
    "        self.sell_all = sell_all\n",
    "        self.slippage_factor = slippage_factor\n",
    "        if qty_scale <= 0:\n",
    "            raise ValueError(\"qty_scale should be a value greater than 0\")\n",
    "        self.qty_scale = qty_scale\n",
    "\n",
    "    def calc_signal(self, data: pd.DataFrame) -> None:\n",
    "        norm_search = data['Normalized_Searches_delayed']\n",
    "        close = data['Close']\n",
    "\n",
    "        # MA of search volume as threshold for Bollinger Bands\n",
    "        norm_search_MA = norm_search.rolling(window=self.bb_window, min_periods=self.bb_window_min).mean()\n",
    "        norm_search_sd = norm_search.rolling(window=self.bb_window, min_periods=self.bb_window_min).std()\n",
    "        upper_band = norm_search_MA + self.bb_threshold * norm_search_sd\n",
    "        lower_band = norm_search_MA - self.bb_threshold * norm_search_sd\n",
    "\n",
    "        data['norm_search_MA_upper_band'] = upper_band\n",
    "        data['norm_search_MA_lower_band'] = lower_band\n",
    "\n",
    "        # RSI of close price as threshold limiting the Bollinger Bands signal\n",
    "        delta = close.diff()\n",
    "        gains = delta.where(delta > 0, 0)\n",
    "        losses = -delta.where(delta < 0, 0)\n",
    "        avg_gain = gains.rolling(window=self.rsi_window, min_periods=self.rsi_window_min).mean()\n",
    "        avg_loss = losses.rolling(window=self.rsi_window, min_periods=self.rsi_window_min).mean()\n",
    "        rs = avg_gain / avg_loss\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "        data['strategy'] = 0\n",
    "        data.loc[(norm_search < lower_band) & (rsi < self.rsi_limits[0]), 'strategy'] = -1 # Sell signal\n",
    "        data.loc[(norm_search > upper_band) & (rsi > self.rsi_limits[1]), 'strategy'] = 1 # Buy signal\n",
    "\n",
    "    def calc_realistic_price(self, row: pd.Series, action: ActionType) -> float:\n",
    "        slippage_rate = ((row['Close'] - row['Open']) / row['Open']) / self.slippage_factor\n",
    "        slippage_price = row['Open'] + row['Open'] * slippage_rate\n",
    "\n",
    "        if action == ActionType.BUY:\n",
    "            return max(slippage_price, row['Open'])\n",
    "        else:\n",
    "            return min(slippage_price, row['Open'])\n",
    "\n",
    "    def backtest(self, data: pd.DataFrame, starting_balance: float, commission: float=0.0) -> pd.DataFrame:\n",
    "        data['qty'] = 0.0\n",
    "        data['balance'] = 0.0\n",
    "\n",
    "        self.calc_signal(data)\n",
    "\n",
    "        data.reset_index(inplace=True)\n",
    "\n",
    "        for i, row in data.iterrows():\n",
    "            # Get the current balance and qty before the action\n",
    "            curr_qty = data.loc[i - 1, 'qty'] if i > 0 else 0.0\n",
    "            curr_balance = data.loc[i - 1, 'balance'] if i > 0 else starting_balance\n",
    "\n",
    "            # Buy signal when strategy says so as long not in the end of trade data and not holding any stock\n",
    "            if (i != data.shape[0] - 1) & (data.loc[i, 'strategy'] == 1):\n",
    "                buy_price = self.calc_realistic_price(row, ActionType.BUY)\n",
    "                buy_qty = (row['Normalized_Searches_delayed'] / row['norm_search_MA_upper_band'] - 1) * self.qty_scale\n",
    "                data.loc[i, 'balance'] = curr_balance - (buy_price * buy_qty + commission if self.commission_type == \"scalar\" else buy_price * buy_qty * (1 + commission))\n",
    "                data.loc[i, 'qty'] = curr_qty + buy_qty\n",
    "                holding = True\n",
    "            \n",
    "            # Sell signal when strategy says so or at the end of trade and holding any stock\n",
    "            elif ((data.loc[i, 'strategy'] == -1) | (i == data.shape[0] - 1)) & (curr_qty > 0):\n",
    "                sell_price = self.calc_realistic_price(row, ActionType.SELL)\n",
    "                sell_qty = curr_qty if (self.sell_all | (i == data.shape[0] - 1)) else min((row['norm_search_MA_lower_band'] / row['Normalized_Searches_delayed'] - 1) * self.qty_scale, curr_qty)\n",
    "                data.loc[i, 'balance'] = curr_balance + (sell_price * sell_qty - commission if self.commission_type == \"scalar\" else sell_price * sell_qty * (1 - commission))\n",
    "                data.loc[i, 'qty'] = 0.0 if (self.sell_all | (i == data.shape[0] - 1)) else curr_qty - sell_qty\n",
    "                holding = False\n",
    "            \n",
    "            # Do nothing\n",
    "            else:\n",
    "                data.loc[i, 'balance'] = curr_balance\n",
    "                data.loc[i, 'qty'] = curr_qty\n",
    "            \n",
    "        data['portfolio_value'] = data['balance'] + data['Close'] * data['qty']\n",
    "\n",
    "        return data\n",
    "\n",
    "def calc_total_return(portfolio_values):\n",
    "    return (portfolio_values.iloc[-1] / portfolio_values.iloc[0]) - 1.0\n",
    "\n",
    "def calc_annualized_return(portfolio_values):\n",
    "    yearly_trading_days = 252\n",
    "    portfolio_trading_days = portfolio_values.shape[0]\n",
    "    portfolio_trading_years = portfolio_trading_days / yearly_trading_days \n",
    "    return (portfolio_values.iloc[-1] / portfolio_values.iloc[0])**(1/portfolio_trading_years) - 1.0\n",
    "\n",
    "def calc_annualized_sharpe(portfolio_values: pd.Series, rf: float=0.0):\n",
    "    yearly_trading_days = 252\n",
    "    annualized_return = calc_annualized_return(portfolio_values)\n",
    "    annualized_std = portfolio_values.pct_change().std() * np.sqrt(yearly_trading_days)\n",
    "    if annualized_std is None or annualized_std == 0:\n",
    "        return 0\n",
    "    sharpe = (annualized_return - rf) / annualized_std\n",
    "    return sharpe\n",
    "\n",
    "def calc_downside_deviation(portfolio_values):\n",
    "    porfolio_returns = portfolio_values.pct_change().dropna()\n",
    "    return porfolio_returns[porfolio_returns < 0].std()\n",
    "\n",
    "def calc_sortino(portfolio_values, rf=0.0):\n",
    "    yearly_trading_days = 252\n",
    "    down_deviation = calc_downside_deviation(portfolio_values) * np.sqrt(yearly_trading_days)\n",
    "    annualized_return = calc_annualized_return(portfolio_values)\n",
    "    if down_deviation is None or down_deviation == 0:\n",
    "        return 0\n",
    "    sortino = (annualized_return - rf) / down_deviation\n",
    "    return sortino\n",
    "\n",
    "def calc_max_drawdown(portfolio_values):\n",
    "    cumulative_max = portfolio_values.cummax()\n",
    "    drawdown = (cumulative_max - portfolio_values) / cumulative_max\n",
    "    return drawdown.max()\n",
    "\n",
    "def calc_calmar(portfolio_values):\n",
    "    max_drawdown = calc_max_drawdown(portfolio_values)\n",
    "    annualized_return = calc_annualized_return(portfolio_values)\n",
    "    return annualized_return / max_drawdown\n",
    "\n",
    "def evaluate_strategy(b_df, strat_name):\n",
    "    total_return = calc_total_return(b_df['portfolio_value'])\n",
    "    annualized_return = calc_annualized_return(b_df['portfolio_value'])\n",
    "    annualized_sharpe = calc_annualized_sharpe(b_df['portfolio_value'])\n",
    "    sortino_ratio = calc_sortino(b_df['portfolio_value'])\n",
    "    max_drawdown = calc_max_drawdown(b_df['portfolio_value'])\n",
    "    calmar_ratio = calc_calmar(b_df['portfolio_value'])\n",
    "    \n",
    "    print(f\"Results for {strat_name}:\")\n",
    "    print(f\"Total Return: {total_return:.2%}\")\n",
    "    print(f\"Annualized Return: {annualized_return:.2%}\")\n",
    "    print(f\"Annualized Sharpe Ratio: {annualized_sharpe:.2f}\")\n",
    "    print(f\"Sortino Ratio: {sortino_ratio:.2f}\")\n",
    "    print(f\"Max Drawdown: {max_drawdown:.2%}\")\n",
    "    print(f\"Calmar Ratio: {calmar_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_strategy = Strategy(bb_window=14, rsi_window=14, bb_window_min=1, bb_threshold=0.5, rsi_window_min=1, rsi_limits=[30.0, 70.0], sell_all=False, qty_scale=0.1, commission_type=\"scalar\", slippage_factor=1.0)\n",
    "tempDF = example_strategy.backtest(coinsDFs[0].copy(), balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>log_returns</th>\n",
       "      <th>Search_Volume</th>\n",
       "      <th>...</th>\n",
       "      <th>Normalized_Searches</th>\n",
       "      <th>log_searches</th>\n",
       "      <th>Normalized_Searches_delayed</th>\n",
       "      <th>log_searches_delayed</th>\n",
       "      <th>qty</th>\n",
       "      <th>balance</th>\n",
       "      <th>norm_search_MA_upper_band</th>\n",
       "      <th>norm_search_MA_lower_band</th>\n",
       "      <th>strategy</th>\n",
       "      <th>portfolio_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>2019-07-08</td>\n",
       "      <td>11446.596680</td>\n",
       "      <td>12345.833008</td>\n",
       "      <td>11393.374023</td>\n",
       "      <td>12285.958008</td>\n",
       "      <td>12285.958008</td>\n",
       "      <td>23482551458</td>\n",
       "      <td>0.070393</td>\n",
       "      <td>284665.003145</td>\n",
       "      <td>...</td>\n",
       "      <td>6.298411</td>\n",
       "      <td>0.440920</td>\n",
       "      <td>10.119808</td>\n",
       "      <td>0.218669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>2019-07-09</td>\n",
       "      <td>12284.326172</td>\n",
       "      <td>12779.131836</td>\n",
       "      <td>12233.261719</td>\n",
       "      <td>12573.812500</td>\n",
       "      <td>12573.812500</td>\n",
       "      <td>28167921523</td>\n",
       "      <td>0.023159</td>\n",
       "      <td>324385.701258</td>\n",
       "      <td>...</td>\n",
       "      <td>7.963752</td>\n",
       "      <td>0.234603</td>\n",
       "      <td>10.200057</td>\n",
       "      <td>0.007899</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>9998.549613</td>\n",
       "      <td>10.188305</td>\n",
       "      <td>10.131560</td>\n",
       "      <td>1</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>2019-07-10</td>\n",
       "      <td>12571.537109</td>\n",
       "      <td>13129.529297</td>\n",
       "      <td>11710.978516</td>\n",
       "      <td>12156.512695</td>\n",
       "      <td>12156.512695</td>\n",
       "      <td>33627574244</td>\n",
       "      <td>-0.033751</td>\n",
       "      <td>364106.399371</td>\n",
       "      <td>...</td>\n",
       "      <td>9.553251</td>\n",
       "      <td>0.181981</td>\n",
       "      <td>8.972541</td>\n",
       "      <td>-0.128224</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>9998.549613</td>\n",
       "      <td>10.107493</td>\n",
       "      <td>9.420778</td>\n",
       "      <td>0</td>\n",
       "      <td>9999.951865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>2019-07-11</td>\n",
       "      <td>12139.713867</td>\n",
       "      <td>12144.623047</td>\n",
       "      <td>11158.922852</td>\n",
       "      <td>11358.662109</td>\n",
       "      <td>11358.662109</td>\n",
       "      <td>28595327690</td>\n",
       "      <td>-0.067884</td>\n",
       "      <td>337625.933962</td>\n",
       "      <td>...</td>\n",
       "      <td>8.392463</td>\n",
       "      <td>-0.129547</td>\n",
       "      <td>7.745025</td>\n",
       "      <td>-0.147118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9999.859833</td>\n",
       "      <td>9.836763</td>\n",
       "      <td>8.681953</td>\n",
       "      <td>-1</td>\n",
       "      <td>9999.859833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>2019-07-12</td>\n",
       "      <td>11354.299805</td>\n",
       "      <td>11905.487305</td>\n",
       "      <td>11179.144531</td>\n",
       "      <td>11815.986328</td>\n",
       "      <td>11815.986328</td>\n",
       "      <td>23534692797</td>\n",
       "      <td>0.039473</td>\n",
       "      <td>304525.352201</td>\n",
       "      <td>...</td>\n",
       "      <td>7.093161</td>\n",
       "      <td>-0.168203</td>\n",
       "      <td>6.984886</td>\n",
       "      <td>-0.103302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9999.859833</td>\n",
       "      <td>9.517702</td>\n",
       "      <td>8.091225</td>\n",
       "      <td>0</td>\n",
       "      <td>9999.859833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>1822</td>\n",
       "      <td>2024-06-25</td>\n",
       "      <td>60266.281250</td>\n",
       "      <td>62258.261719</td>\n",
       "      <td>60239.750000</td>\n",
       "      <td>61804.640625</td>\n",
       "      <td>61804.640625</td>\n",
       "      <td>29201215431</td>\n",
       "      <td>0.025021</td>\n",
       "      <td>336946.076087</td>\n",
       "      <td>...</td>\n",
       "      <td>8.348660</td>\n",
       "      <td>-0.110554</td>\n",
       "      <td>7.453743</td>\n",
       "      <td>0.055626</td>\n",
       "      <td>3.542072</td>\n",
       "      <td>-98191.512122</td>\n",
       "      <td>7.908932</td>\n",
       "      <td>6.313679</td>\n",
       "      <td>0</td>\n",
       "      <td>120724.985091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>1823</td>\n",
       "      <td>2024-06-26</td>\n",
       "      <td>61789.675781</td>\n",
       "      <td>62434.136719</td>\n",
       "      <td>60695.187500</td>\n",
       "      <td>60811.277344</td>\n",
       "      <td>60811.277344</td>\n",
       "      <td>22506003064</td>\n",
       "      <td>-0.016203</td>\n",
       "      <td>293469.163043</td>\n",
       "      <td>...</td>\n",
       "      <td>6.570504</td>\n",
       "      <td>-0.239511</td>\n",
       "      <td>6.647134</td>\n",
       "      <td>-0.114531</td>\n",
       "      <td>3.542072</td>\n",
       "      <td>-98191.512122</td>\n",
       "      <td>7.568608</td>\n",
       "      <td>6.188879</td>\n",
       "      <td>0</td>\n",
       "      <td>117206.420663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>1824</td>\n",
       "      <td>2024-06-27</td>\n",
       "      <td>60811.226562</td>\n",
       "      <td>62293.863281</td>\n",
       "      <td>60585.332031</td>\n",
       "      <td>61604.800781</td>\n",
       "      <td>61604.800781</td>\n",
       "      <td>21231745045</td>\n",
       "      <td>0.012965</td>\n",
       "      <td>271730.706522</td>\n",
       "      <td>...</td>\n",
       "      <td>5.768274</td>\n",
       "      <td>-0.130218</td>\n",
       "      <td>6.243830</td>\n",
       "      <td>-0.062592</td>\n",
       "      <td>3.542072</td>\n",
       "      <td>-98191.512122</td>\n",
       "      <td>7.260522</td>\n",
       "      <td>6.061894</td>\n",
       "      <td>0</td>\n",
       "      <td>120017.137943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>1825</td>\n",
       "      <td>2024-06-28</td>\n",
       "      <td>61612.804688</td>\n",
       "      <td>62126.097656</td>\n",
       "      <td>59985.402344</td>\n",
       "      <td>60320.136719</td>\n",
       "      <td>60320.136719</td>\n",
       "      <td>24952866877</td>\n",
       "      <td>-0.021074</td>\n",
       "      <td>271730.706522</td>\n",
       "      <td>...</td>\n",
       "      <td>5.768274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.647134</td>\n",
       "      <td>0.062592</td>\n",
       "      <td>3.542072</td>\n",
       "      <td>-98191.512122</td>\n",
       "      <td>6.901861</td>\n",
       "      <td>6.016571</td>\n",
       "      <td>0</td>\n",
       "      <td>115466.765127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>1826</td>\n",
       "      <td>2024-06-29</td>\n",
       "      <td>60319.875000</td>\n",
       "      <td>61097.621094</td>\n",
       "      <td>60300.964844</td>\n",
       "      <td>60887.378906</td>\n",
       "      <td>60887.378906</td>\n",
       "      <td>12652903396</td>\n",
       "      <td>0.009360</td>\n",
       "      <td>228253.793478</td>\n",
       "      <td>...</td>\n",
       "      <td>3.990118</td>\n",
       "      <td>-0.368552</td>\n",
       "      <td>4.858031</td>\n",
       "      <td>-0.313553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>115465.838100</td>\n",
       "      <td>6.792803</td>\n",
       "      <td>5.824811</td>\n",
       "      <td>-1</td>\n",
       "      <td>115465.838100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1819 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index        Date          Open          High           Low  \\\n",
       "0         8  2019-07-08  11446.596680  12345.833008  11393.374023   \n",
       "1         9  2019-07-09  12284.326172  12779.131836  12233.261719   \n",
       "2        10  2019-07-10  12571.537109  13129.529297  11710.978516   \n",
       "3        11  2019-07-11  12139.713867  12144.623047  11158.922852   \n",
       "4        12  2019-07-12  11354.299805  11905.487305  11179.144531   \n",
       "...     ...         ...           ...           ...           ...   \n",
       "1814   1822  2024-06-25  60266.281250  62258.261719  60239.750000   \n",
       "1815   1823  2024-06-26  61789.675781  62434.136719  60695.187500   \n",
       "1816   1824  2024-06-27  60811.226562  62293.863281  60585.332031   \n",
       "1817   1825  2024-06-28  61612.804688  62126.097656  59985.402344   \n",
       "1818   1826  2024-06-29  60319.875000  61097.621094  60300.964844   \n",
       "\n",
       "             Close     Adj Close       Volume  log_returns  Search_Volume  \\\n",
       "0     12285.958008  12285.958008  23482551458     0.070393  284665.003145   \n",
       "1     12573.812500  12573.812500  28167921523     0.023159  324385.701258   \n",
       "2     12156.512695  12156.512695  33627574244    -0.033751  364106.399371   \n",
       "3     11358.662109  11358.662109  28595327690    -0.067884  337625.933962   \n",
       "4     11815.986328  11815.986328  23534692797     0.039473  304525.352201   \n",
       "...            ...           ...          ...          ...            ...   \n",
       "1814  61804.640625  61804.640625  29201215431     0.025021  336946.076087   \n",
       "1815  60811.277344  60811.277344  22506003064    -0.016203  293469.163043   \n",
       "1816  61604.800781  61604.800781  21231745045     0.012965  271730.706522   \n",
       "1817  60320.136719  60320.136719  24952866877    -0.021074  271730.706522   \n",
       "1818  60887.378906  60887.378906  12652903396     0.009360  228253.793478   \n",
       "\n",
       "      ...  Normalized_Searches  log_searches  Normalized_Searches_delayed  \\\n",
       "0     ...             6.298411      0.440920                    10.119808   \n",
       "1     ...             7.963752      0.234603                    10.200057   \n",
       "2     ...             9.553251      0.181981                     8.972541   \n",
       "3     ...             8.392463     -0.129547                     7.745025   \n",
       "4     ...             7.093161     -0.168203                     6.984886   \n",
       "...   ...                  ...           ...                          ...   \n",
       "1814  ...             8.348660     -0.110554                     7.453743   \n",
       "1815  ...             6.570504     -0.239511                     6.647134   \n",
       "1816  ...             5.768274     -0.130218                     6.243830   \n",
       "1817  ...             5.768274      0.000000                     6.647134   \n",
       "1818  ...             3.990118     -0.368552                     4.858031   \n",
       "\n",
       "      log_searches_delayed       qty        balance  \\\n",
       "0                 0.218669  0.000000   10000.000000   \n",
       "1                 0.007899  0.000115    9998.549613   \n",
       "2                -0.128224  0.000115    9998.549613   \n",
       "3                -0.147118  0.000000    9999.859833   \n",
       "4                -0.103302  0.000000    9999.859833   \n",
       "...                    ...       ...            ...   \n",
       "1814              0.055626  3.542072  -98191.512122   \n",
       "1815             -0.114531  3.542072  -98191.512122   \n",
       "1816             -0.062592  3.542072  -98191.512122   \n",
       "1817              0.062592  3.542072  -98191.512122   \n",
       "1818             -0.313553  0.000000  115465.838100   \n",
       "\n",
       "      norm_search_MA_upper_band  norm_search_MA_lower_band  strategy  \\\n",
       "0                           NaN                        NaN         0   \n",
       "1                     10.188305                  10.131560         1   \n",
       "2                     10.107493                   9.420778         0   \n",
       "3                      9.836763                   8.681953        -1   \n",
       "4                      9.517702                   8.091225         0   \n",
       "...                         ...                        ...       ...   \n",
       "1814                   7.908932                   6.313679         0   \n",
       "1815                   7.568608                   6.188879         0   \n",
       "1816                   7.260522                   6.061894         0   \n",
       "1817                   6.901861                   6.016571         0   \n",
       "1818                   6.792803                   5.824811        -1   \n",
       "\n",
       "      portfolio_value  \n",
       "0        10000.000000  \n",
       "1        10000.000000  \n",
       "2         9999.951865  \n",
       "3         9999.859833  \n",
       "4         9999.859833  \n",
       "...               ...  \n",
       "1814    120724.985091  \n",
       "1815    117206.420663  \n",
       "1816    120017.137943  \n",
       "1817    115466.765127  \n",
       "1818    115465.838100  \n",
       "\n",
       "[1819 rows x 21 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previously used chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trend Data for ['Shiba Inu'] retrieved successfully.\n",
      "Stock Data for SHIB-USD retrieved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Calculate correlations for each category\n",
    "volt_del_corr_general = [trend_corr(stock)[0] for stock in general_stocks]\n",
    "volt_del_corr_tech = [trend_corr(stock)[0] for stock in tech_stocks]\n",
    "volt_del_corr_finance = [trend_corr(stock)[0] for stock in finance_stocks]\n",
    "volt_del_corr_crypto = [trend_corr(crypto)[0] for crypto in decentralized_currencies]\n",
    "\n",
    "# Combine the results\n",
    "volt_del_corr = volt_del_corr_general + volt_del_corr_tech + volt_del_corr_finance + volt_del_corr_crypto\n",
    "\n",
    "# Create labels for the scatter plot\n",
    "labels = general_stocks + tech_stocks + finance_stocks + decentralized_currencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scatter plot\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.scatter(range(len(general_stocks)), volt_del_corr_general, color = color_map['general'], label = 'General Stocks')\n",
    "for i in range(len(general_stocks)):\n",
    "    plt.axvline(x = i, color = color_map['general'], linestyle = ':', alpha = 0.3)  # Add vertical lines to separate the stocks\n",
    "plt.scatter(range(len(general_stocks), len(general_stocks) + len(tech_stocks)), volt_del_corr_tech, color = color_map['tech'], label = 'Tech Stocks')\n",
    "for i in range(len(general_stocks), len(general_stocks) + len(tech_stocks)):\n",
    "    plt.axvline(x = i, color = color_map['tech'], linestyle = ':', alpha = 0.3)  # Add vertical lines to separate the stocks\n",
    "plt.scatter(range(len(general_stocks) + len(tech_stocks), len(general_stocks) + len(tech_stocks) + len(finance_stocks)), volt_del_corr_finance, color = color_map['finance'], label = 'Finance Stocks')\n",
    "for i in range(len(general_stocks) + len(tech_stocks), len(general_stocks) + len(tech_stocks) + len(finance_stocks)):\n",
    "    plt.axvline(x = i, color = color_map['finance'], linestyle = ':', alpha = 0.3)  # Add vertical lines to separate the stocks\n",
    "plt.scatter(range(len(general_stocks) + len(tech_stocks) + len(finance_stocks), len(general_stocks) + len(tech_stocks) + len(finance_stocks) + len(decentralized_currencies)), volt_del_corr_crypto, color = color_map['crypto'], label = 'Decentralized Currencies')\n",
    "for i in range(len(general_stocks) + len(tech_stocks) + len(finance_stocks), len(general_stocks) + len(tech_stocks) + len(finance_stocks) + len(decentralized_currencies)):\n",
    "    plt.axvline(x = i, color = color_map['crypto'], linestyle = ':', alpha = 0.3)  # Add vertical lines to separate the stocks\n",
    "plt.axhline(y = 0, color = 'black', linestyle = '--')  # Add a horizontal line at y = 0\n",
    "plt.xlabel('Assets')\n",
    "plt.ylabel('Correlation with 7-Day Delayed Trend')\n",
    "plt.title('Correlation of Close Price and 7-Day Delayed Trend')\n",
    "legend = plt.legend()\n",
    "legend.get_frame().set_alpha(0.3)\n",
    "plt.xticks(range(len(labels)), labels, rotation = 60)\n",
    "plt.tight_layout(pad = 2)\n",
    "plt.savefig('Correlation_Scatter_Plot.png')\n",
    "plt.show()\n",
    "\n",
    "# Print the correlation values and their mean\n",
    "print(volt_del_corr, np.mean(volt_del_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for each stock and decentralized currency\n",
    "for stock in general_stocks + tech_stocks + finance_stocks + decentralized_currencies:\n",
    "    plot_stock_data(stock, download = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
