{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas.core.api import Series as Series\n",
    "import matplotlib.pyplot as plt\n",
    "from pytrends.request import TrendReq\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import warnings as wrn\n",
    "import os\n",
    "from enum import Enum\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general_stocks = ['KO', 'PFE', 'WMT', 'PG', 'JNJ', 'DIS', 'PEP', 'MCD', 'T', 'VZ']\n",
    "# tech_stocks = ['AAPL', 'AMZN', 'MSFT', 'GOOGL', 'NVDA', 'TSLA', 'META', 'INTC', 'IBM', 'AMD']\n",
    "# finance_stocks = ['GS', 'BAC', 'WFC', 'USB', 'JPM', 'MA', 'V', 'AXP', 'C', 'BLK']\n",
    "decentralized_currencies = ['BTC', 'ETH', 'ADA', 'SOL', 'XRP', 'XMR', 'LTC', 'DOT', 'LINK', 'XTZ', 'DOGE', 'SHIB']\n",
    "\n",
    "# general_stocks_names = ['Coca-Cola', 'Pfizer', 'Walmart', 'Procter & Gamble', 'Johnson & Johnson', 'Disney', 'Pepsi', 'McDonalds', 'AT&T', 'Verizon']\n",
    "# tech_stocks_names = ['Apple', 'Amazon', 'Microsoft', 'Google', 'Nvidia', 'Tesla', 'Meta', 'Intel', 'IBM', 'AMD']\n",
    "# finance_stocks_names = ['Goldman Sachs', 'Bank of America', 'Wells Fargo', 'US Bancorp', 'JPMorgan Chase', 'Mastercard', 'Visa', 'American Express', 'Citigroup', 'BlackRock']\n",
    "decentralized_currencies_names = ['Bitcoin', 'Ethereum', 'Cardano', 'Solana', 'Ripple', 'Monero', 'Litecoin', 'Polkadot', 'Chainlink', 'Tezos', 'Dogecoin', 'Shiba Inu']\n",
    "\n",
    "# color_map = {\n",
    "#     'general': 'deepskyblue',\n",
    "#     'tech': 'limegreen',\n",
    "#     'finance': 'darkorchid',\n",
    "#     'crypto': 'red'\n",
    "# }\n",
    "\n",
    "start = '2019-06-30'\n",
    "end = '2024-07-01'\n",
    "max_lags = 7\n",
    "\n",
    "balance = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "## get_trends_data\n",
    "gets the trend data using pytrends, given a certain timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trends_data(keyword, \n",
    "                    timeframe=datetime.date.today().strftime('%Y-%m-%d') + ' ' + (datetime.date.today() - datetime.timedelta(days = 269)).strftime('%Y-%m-%d'),\n",
    "                    retries=5, \n",
    "                    backoff_factor=1.0,\n",
    "                    verbose=True):\n",
    "    pytrends = TrendReq(hl='en-US', tz=360, timeout=(10,25), )\n",
    "    pytrends.build_payload(keyword, cat = 0, timeframe = timeframe, geo='')\n",
    "    \n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            df = pytrends.interest_over_time()\n",
    "            if df is not None and not df.empty:\n",
    "                if verbose:\n",
    "                    print(f\"Trend Data for {keyword[0]} at timeframe {timeframe} retrieved successfully.\")\n",
    "                df.reset_index(inplace = True)\n",
    "                df.rename(columns = {'date': 'Date', keyword[0]: 'Trend'}, inplace = True)\n",
    "                df['Date'] = pd.to_datetime(df['Date'].dt.strftime('%m/%d/%Y'))\n",
    "                return df\n",
    "            else:\n",
    "                print(\"No data retrieved or DataFrame is empty.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            if \"429\" in str(e):\n",
    "                sleep_time = backoff_factor * (2 ** i)\n",
    "                if verbose:\n",
    "                    print(f\"Rate limit exceeded. Retrying in {sleep_time} seconds...\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                raise(f\"An error occurred: {e}\")\n",
    "    print(\"Failed to retrieve data after several retries.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_stock_data\n",
    "gets the prices of a certain stock in a certain timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(ticker, start, end, verbose = True):\n",
    "    currTicker = yf.Ticker(ticker)\n",
    "    tickerDF = currTicker.history(repair = True, start = start, end = end, auto_adjust = False).drop(columns = ['Dividends', 'Stock Splits', 'Repaired?']).reset_index()\n",
    "    if verbose:\n",
    "        print(f\"Stock Data for {ticker} retrieved successfully.\")\n",
    "    tickerDF['Date'] = pd.to_datetime(tickerDF['Date'].dt.strftime('%m/%d/%Y'))\n",
    "    return tickerDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trend_corr\n",
    "gets the data of the trends and prices of the stock given to it in a certain timeframe and calculates the correlation between the log_returns and the trends delayed by certain delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_corr(stock, days = 60, start = '2023-10-01', end = '2024-06-01', delay = 7):\n",
    "    if not os.path.exists(f\"./Data/{stock}_trends({start} - {end}).csv\"):\n",
    "        if stock in general_stocks:\n",
    "            name = general_stocks_names[general_stocks.index(stock)]\n",
    "        elif stock in tech_stocks:\n",
    "            name = tech_stocks_names[tech_stocks.index(stock)]\n",
    "        elif stock in finance_stocks:\n",
    "            name = finance_stocks_names[finance_stocks.index(stock)]\n",
    "        else:\n",
    "            name = decentralized_currencies_names[decentralized_currencies.index(stock)]\n",
    "        t = get_trends_data([name], timeframe = f\"{start} {end}\")\n",
    "        if t is None:\n",
    "            raise Exception(f'Failed to retrieve Trend Data of {stock}.')\n",
    "        t.to_csv(f\"./Data/{stock}_trends({start} - {end}).csv\")\n",
    "    else:\n",
    "        t = pd.read_csv(f\"./Data/{stock}_trends({start} - {end}).csv\")\n",
    "    if not os.path.exists(f\"./Data/{stock}_Prices({start} - {end}).csv\"):\n",
    "        if stock in decentralized_currencies:\n",
    "            p = get_stock_data(f'{stock}-USD', start = start, end = end)\n",
    "        else:\n",
    "            p = get_stock_data(stock, start = start, end = end)\n",
    "        p.to_csv(f\"./Data/{stock}_Prices({start} - {end}).csv\")\n",
    "    else:\n",
    "        p = pd.read_csv(f\"./Data/{stock}_Prices({start} - {end}).csv\")\n",
    "\n",
    "    t['Date'] = pd.to_datetime(t['Date'])\n",
    "    p['Date'] = pd.to_datetime(p['Date'])\n",
    "\n",
    "    full_data = pd.merge(p, t, on='Date')\n",
    "\n",
    "    full_data['log_returns'] = np.log(full_data.Close / full_data.Close.shift(1))\n",
    "    full_data['Volatility'] = full_data['log_returns'].rolling(window=days).std() * np.sqrt(days)\n",
    "\n",
    "    for i in range(1, 8):\n",
    "        full_data[f'Delay_{i}'] = full_data['Trend'].shift(i)\n",
    "\n",
    "    rho = full_data.corr()\n",
    "    rho_c = rho['Close'][f'Delay_{delay}']\n",
    "    return rho_c, full_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot_stock_data\n",
    "gets the data of the trends and the prices of the stock given to it in a certain timeframe and plots its close and its delayed trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stock_data(stock, days = 60, start = '2023-10-01', end = '2024-06-01', delay = 7, download = False):\n",
    "    if not os.path.exists(f\"./Data/{stock}_trends({start} - {end}).csv\"):\n",
    "        if stock in general_stocks:\n",
    "            name = general_stocks_names[general_stocks.index(stock)]\n",
    "        elif stock in tech_stocks:\n",
    "            name = tech_stocks_names[tech_stocks.index(stock)]\n",
    "        elif stock in finance_stocks:\n",
    "            name = finance_stocks_names[finance_stocks.index(stock)]\n",
    "        else:\n",
    "            name = decentralized_currencies_names[decentralized_currencies.index(stock)]\n",
    "        t = get_trends_data([name], timeframe = f\"{start} {end}\")\n",
    "        t.to_csv(f\"./Data/{stock}_trends({start} - {end}).csv\")\n",
    "    else:\n",
    "        t = pd.read_csv(f\"./Data/{stock}_trends({start} - {end}).csv\")\n",
    "    if not os.path.exists(f\"./Data/{stock}_Prices({start} - {end}).csv\"):\n",
    "        p = get_stock_data(stock, start = start, end = end)\n",
    "        p.to_csv(f\"./Data/{stock}_Prices({start} - {end}).csv\")\n",
    "    else:\n",
    "        p = pd.read_csv(f\"./Data/{stock}_Prices({start} - {end}).csv\")\n",
    "\n",
    "    t['Date'] = pd.to_datetime(t['Date'])\n",
    "    p['Date'] = pd.to_datetime(p['Date'])\n",
    "\n",
    "    full_data = pd.merge(p, t, on='Date')\n",
    "\n",
    "    full_data['log_returns'] = np.log(full_data.Close / full_data.Close.shift(1))\n",
    "    full_data['Volatility'] = full_data['log_returns'].rolling(window=days).std() * np.sqrt(days)\n",
    "\n",
    "    full_data[f'Delay_{delay}'] = full_data.Trend.shift(7)\n",
    "\n",
    "    # Determine the color based on the stock category\n",
    "    if stock in general_stocks:\n",
    "        color = color_map['general']\n",
    "        name = general_stocks_names[general_stocks.index(stock)]\n",
    "    elif stock in tech_stocks:\n",
    "        color = color_map['tech']\n",
    "        name = tech_stocks_names[tech_stocks.index(stock)]\n",
    "    elif stock in finance_stocks:\n",
    "        color = color_map['finance']\n",
    "        name = finance_stocks_names[finance_stocks.index(stock)]\n",
    "    else:\n",
    "        color = color_map['crypto']\n",
    "        name = decentralized_currencies_names[decentralized_currencies.index(stock)]\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "    # Plot Close price\n",
    "    axes[0].plot(full_data['Date'], full_data['Close'], label = 'Close Price', color = color)\n",
    "    axes[0].set_xlabel('Date')\n",
    "    axes[0].set_ylabel('Close Price')\n",
    "    axes[0].set_title(f'{stock}: Close Price')\n",
    "    legend = axes[0].legend(loc='upper left')\n",
    "    legend.get_frame().set_alpha(0.3)\n",
    "\n",
    "    # Plot 7-days delay trend\n",
    "    axes[1].plot(full_data['Date'], full_data[f'Delay_{delay}'], label = f'{delay}-Days Delayed Trend', color = 'black')\n",
    "    axes[1].set_xlabel('Date')\n",
    "    axes[1].set_ylabel(f'{delay}-Days Delayed Trend')\n",
    "    axes[1].set_title(f'{stock}: {delay}-Days Delayed Trend')\n",
    "    legend = axes[1].legend(loc='upper right')\n",
    "    legend.get_frame().set_alpha(0.3)\n",
    "\n",
    "    fig.suptitle(f'{name} ({stock})', fontsize=20, verticalalignment = 'bottom', fontweight = 'bold')\n",
    "    plt.tight_layout(pad=2.0)\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    if download:\n",
    "        plt.savefig(f\"./Plots/{stock}_plot({start} - {end}).png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time_jump\n",
    "adds time in days to a given date that was accepted as string, returns as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the date in string format after a certain number of days\n",
    "def time_jump(start, days = 7 * 38):\n",
    "    return (datetime.datetime.strptime(start, '%Y-%m-%d') + datetime.timedelta(days = days)).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_breakpoints\n",
    "calculates the breakpoints needed for the multiple requests of data to make the data as long as possible for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_breakpoints(start, end, days = 7 * 38):\n",
    "    breakpoints = [start]\n",
    "    while datetime.datetime.strptime(breakpoints[-1], '%Y-%m-%d') < datetime.datetime.strptime(end, '%Y-%m-%d'):\n",
    "        temp = time_jump(breakpoints[-1], days)\n",
    "        if datetime.datetime.strptime(temp, '%Y-%m-%d') < datetime.datetime.strptime(end, '%Y-%m-%d'):\n",
    "            breakpoints.append(temp)\n",
    "        else:\n",
    "            breakpoints.append(end)\n",
    "    return breakpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## connectNnormalizeTrends\n",
    "gets the data of the trends and prices daily in the whole time frame and in parts, and connects them by estimating an eproximation of their absolute amount of searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connectNnormalizeTrends(Dfs, stock):\n",
    "    # setting up the data for step 1\n",
    "    if not os.path.exists(f'./Data/glimpse_{stock}_5Y.csv'):\n",
    "        raise Exception(f'Failed to retrieve Glimpse Data of {stock}.')\n",
    "    glmpsDf = pd.read_csv(f'./Data/glimpse_{stock}_5Y.csv')\n",
    "    glmpsDf.rename(columns={'Time (week of)': 'Date', 'Absolute Google Search Volume': 'Absolute_Volume'}, inplace=True)\n",
    "    glmpsDf['Date'] = pd.to_datetime(glmpsDf['Date'])\n",
    "\n",
    "    df_concat = pd.concat(Dfs).reset_index(drop = True)\n",
    "    df_concat['Date'] = pd.to_datetime(df_concat['Date'])\n",
    "\n",
    "    # calculating the mean trend for each week and merging it into glmpsDf\n",
    "    df_concat['MeanTrend'] = df_concat['Trend'].rolling(window=7, min_periods=1).mean().shift(-6)\n",
    "    glmpsDf = pd.merge(glmpsDf, df_concat, on='Date', how='left').drop(columns = ['Trend'])\n",
    "\n",
    "    # setting up the data for step 2\n",
    "    glmpsDf.rename(columns={'Date': 'Date_Week'}, inplace=True)\n",
    "    df_concat = df_concat.drop(columns = ['MeanTrend'])\n",
    "    df_concat['Date_Week'] = (df_concat['Date'] - pd.to_timedelta((df_concat['Date'].dt.weekday + 1) % 7, unit='d')).dt.strftime('%Y-%m-%d')\n",
    "    df_concat['Date_Week'] = pd.to_datetime(df_concat['Date_Week'])\n",
    "\n",
    "    # calculating the ratio and search volume for each week\n",
    "    df_concat = pd.merge(df_concat, glmpsDf[['Date_Week', 'MeanTrend', 'Absolute_Volume']], on='Date_Week', how='left')\n",
    "    df_concat['Ratio'] = df_concat['Trend'] / (df_concat['MeanTrend'] * 7)\n",
    "    df_concat['Search_Volume'] = df_concat['Ratio'] * df_concat['Absolute_Volume']\n",
    "\n",
    "    # adding to df_concat the check ratio to check validity of the data\n",
    "    df_concat['check_ratio'] = df_concat['Search_Volume'] / df_concat['Trend']\n",
    "\n",
    "    # renormalizing the data\n",
    "    df_concat['Normalized_Searches'] = ((df_concat['Search_Volume'] - df_concat['Search_Volume'].min()) / (df_concat['Search_Volume'].max() - df_concat['Search_Volume'].min())) * 100\n",
    "\n",
    "    # cleaning out unnecessary columns\n",
    "    df_concat = df_concat.drop(columns = ['Trend', 'Date_Week', 'MeanTrend', 'Absolute_Volume', 'Ratio'])\n",
    "\n",
    "    df_concat['Date'] = df_concat['Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getNormalizedData\n",
    "gets the data normalized, and performs validity checks (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNormalizedData(stock, ticker, start = '2019-06-23', end = '2024-06-01', weeks = 38, do_double = True, verbose = False):\n",
    "    if weeks > 38:\n",
    "        wrn.warn('The maximum number of weeks is 38. \\nThe number of weeks will be set to 38.', category=Warning)\n",
    "        weeks = 38\n",
    "    if do_double & (weeks % 2 != 0):\n",
    "        wrn.warn('The number of weeks must be even to use the double method. \\nThe number of weeks will be rounded down to the nearest even number.', category=Warning)\n",
    "        weeks -= 1\n",
    "    breakpoints = get_breakpoints(start, end, days = 7 * weeks)\n",
    "    if do_double:\n",
    "        breakpoints2 = get_breakpoints(start, end, days = 7 * weeks / 2)\n",
    "        breakpoints2 = [x for x in breakpoints2 if x not in breakpoints[1:-1]]\n",
    "    \n",
    "    # initializing the list to store the dataframes\n",
    "    Dfs = []\n",
    "\n",
    "    # extracting the data for each time period\n",
    "    for i in range(len(breakpoints) - 1):\n",
    "        startTemp = breakpoints[i]\n",
    "        endTemp = time_jump(breakpoints[i + 1], days=-1)\n",
    "        # checking if the data is already extracted\n",
    "        if not os.path.exists(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\"):\n",
    "            # extracting the data\n",
    "            t = get_trends_data([stock], timeframe = f\"{startTemp} {endTemp}\", verbose = verbose).drop(columns = ['isPartial'])\n",
    "            if t is None:\n",
    "                raise Exception(f'Failed to retrieve Trend Data of {stock}.')\n",
    "            t.to_csv(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\")\n",
    "        else:\n",
    "            t = pd.read_csv(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\").drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "        Dfs.append(t)\n",
    "\n",
    "    df_concat = connectNnormalizeTrends(Dfs, stock)\n",
    "\n",
    "    if do_double:\n",
    "        # initializing the list to store the dataframes\n",
    "        Dfs2 = []\n",
    "\n",
    "        # extracting the data for each time period\n",
    "        for i in range(len(breakpoints2) - 1):\n",
    "            startTemp = breakpoints2[i]\n",
    "            endTemp = time_jump(breakpoints2[i + 1], days=-1)\n",
    "            # checking if the data is already extracted\n",
    "            if not os.path.exists(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\"):\n",
    "                # extracting the data\n",
    "                t = get_trends_data([stock], timeframe = f\"{startTemp} {endTemp}\", verbose = verbose).drop(columns = ['isPartial'])\n",
    "                if t is None:\n",
    "                    raise Exception(f'Failed to retrieve Trend Data of {stock}.')\n",
    "                t.to_csv(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\")\n",
    "            else:\n",
    "                t = pd.read_csv(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\").drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "            Dfs2.append(t)\n",
    "        \n",
    "        df_concat2 = connectNnormalizeTrends(Dfs2, stock)\n",
    "    \n",
    "        df_concat[\"Normalized_Searches\"] = (df_concat[\"Normalized_Searches\"] + df_concat2[\"Normalized_Searches\"]) / 2\n",
    "    \n",
    "    if not os.path.exists(f\"./Data/{stock}_Prices({start}-{end}).csv\"):\n",
    "        stockData = get_stock_data(f'{ticker}-USD', start = start, end = end, verbose = verbose)\n",
    "        stockData.to_csv(f\"./Data/{stock}_Prices({start}-{end}).csv\")\n",
    "    else:\n",
    "        stockData = pd.read_csv(f\"./Data/{stock}_Prices({start}-{end}).csv\").drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "    df_concat[df_concat.Normalized_Searches == 0] = 0.1\n",
    "\n",
    "    df_concat['log_searches'] = np.log(df_concat.Normalized_Searches / df_concat.Normalized_Searches.shift(1))\n",
    "    stockData['log_returns'] = np.log(stockData.Close / stockData.Close.shift(1))\n",
    "\n",
    "    try:\n",
    "        df_concat['Date'] = df_concat['Date'].dt.strftime('%Y-%m-%d')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        stockData['Date'] = stockData['Date'].dt.strftime('%Y-%m-%d')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    finalDf = pd.merge(stockData, df_concat, on='Date', how='left').dropna()\n",
    "\n",
    "    if verbose:\n",
    "        # data normalization validity check\n",
    "        top = 0\n",
    "        bottom = 0\n",
    "        for i in range(len(Dfs)):\n",
    "            top += Dfs[i].shape[0]\n",
    "            print(f'period {i + 1}:\\n=========\\nmean: {df_concat.iloc[bottom:top]['check_ratio'].mean():.4f}\\nsd: {df_concat.iloc[bottom:top][\"check_ratio\"].std():.4f}\\n\\nmean/sd: {df_concat.iloc[bottom:top]['check_ratio'].mean()/df_concat.iloc[bottom:top][\"check_ratio\"].std():.4f}\\n')\n",
    "            bottom += Dfs[i].shape[0]\n",
    "\n",
    "        if do_double:\n",
    "            top = 0\n",
    "            bottom = 0\n",
    "            for i in range(len(Dfs2)):\n",
    "                top += Dfs2[i].shape[0]\n",
    "                print(f'period {i + len(Dfs) + 1}:\\n=========\\nmean: {df_concat.iloc[bottom:top]['check_ratio'].mean():.4f}\\nsd: {df_concat.iloc[bottom:top][\"check_ratio\"].std():.4f}\\n\\nmean/sd: {df_concat.iloc[bottom:top]['check_ratio'].mean()/df_concat.iloc[bottom:top][\"check_ratio\"].std():.4f}\\n')\n",
    "                bottom += Dfs2[i].shape[0]\n",
    "    return finalDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtesting\n",
    "## Collecting The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrn.filterwarnings('ignore', category=UserWarning)\n",
    "wrn.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "coinsDFs = []\n",
    "\n",
    "for i in range(len(decentralized_currencies)):\n",
    "    coinsDFs.append(getNormalizedData(decentralized_currencies_names[i], decentralized_currencies[i], \n",
    "                                      start = start, end = end, \n",
    "                                      do_double = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Granger Causality Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitcoin: 7 lags, p-value = 0.1163\n",
      "Ethereum: 1 lags, p-value = 0.1807\n",
      "Cardano: 2 lags, p-value = 0.0776\n",
      "Solana: 3 lags, p-value = 0.0782\n",
      "Ripple: 1 lags, p-value = 0.5286\n",
      "Monero: 1 lags, p-value = 0.1025\n",
      "Litecoin: 2 lags, p-value = 0.2933\n",
      "Polkadot: 3 lags, p-value = 0.0056\n",
      "Chainlink: 1 lags, p-value = 0.0093\n",
      "Tezos: 3 lags, p-value = 0.4993\n",
      "Dogecoin: 5 lags, p-value = 0.0005\n",
      "Shiba Inu: 2 lags, p-value = 0.0000\n"
     ]
    }
   ],
   "source": [
    "wrn.filterwarnings('ignore', category=UserWarning)\n",
    "wrn.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "lags = []\n",
    "\n",
    "for i in range(len(decentralized_currencies)):\n",
    "    cause = grangercausalitytests(coinsDFs[i][['log_returns', 'log_searches']], \n",
    "                                  maxlag = max_lags, \n",
    "                                  verbose = False)\n",
    "\n",
    "    min_p_value = float('inf')\n",
    "    min_p_lag = None\n",
    "\n",
    "    for lag, result in cause.items():\n",
    "        p_value = result[0]['ssr_ftest'][1]\n",
    "        if p_value < min_p_value:\n",
    "            min_p_value = p_value\n",
    "            min_p_lag = lag\n",
    "\n",
    "    lags.append(min_p_lag)\n",
    "\n",
    "    coinsDFs[i]['Normalized_Searches_delayed'] = coinsDFs[i]['Normalized_Searches'].shift(min_p_lag)\n",
    "    coinsDFs[i]['log_searches_delayed'] = coinsDFs[i]['log_searches'].shift(min_p_lag)\n",
    "    coinsDFs[i].dropna(inplace = True)\n",
    "\n",
    "    print(f'{decentralized_currencies_names[i]}: {min_p_lag} lags, p-value = {min_p_value:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples for checks code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # see data validity checks of a certain stock\n",
    "# stock_name = 'Bitcoin' # change to any stock name that is in the list at the top\n",
    "# stock_ticker = 'BTC' # change to the corresponding ticker of the stock\n",
    "\n",
    "# getNormalizedData(stock_name, stock_ticker, \n",
    "#                   start = start, end = end, \n",
    "#                   do_double = True, verbose = True)\n",
    "\n",
    "# # see the granger causality test results of a certain stock\n",
    "# df = getNormalizedData(stock_name, stock_ticker, start = start, end = end, do_double = True)\n",
    "\n",
    "# grangercausalitytests(df[['log_returns', 'log_searches']], \n",
    "#                       maxlag = max_lags, \n",
    "#                       verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtesting\n",
    "## Data Split\n",
    "* 4 years train\n",
    "* 1 year test\n",
    "\n",
    "## Modeling description\n",
    "### hyper-parameters:\n",
    "* bb_window: size of the window of MA of the normalized searches and the bollinger bands on that MA\n",
    "* bb_window_min: minimum size of the window of MA of the normalized searches and the bollinger bands on that MA\n",
    "* bb_threshold: how many sds should be added to/subtracted from the bollinger bands\n",
    "* rsi_window: size of the window of the RSI on the Close\n",
    "* rsi_window_min: minimum size of the window of RSI on the Close\n",
    "* rsi_limits: how sensitive the rsi is, \n",
    "    - the higher the lower value (the first value of rsi_limits) the less the RSI blocks a BUY signal\n",
    "    - the lower the higher value (the second value of rsi_limits) the less the RSI blocks a SELL signal\n",
    "* sell_all: a boolian value that indicates whether the strategy says to sell all of the quantity of the asset when it gives a SELL signal or the strategy says to sell some relative amount of the quantity when it gives a SELL signal\n",
    "* qty_scale: represents the scale of the quantity that is calculated by the strategy\n",
    "\n",
    "### additional parameters:\n",
    "* starting_balance: the starting balance\n",
    "* commission: the commission for every market dealing\n",
    "* commission_type: whether the commission is a set amount or some precentage from the price\n",
    "* slippage_factor: how much slippage exists in the market "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enums\n",
    "class ActionType(Enum):\n",
    "    BUY = 1\n",
    "    SELL = -1\n",
    "    DONOTHING = 0\n",
    "\n",
    "# strategy class\n",
    "class Strategy():\n",
    "    def __init__(self, bb_window: int, rsi_window: int, \n",
    "                 bb_window_min: int=None, bb_threshold: float=0.5, rsi_window_min: int=None, rsi_limits: Tuple[float,float]=[30.0, 70.0], sell_all: bool=False, qty_scale: float=0.1,\n",
    "                 commission_type: object=\"scalar\", slippage_factor=np.inf) -> None:\n",
    "        self.bb_window = bb_window\n",
    "        self.rsi_window = rsi_window\n",
    "        if bb_window_min is None:\n",
    "            bb_window_min = bb_window\n",
    "        else:\n",
    "            if bb_window_min > bb_window:\n",
    "                raise ValueError(\"bb_window_min should be less than or equal to bb_window\")\n",
    "            if bb_window_min <= 0:\n",
    "                raise ValueError(\"bb_window_min should be a value greater than 0\")\n",
    "            self.bb_window_min = bb_window_min\n",
    "        if bb_threshold <= 0:\n",
    "            raise ValueError(\"bb_threshold should be a value greater than 0\")\n",
    "        self.bb_threshold = bb_threshold\n",
    "        if rsi_window_min is None:\n",
    "            rsi_window_min = rsi_window\n",
    "        else:\n",
    "            if rsi_window_min > rsi_window:\n",
    "                raise ValueError(\"rsi_window_min should be less than or equal to rsi_window\")\n",
    "            if rsi_window_min <= 0:\n",
    "                raise ValueError(\"rsi_window_min should be a value greater than 0\")\n",
    "            self.rsi_window_min = rsi_window_min\n",
    "        if rsi_limits[0] <= 0 or rsi_limits[1] > 100 or rsi_limits[0] >= rsi_limits[1]:\n",
    "            raise ValueError(\"rsi_limit should be a tuple of two values between 0 and 100 where the first value is less than the second\")\n",
    "        self.rsi_limits = rsi_limits\n",
    "        if commission_type not in [\"scalar\", \"precentage\"]:\n",
    "            raise ValueError(\"commision_type should be either 'scalar' or 'precentage'\")\n",
    "        self.commission_type = commission_type\n",
    "        self.sell_all = sell_all\n",
    "        self.slippage_factor = slippage_factor\n",
    "        if qty_scale <= 0:\n",
    "            raise ValueError(\"qty_scale should be a value greater than 0\")\n",
    "        self.qty_scale = qty_scale\n",
    "\n",
    "    def calc_signal(self, data: pd.DataFrame) -> None:\n",
    "        norm_search = data['Normalized_Searches_delayed']\n",
    "        close = data['Close']\n",
    "\n",
    "        # MA of search volume as threshold for Bollinger Bands\n",
    "        norm_search_MA = norm_search.rolling(window=self.bb_window, min_periods=self.bb_window_min).mean()\n",
    "        norm_search_sd = norm_search.rolling(window=self.bb_window, min_periods=self.bb_window_min).std()\n",
    "        upper_band = norm_search_MA + self.bb_threshold * norm_search_sd\n",
    "        lower_band = norm_search_MA - self.bb_threshold * norm_search_sd\n",
    "\n",
    "        data['norm_search_MA_upper_band'] = upper_band\n",
    "        data['norm_search_MA_lower_band'] = lower_band\n",
    "\n",
    "        # RSI of close price as threshold limiting the Bollinger Bands signal\n",
    "        delta = close.diff()\n",
    "        gains = delta.where(delta > 0, 0)\n",
    "        losses = -delta.where(delta < 0, 0)\n",
    "        avg_gain = gains.rolling(window=self.rsi_window, min_periods=self.rsi_window_min).mean()\n",
    "        avg_loss = losses.rolling(window=self.rsi_window, min_periods=self.rsi_window_min).mean()\n",
    "        rs = avg_gain / avg_loss\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "        data['strategy'] = 0\n",
    "        data.loc[(norm_search < lower_band) & (rsi < self.rsi_limits[0]), 'strategy'] = -1 # Sell signal\n",
    "        data.loc[(norm_search > upper_band) & (rsi > self.rsi_limits[1]), 'strategy'] = 1 # Buy signal\n",
    "\n",
    "    def calc_realistic_price(self, row: pd.Series, action: ActionType) -> float:\n",
    "        slippage_rate = ((row['Close'] - row['Open']) / row['Open']) / self.slippage_factor\n",
    "        slippage_price = row['Open'] + row['Open'] * slippage_rate\n",
    "\n",
    "        if action == ActionType.BUY:\n",
    "            return max(slippage_price, row['Open'])\n",
    "        elif action == ActionType.SELL:\n",
    "            return min(slippage_price, row['Open'])\n",
    "        else:\n",
    "            return slippage_price\n",
    "\n",
    "def calc_total_return(portfolio_values):\n",
    "    return (portfolio_values.iloc[-1] / portfolio_values.iloc[0]) - 1.0\n",
    "\n",
    "def calc_annualized_return(portfolio_values):\n",
    "    yearly_trading_days = 252\n",
    "    portfolio_trading_days = portfolio_values.shape[0]\n",
    "    portfolio_trading_years = portfolio_trading_days / yearly_trading_days \n",
    "    return (portfolio_values.iloc[-1] / portfolio_values.iloc[0])**(1/portfolio_trading_years) - 1.0\n",
    "\n",
    "def calc_annualized_sharpe(portfolio_values: pd.Series, rf: float=0.0):\n",
    "    yearly_trading_days = 252\n",
    "    annualized_return = calc_annualized_return(portfolio_values)\n",
    "    annualized_std = portfolio_values.pct_change().std() * np.sqrt(yearly_trading_days)\n",
    "    if annualized_std is None or annualized_std == 0:\n",
    "        return 0\n",
    "    sharpe = (annualized_return - rf) / annualized_std\n",
    "    return sharpe\n",
    "\n",
    "def calc_downside_deviation(portfolio_values):\n",
    "    porfolio_returns = portfolio_values.pct_change().dropna()\n",
    "    return porfolio_returns[porfolio_returns < 0].std()\n",
    "\n",
    "def calc_sortino(portfolio_values, rf=0.0):\n",
    "    yearly_trading_days = 252\n",
    "    down_deviation = calc_downside_deviation(portfolio_values) * np.sqrt(yearly_trading_days)\n",
    "    annualized_return = calc_annualized_return(portfolio_values)\n",
    "    if down_deviation is None or down_deviation == 0:\n",
    "        return 0\n",
    "    sortino = (annualized_return - rf) / down_deviation\n",
    "    return sortino\n",
    "\n",
    "def calc_max_drawdown(portfolio_values):\n",
    "    cumulative_max = portfolio_values.cummax()\n",
    "    drawdown = (cumulative_max - portfolio_values) / cumulative_max\n",
    "    return drawdown.max()\n",
    "\n",
    "def calc_calmar(portfolio_values):\n",
    "    max_drawdown = calc_max_drawdown(portfolio_values)\n",
    "    annualized_return = calc_annualized_return(portfolio_values)\n",
    "    return annualized_return / max_drawdown\n",
    "\n",
    "def evaluate_strategy(b_df, strat_name):\n",
    "    total_return = calc_total_return(b_df['portfolio_value'])\n",
    "    annualized_return = calc_annualized_return(b_df['portfolio_value'])\n",
    "    annualized_sharpe = calc_annualized_sharpe(b_df['portfolio_value'])\n",
    "    sortino_ratio = calc_sortino(b_df['portfolio_value'])\n",
    "    max_drawdown = calc_max_drawdown(b_df['portfolio_value'])\n",
    "    calmar_ratio = calc_calmar(b_df['portfolio_value'])\n",
    "    \n",
    "    print(f\"Results for {strat_name}:\")\n",
    "    print(f\"Total Return: {total_return:.2%}\")\n",
    "    print(f\"Annualized Return: {annualized_return:.2%}\")\n",
    "    print(f\"Annualized Sharpe Ratio: {annualized_sharpe:.2f}\")\n",
    "    print(f\"Sortino Ratio: {sortino_ratio:.2f}\")\n",
    "    print(f\"Max Drawdown: {max_drawdown:.2%}\")\n",
    "    print(f\"Calmar Ratio: {calmar_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting the dataframes\n",
    "so they could be interpeted as one complete investment plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfConnect(DFs: list) -> pd.DataFrame:\n",
    "    combinedDF = pd.concat(DFs).reset_index(drop = True)\n",
    "    combinedDF['TempDate'] = pd.to_datetime(combinedDF['Date'])\n",
    "    combinedDF = combinedDF.sort_values(by = 'TempDate').reset_index(drop = True).drop(columns = ['TempDate', 'index'])\n",
    "    return combinedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(DFs: list, strategy: Strategy, starting_balance: float, commission: float=0.0) -> pd.DataFrame:\n",
    "    for i in range(len(DFs)):\n",
    "        strategy.calc_signal(DFs[i])\n",
    "        DFs[i]['coin'] = decentralized_currencies_names[i]\n",
    "        DFs[i].reset_index(inplace=True)\n",
    "\n",
    "    # memory dataframe\n",
    "    mem = pd.DataFrame({'coin': decentralized_currencies_names, \n",
    "                        'qty': 0,\n",
    "                        'price': 0.0})\n",
    "\n",
    "    data = dfConnect(DFs)\n",
    "    data['qty'] = 0.0\n",
    "    data['balance'] = 0.0\n",
    "    data['portfolio_value'] = 0.0\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        # Get the current balance and qty before the action\n",
    "        curr_qty = mem.loc[mem.coin == data.loc[i, 'coin'], 'qty'].values[0]\n",
    "        curr_portfolio = data.loc[i - 1, 'portfolio_value'] if i > 0 else starting_balance\n",
    "        prev_price = mem.loc[mem.coin == data.loc[i, 'coin'], 'price'].values[0]\n",
    "        new_price = strategy.calc_realistic_price(row, ActionType.BUY if row['strategy'] == 1 else ActionType.DONOTHING if ((row['strategy'] != -1) | (row['Date'] == data.Date.iloc[-1])) else ActionType.SELL)\n",
    "        mem.loc[mem.coin == data.loc[i, 'coin'], 'price'] = new_price\n",
    "        curr_balance = data.loc[i - 1, 'balance'] + (new_price - prev_price) * curr_qty if i > 0 else starting_balance        \n",
    "\n",
    "        # Sell signal when strategy says so or at the end of trade and holding any stock\n",
    "        if (curr_qty > 0) & ((row['Date'] == data.Date.iloc[-1]) | (row['strategy'] == -1)):\n",
    "            sell_qty = curr_qty if (strategy.sell_all | (row['Date'] == data.Date.iloc[-1])) else min((row['norm_search_MA_lower_band'] / row['Normalized_Searches_delayed'] - 1) * strategy.qty_scale, curr_qty)\n",
    "            data.loc[i, 'balance'] = curr_balance + (new_price * sell_qty - commission if strategy.commission_type == \"scalar\" else new_price * sell_qty * (1 - commission))\n",
    "            data.loc[i, 'qty'] = 0.0 if (strategy.sell_all | (row['Date'] == data.Date.iloc[-1])) else curr_qty - sell_qty\n",
    "            mem.loc[mem.coin == data.loc[i, 'coin'], 'qty'] = 0.0 if (strategy.sell_all | (row['Date'] == data.Date.iloc[-1])) else curr_qty - sell_qty\n",
    "\n",
    "        # Buy signal when strategy says so as long not in the end of trade data and not holding any stock\n",
    "        elif (row['Date'] != data.Date.iloc[-1]) & (data.loc[i, 'strategy'] == 1):\n",
    "            buy_qty = min((row['Normalized_Searches_delayed'] / row['norm_search_MA_upper_band'] - 1) * strategy.qty_scale, (curr_balance - commission) / new_price if strategy.commission_type == \"scalar\" else curr_balance / (new_price * (1 + commission)))\n",
    "            data.loc[i, 'balance'] = curr_balance - (new_price * buy_qty + commission if strategy.commission_type == \"scalar\" else new_price * buy_qty * (1 + commission))\n",
    "            data.loc[i, 'qty'] = curr_qty + buy_qty\n",
    "            mem.loc[mem.coin == data.loc[i, 'coin'], 'qty'] = curr_qty + buy_qty\n",
    "\n",
    "        # Do nothing\n",
    "        else:\n",
    "            data.loc[i, 'balance'] = curr_balance\n",
    "            data.loc[i, 'qty'] = curr_qty\n",
    "        \n",
    "    data['portfolio_value'] = data['balance'] + data['Close'] * data['qty']\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_strategy = Strategy(bb_window=14, rsi_window=14, bb_window_min=1, bb_threshold=0.5, rsi_window_min=1, rsi_limits=[30.0, 70.0], sell_all=False, qty_scale=0.1, commission_type=\"scalar\", slippage_factor=1.0)\n",
    "tempDF = backtest(coinsDFs.copy(), example_strategy, balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>log_returns</th>\n",
       "      <th>Search_Volume</th>\n",
       "      <th>check_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>log_searches</th>\n",
       "      <th>Normalized_Searches_delayed</th>\n",
       "      <th>log_searches_delayed</th>\n",
       "      <th>norm_search_MA_upper_band</th>\n",
       "      <th>norm_search_MA_lower_band</th>\n",
       "      <th>strategy</th>\n",
       "      <th>coin</th>\n",
       "      <th>qty</th>\n",
       "      <th>balance</th>\n",
       "      <th>portfolio_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-07-02</td>\n",
       "      <td>293.537262</td>\n",
       "      <td>295.655853</td>\n",
       "      <td>272.602570</td>\n",
       "      <td>291.596436</td>\n",
       "      <td>291.596436</td>\n",
       "      <td>10618413952</td>\n",
       "      <td>-0.006988</td>\n",
       "      <td>28947.541667</td>\n",
       "      <td>933.791667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087434</td>\n",
       "      <td>3.581607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-07-02</td>\n",
       "      <td>3.590160</td>\n",
       "      <td>3.818431</td>\n",
       "      <td>3.232567</td>\n",
       "      <td>3.645436</td>\n",
       "      <td>3.645436</td>\n",
       "      <td>424338055</td>\n",
       "      <td>0.014774</td>\n",
       "      <td>6305.510204</td>\n",
       "      <td>77.845805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042149</td>\n",
       "      <td>22.238789</td>\n",
       "      <td>-0.274370</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Chainlink</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-07-02</td>\n",
       "      <td>0.406414</td>\n",
       "      <td>0.409737</td>\n",
       "      <td>0.384477</td>\n",
       "      <td>0.400204</td>\n",
       "      <td>0.400204</td>\n",
       "      <td>1676193189</td>\n",
       "      <td>-0.016500</td>\n",
       "      <td>24047.294798</td>\n",
       "      <td>308.298651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107400</td>\n",
       "      <td>6.976213</td>\n",
       "      <td>-0.157064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Ripple</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-07-02</td>\n",
       "      <td>88.441093</td>\n",
       "      <td>88.833931</td>\n",
       "      <td>82.011810</td>\n",
       "      <td>86.264908</td>\n",
       "      <td>86.264908</td>\n",
       "      <td>132861597</td>\n",
       "      <td>-0.024712</td>\n",
       "      <td>2437.160752</td>\n",
       "      <td>37.494781</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027148</td>\n",
       "      <td>12.820515</td>\n",
       "      <td>-0.040331</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Monero</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-07-03</td>\n",
       "      <td>86.238914</td>\n",
       "      <td>92.061852</td>\n",
       "      <td>86.042488</td>\n",
       "      <td>90.001793</td>\n",
       "      <td>90.001793</td>\n",
       "      <td>127688765</td>\n",
       "      <td>0.042407</td>\n",
       "      <td>2549.645094</td>\n",
       "      <td>37.494781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067479</td>\n",
       "      <td>12.477146</td>\n",
       "      <td>-0.027148</td>\n",
       "      <td>12.770230</td>\n",
       "      <td>12.527431</td>\n",
       "      <td>0</td>\n",
       "      <td>Monero</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>2024-06-29</td>\n",
       "      <td>6.174864</td>\n",
       "      <td>6.285307</td>\n",
       "      <td>6.061954</td>\n",
       "      <td>6.072340</td>\n",
       "      <td>6.072340</td>\n",
       "      <td>94858682</td>\n",
       "      <td>-0.016700</td>\n",
       "      <td>2742.016393</td>\n",
       "      <td>80.647541</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090395</td>\n",
       "      <td>6.174626</td>\n",
       "      <td>-0.115126</td>\n",
       "      <td>6.995337</td>\n",
       "      <td>6.454482</td>\n",
       "      <td>0</td>\n",
       "      <td>Polkadot</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>216043.402760</td>\n",
       "      <td>216043.402760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>2024-06-29</td>\n",
       "      <td>0.470977</td>\n",
       "      <td>0.476143</td>\n",
       "      <td>0.470915</td>\n",
       "      <td>0.471994</td>\n",
       "      <td>0.471994</td>\n",
       "      <td>402714114</td>\n",
       "      <td>0.002352</td>\n",
       "      <td>14286.422619</td>\n",
       "      <td>310.574405</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.452835</td>\n",
       "      <td>2.459450</td>\n",
       "      <td>0.352010</td>\n",
       "      <td>2.554765</td>\n",
       "      <td>1.801398</td>\n",
       "      <td>0</td>\n",
       "      <td>Ripple</td>\n",
       "      <td>-2.788529e+03</td>\n",
       "      <td>216040.310270</td>\n",
       "      <td>214724.141335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>2024-06-29</td>\n",
       "      <td>13.758665</td>\n",
       "      <td>14.130752</td>\n",
       "      <td>13.483731</td>\n",
       "      <td>13.511538</td>\n",
       "      <td>13.511538</td>\n",
       "      <td>189101546</td>\n",
       "      <td>-0.018131</td>\n",
       "      <td>1714.555184</td>\n",
       "      <td>61.234114</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.333604</td>\n",
       "      <td>6.056291</td>\n",
       "      <td>0.020965</td>\n",
       "      <td>9.224848</td>\n",
       "      <td>5.842442</td>\n",
       "      <td>0</td>\n",
       "      <td>Chainlink</td>\n",
       "      <td>-7.227519e+02</td>\n",
       "      <td>216218.986966</td>\n",
       "      <td>206453.496914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>2024-06-29</td>\n",
       "      <td>0.122795</td>\n",
       "      <td>0.124121</td>\n",
       "      <td>0.121389</td>\n",
       "      <td>0.121699</td>\n",
       "      <td>0.121699</td>\n",
       "      <td>285391178</td>\n",
       "      <td>-0.008917</td>\n",
       "      <td>7337.750000</td>\n",
       "      <td>815.305556</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071780</td>\n",
       "      <td>0.448656</td>\n",
       "      <td>0.182322</td>\n",
       "      <td>0.413758</td>\n",
       "      <td>0.371719</td>\n",
       "      <td>0</td>\n",
       "      <td>Dogecoin</td>\n",
       "      <td>-4.095672e+04</td>\n",
       "      <td>216263.630001</td>\n",
       "      <td>211279.237797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>2024-06-29</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>112260218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15055.157895</td>\n",
       "      <td>1881.894737</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022537</td>\n",
       "      <td>0.878210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.016301</td>\n",
       "      <td>0.895223</td>\n",
       "      <td>-1</td>\n",
       "      <td>Shiba Inu</td>\n",
       "      <td>-7.861123e+08</td>\n",
       "      <td>216263.630001</td>\n",
       "      <td>202899.720326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date        Open        High         Low       Close   Adj Close  \\\n",
       "0      2019-07-02  293.537262  295.655853  272.602570  291.596436  291.596436   \n",
       "1      2019-07-02    3.590160    3.818431    3.232567    3.645436    3.645436   \n",
       "2      2019-07-02    0.406414    0.409737    0.384477    0.400204    0.400204   \n",
       "3      2019-07-02   88.441093   88.833931   82.011810   86.264908   86.264908   \n",
       "4      2019-07-03   86.238914   92.061852   86.042488   90.001793   90.001793   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "19995  2024-06-29    6.174864    6.285307    6.061954    6.072340    6.072340   \n",
       "19996  2024-06-29    0.470977    0.476143    0.470915    0.471994    0.471994   \n",
       "19997  2024-06-29   13.758665   14.130752   13.483731   13.511538   13.511538   \n",
       "19998  2024-06-29    0.122795    0.124121    0.121389    0.121699    0.121699   \n",
       "19999  2024-06-29    0.000017    0.000017    0.000017    0.000017    0.000017   \n",
       "\n",
       "            Volume  log_returns  Search_Volume  check_ratio  ...  \\\n",
       "0      10618413952    -0.006988   28947.541667   933.791667  ...   \n",
       "1        424338055     0.014774    6305.510204    77.845805  ...   \n",
       "2       1676193189    -0.016500   24047.294798   308.298651  ...   \n",
       "3        132861597    -0.024712    2437.160752    37.494781  ...   \n",
       "4        127688765     0.042407    2549.645094    37.494781  ...   \n",
       "...            ...          ...            ...          ...  ...   \n",
       "19995     94858682    -0.016700    2742.016393    80.647541  ...   \n",
       "19996    402714114     0.002352   14286.422619   310.574405  ...   \n",
       "19997    189101546    -0.018131    1714.555184    61.234114  ...   \n",
       "19998    285391178    -0.008917    7337.750000   815.305556  ...   \n",
       "19999    112260218     0.000000   15055.157895  1881.894737  ...   \n",
       "\n",
       "       log_searches  Normalized_Searches_delayed  log_searches_delayed  \\\n",
       "0          0.087434                     3.581607              0.000000   \n",
       "1          0.042149                    22.238789             -0.274370   \n",
       "2          0.107400                     6.976213             -0.157064   \n",
       "3         -0.027148                    12.820515             -0.040331   \n",
       "4          0.067479                    12.477146             -0.027148   \n",
       "...             ...                          ...                   ...   \n",
       "19995     -0.090395                     6.174626             -0.115126   \n",
       "19996     -0.452835                     2.459450              0.352010   \n",
       "19997     -0.333604                     6.056291              0.020965   \n",
       "19998     -0.071780                     0.448656              0.182322   \n",
       "19999      0.022537                     0.878210              0.000000   \n",
       "\n",
       "       norm_search_MA_upper_band  norm_search_MA_lower_band  strategy  \\\n",
       "0                            NaN                        NaN         0   \n",
       "1                            NaN                        NaN         0   \n",
       "2                            NaN                        NaN         0   \n",
       "3                            NaN                        NaN         0   \n",
       "4                      12.770230                  12.527431         0   \n",
       "...                          ...                        ...       ...   \n",
       "19995                   6.995337                   6.454482         0   \n",
       "19996                   2.554765                   1.801398         0   \n",
       "19997                   9.224848                   5.842442         0   \n",
       "19998                   0.413758                   0.371719         0   \n",
       "19999                   1.016301                   0.895223        -1   \n",
       "\n",
       "            coin           qty        balance  portfolio_value  \n",
       "0       Ethereum  0.000000e+00   10000.000000     10000.000000  \n",
       "1      Chainlink  0.000000e+00   10000.000000     10000.000000  \n",
       "2         Ripple  0.000000e+00   10000.000000     10000.000000  \n",
       "3         Monero  0.000000e+00   10000.000000     10000.000000  \n",
       "4         Monero  0.000000e+00   10000.000000     10000.000000  \n",
       "...          ...           ...            ...              ...  \n",
       "19995   Polkadot  0.000000e+00  216043.402760    216043.402760  \n",
       "19996     Ripple -2.788529e+03  216040.310270    214724.141335  \n",
       "19997  Chainlink -7.227519e+02  216218.986966    206453.496914  \n",
       "19998   Dogecoin -4.095672e+04  216263.630001    211279.237797  \n",
       "19999  Shiba Inu -7.861123e+08  216263.630001    202899.720326  \n",
       "\n",
       "[20000 rows x 21 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempDF\n",
    "\n",
    "# 2 bugs:\n",
    "# 1. the strategy somehow sells more than it has (shorts a coin)\n",
    "# 2. the portfolio value can be negative when the balance is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previously used chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trend Data for ['Shiba Inu'] retrieved successfully.\n",
      "Stock Data for SHIB-USD retrieved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Calculate correlations for each category\n",
    "volt_del_corr_general = [trend_corr(stock)[0] for stock in general_stocks]\n",
    "volt_del_corr_tech = [trend_corr(stock)[0] for stock in tech_stocks]\n",
    "volt_del_corr_finance = [trend_corr(stock)[0] for stock in finance_stocks]\n",
    "volt_del_corr_crypto = [trend_corr(crypto)[0] for crypto in decentralized_currencies]\n",
    "\n",
    "# Combine the results\n",
    "volt_del_corr = volt_del_corr_general + volt_del_corr_tech + volt_del_corr_finance + volt_del_corr_crypto\n",
    "\n",
    "# Create labels for the scatter plot\n",
    "labels = general_stocks + tech_stocks + finance_stocks + decentralized_currencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scatter plot\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.scatter(range(len(general_stocks)), volt_del_corr_general, color = color_map['general'], label = 'General Stocks')\n",
    "for i in range(len(general_stocks)):\n",
    "    plt.axvline(x = i, color = color_map['general'], linestyle = ':', alpha = 0.3)  # Add vertical lines to separate the stocks\n",
    "plt.scatter(range(len(general_stocks), len(general_stocks) + len(tech_stocks)), volt_del_corr_tech, color = color_map['tech'], label = 'Tech Stocks')\n",
    "for i in range(len(general_stocks), len(general_stocks) + len(tech_stocks)):\n",
    "    plt.axvline(x = i, color = color_map['tech'], linestyle = ':', alpha = 0.3)  # Add vertical lines to separate the stocks\n",
    "plt.scatter(range(len(general_stocks) + len(tech_stocks), len(general_stocks) + len(tech_stocks) + len(finance_stocks)), volt_del_corr_finance, color = color_map['finance'], label = 'Finance Stocks')\n",
    "for i in range(len(general_stocks) + len(tech_stocks), len(general_stocks) + len(tech_stocks) + len(finance_stocks)):\n",
    "    plt.axvline(x = i, color = color_map['finance'], linestyle = ':', alpha = 0.3)  # Add vertical lines to separate the stocks\n",
    "plt.scatter(range(len(general_stocks) + len(tech_stocks) + len(finance_stocks), len(general_stocks) + len(tech_stocks) + len(finance_stocks) + len(decentralized_currencies)), volt_del_corr_crypto, color = color_map['crypto'], label = 'Decentralized Currencies')\n",
    "for i in range(len(general_stocks) + len(tech_stocks) + len(finance_stocks), len(general_stocks) + len(tech_stocks) + len(finance_stocks) + len(decentralized_currencies)):\n",
    "    plt.axvline(x = i, color = color_map['crypto'], linestyle = ':', alpha = 0.3)  # Add vertical lines to separate the stocks\n",
    "plt.axhline(y = 0, color = 'black', linestyle = '--')  # Add a horizontal line at y = 0\n",
    "plt.xlabel('Assets')\n",
    "plt.ylabel('Correlation with 7-Day Delayed Trend')\n",
    "plt.title('Correlation of Close Price and 7-Day Delayed Trend')\n",
    "legend = plt.legend()\n",
    "legend.get_frame().set_alpha(0.3)\n",
    "plt.xticks(range(len(labels)), labels, rotation = 60)\n",
    "plt.tight_layout(pad = 2)\n",
    "plt.savefig('Correlation_Scatter_Plot.png')\n",
    "plt.show()\n",
    "\n",
    "# Print the correlation values and their mean\n",
    "print(volt_del_corr, np.mean(volt_del_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for each stock and decentralized currency\n",
    "for stock in general_stocks + tech_stocks + finance_stocks + decentralized_currencies:\n",
    "    plot_stock_data(stock, download = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
