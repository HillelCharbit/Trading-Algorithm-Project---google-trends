{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas.core.api import Series as Series\n",
    "import matplotlib.pyplot as plt\n",
    "from pytrends.request import TrendReq\n",
    "import yfinance as yf\n",
    "import requests\n",
    "import pytz\n",
    "import datetime\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import warnings as wrn\n",
    "import os\n",
    "from enum import Enum\n",
    "from typing import Tuple\n",
    "import itertools\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general_stocks = ['KO', 'PFE', 'WMT', 'PG', 'JNJ', 'DIS', 'PEP', 'MCD', 'T', 'VZ']\n",
    "# tech_stocks = ['AAPL', 'AMZN', 'MSFT', 'GOOGL', 'NVDA', 'TSLA', 'META', 'INTC', 'IBM', 'AMD']\n",
    "# finance_stocks = ['GS', 'BAC', 'WFC', 'USB', 'JPM', 'MA', 'V', 'AXP', 'C', 'BLK']\n",
    "decentralized_currencies = ['BTC', 'ETH', 'ADA', 'SOL', 'XRP', 'XMR', 'LTC', 'DOT', 'LINK', 'XTZ', 'DOGE']\n",
    "\n",
    "# general_stocks_names = ['Coca-Cola', 'Pfizer', 'Walmart', 'Procter & Gamble', 'Johnson & Johnson', 'Disney', 'Pepsi', 'McDonalds', 'AT&T', 'Verizon']\n",
    "# tech_stocks_names = ['Apple', 'Amazon', 'Microsoft', 'Google', 'Nvidia', 'Tesla', 'Meta', 'Intel', 'IBM', 'AMD']\n",
    "# finance_stocks_names = ['Goldman Sachs', 'Bank of America', 'Wells Fargo', 'US Bancorp', 'JPMorgan Chase', 'Mastercard', 'Visa', 'American Express', 'Citigroup', 'BlackRock']\n",
    "decentralized_currencies_names = ['Bitcoin', 'Ethereum', 'Cardano', 'Solana', 'Ripple', 'Monero', 'Litecoin', 'Polkadot', 'Chainlink', 'Tezos', 'Dogecoin']\n",
    "\n",
    "# color_map = {\n",
    "#     'general': 'deepskyblue',\n",
    "#     'tech': 'limegreen',\n",
    "#     'finance': 'darkorchid',\n",
    "#     'crypto': 'red'\n",
    "# }\n",
    "\n",
    "start = '2019-06-30'\n",
    "end = '2024-07-01'\n",
    "max_lags = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and Classes\n",
    "## get_trends_data\n",
    "gets the trend data using pytrends, given a certain timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trends_data(keyword, \n",
    "                    timeframe=datetime.date.today().strftime('%Y-%m-%d') + ' ' + (datetime.date.today() - datetime.timedelta(days = 269)).strftime('%Y-%m-%d'),\n",
    "                    retries=5, backoff_factor=1.0, verbose=True):\n",
    "    pytrends = TrendReq(hl='en-US', tz=360, timeout=(10,25), )\n",
    "    pytrends.build_payload(keyword, cat = 0, timeframe = timeframe, geo='')\n",
    "    \n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            df = pytrends.interest_over_time()\n",
    "            if df is not None and not df.empty:\n",
    "                if verbose:\n",
    "                    print(f\"Trend Data for {keyword[0]} at timeframe {timeframe} retrieved successfully.\")\n",
    "                df.reset_index(inplace = True)\n",
    "                df.rename(columns = {'date': 'Date', keyword[0]: 'Trend'}, inplace = True)\n",
    "                df['Date'] = pd.to_datetime(df['Date'].dt.strftime('%m/%d/%Y'))\n",
    "                return df\n",
    "            else:\n",
    "                print(\"No data retrieved or DataFrame is empty.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            if \"429\" in str(e):\n",
    "                sleep_time = backoff_factor * (2 ** i)\n",
    "                if verbose:\n",
    "                    print(f\"Rate limit exceeded. Retrying in {sleep_time} seconds...\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                raise(f\"An error occurred: {e}\")\n",
    "    print(\"Failed to retrieve data after several retries.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_stock_data\n",
    "gets the prices of a certain stock in a certain timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_api_call(base_url, endpoint=\"\", method=\"GET\", **kwargs):\n",
    "    # Construct the full URL\n",
    "    full_url = f'{base_url}{endpoint}'\n",
    "\n",
    "    # Make the API call\n",
    "    response = requests.request(method=method, url=full_url, **kwargs)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        return response\n",
    "    else:\n",
    "        # If the request was not successful, raise an exception with the error message\n",
    "        raise Exception(f'API request failed with status code {response.status_code}: {response.text}')\n",
    "\n",
    "def get_binance_historical_data(symbol, interval= '1d', start_date='2019-06-30', end_date='2024-07-01'):\n",
    "    \n",
    "    # define basic parameters for call\n",
    "    base_url = 'https://fapi.binance.com'\n",
    "    endpoint = '/fapi/v1/klines'\n",
    "    method = 'GET'\n",
    "    \n",
    "    # Set the start time parameter in the params dictionary\n",
    "    params = {\n",
    "        'symbol': symbol,\n",
    "        'interval': interval,\n",
    "        'limit': 1500,\n",
    "        'startTime': int(datetime.datetime.strptime(start_date, \"%Y-%m-%d\").timestamp() * 1000), # Start time in milliseconds,\n",
    "        'endTime': int(datetime.datetime.strptime(end_date, \"%Y-%m-%d\").timestamp() * 1000) # End time in milliseconds\n",
    "    }\n",
    "\n",
    "    # Make initial API call to get candles\n",
    "    response = make_api_call(base_url, endpoint=endpoint, method=method, params=params)\n",
    "\n",
    "    candles_data = []\n",
    "\n",
    "    while len(response.json()) > 0:\n",
    "        # Append the received candles to the list\n",
    "        candles_data.extend(response.json())\n",
    "\n",
    "        # Update the start time for the next API call\n",
    "        params['startTime'] = candles_data[-1][0] + 1 # last candle open_time + 1ms\n",
    "\n",
    "        try:\n",
    "            # Make the next API call\n",
    "            response = make_api_call(base_url, endpoint=endpoint, method=method, params=params)\n",
    "        except Exception as e:\n",
    "            raise Exception(f'{symbol} - {e}')\n",
    "\n",
    "    \n",
    "    # Wrap the candles data as a pandas DataFrame\n",
    "    columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'close_time', 'quote_asset_volume',\n",
    "               'number_of_trades', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore']\n",
    "    dtype={\n",
    "    'Date': 'datetime64[ms, Asia/Jerusalem]',\n",
    "    'Open': 'float64',\n",
    "    'High': 'float64',\n",
    "    'Low': 'float64',\n",
    "    'Close': 'float64',\n",
    "    'Volume': 'float64',\n",
    "    'close_time': 'datetime64[ms, Asia/Jerusalem]',\n",
    "    'quote_asset_volume': 'float64',\n",
    "    'number_of_trades': 'int64',\n",
    "    'taker_buy_base_asset_volume': 'float64',\n",
    "    'taker_buy_quote_asset_volume': 'float64',\n",
    "    'ignore': 'float64'\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(candles_data, columns=columns)\n",
    "    df = df.astype(dtype)\n",
    "\n",
    "    df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    return df.drop(columns=['close_time', 'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(ticker, start, end, verbose = True):\n",
    "    currTicker = yf.Ticker(ticker)\n",
    "    tickerDF = currTicker.history(repair = True, start = start, end = end, auto_adjust = False).drop(columns = ['Dividends', 'Stock Splits', 'Repaired?']).reset_index()\n",
    "    if verbose:\n",
    "        print(f\"Stock Data for {ticker} retrieved successfully.\")\n",
    "    tickerDF['Date'] = pd.to_datetime(tickerDF['Date'].dt.strftime('%m/%d/%Y'))\n",
    "    return tickerDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trend_corr\n",
    "gets the data of the trends and prices of the stock given to it in a certain timeframe and calculates the correlation between the log_returns and the trends delayed by certain delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_corr(stock, days = 60, start = '2023-10-01', end = '2024-06-01', delay = 7):\n",
    "    if not os.path.exists(f\"./Data/{stock}_trends({start} - {end}).csv\"):\n",
    "        if stock in general_stocks:\n",
    "            name = general_stocks_names[general_stocks.index(stock)]\n",
    "        elif stock in tech_stocks:\n",
    "            name = tech_stocks_names[tech_stocks.index(stock)]\n",
    "        elif stock in finance_stocks:\n",
    "            name = finance_stocks_names[finance_stocks.index(stock)]\n",
    "        else:\n",
    "            name = decentralized_currencies_names[decentralized_currencies.index(stock)]\n",
    "        t = get_trends_data([name], timeframe = f\"{start} {end}\")\n",
    "        if t is None:\n",
    "            raise Exception(f'Failed to retrieve Trend Data of {stock}.')\n",
    "        t.to_csv(f\"./Data/{stock}_trends({start} - {end}).csv\")\n",
    "    else:\n",
    "        t = pd.read_csv(f\"./Data/{stock}_trends({start} - {end}).csv\")\n",
    "    if not os.path.exists(f\"./Data/{stock}_Prices({start} - {end}).csv\"):\n",
    "        if stock in decentralized_currencies:\n",
    "            p = get_stock_data(f'{stock}-USD', start = start, end = end)\n",
    "        else:\n",
    "            p = get_stock_data(stock, start = start, end = end)\n",
    "        p.to_csv(f\"./Data/{stock}_Prices({start} - {end}).csv\")\n",
    "    else:\n",
    "        p = pd.read_csv(f\"./Data/{stock}_Prices({start} - {end}).csv\")\n",
    "\n",
    "    t['Date'] = pd.to_datetime(t['Date'])\n",
    "    p['Date'] = pd.to_datetime(p['Date'])\n",
    "\n",
    "    full_data = pd.merge(p, t, on='Date')\n",
    "\n",
    "    full_data['log_returns'] = np.log(full_data.Close / full_data.Close.shift(1))\n",
    "    full_data['Volatility'] = full_data['log_returns'].rolling(window=days).std() * np.sqrt(days)\n",
    "\n",
    "    for i in range(1, 8):\n",
    "        full_data[f'Delay_{i}'] = full_data['Trend'].shift(i)\n",
    "\n",
    "    rho = full_data.corr()\n",
    "    rho_c = rho['Close'][f'Delay_{delay}']\n",
    "    return rho_c, full_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot_stock_data\n",
    "gets the data of the trends and the prices of the stock given to it in a certain timeframe and plots its close and its delayed trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stock_data(stock, days = 60, start = '2023-10-01', end = '2024-06-01', delay = 7, download = False):\n",
    "    if not os.path.exists(f\"./Data/{stock}_trends({start} - {end}).csv\"):\n",
    "        if stock in general_stocks:\n",
    "            name = general_stocks_names[general_stocks.index(stock)]\n",
    "        elif stock in tech_stocks:\n",
    "            name = tech_stocks_names[tech_stocks.index(stock)]\n",
    "        elif stock in finance_stocks:\n",
    "            name = finance_stocks_names[finance_stocks.index(stock)]\n",
    "        else:\n",
    "            name = decentralized_currencies_names[decentralized_currencies.index(stock)]\n",
    "        t = get_trends_data([name], timeframe = f\"{start} {end}\")\n",
    "        t.to_csv(f\"./Data/{stock}_trends({start} - {end}).csv\")\n",
    "    else:\n",
    "        t = pd.read_csv(f\"./Data/{stock}_trends({start} - {end}).csv\")\n",
    "    if not os.path.exists(f\"./Data/{stock}_Prices({start} - {end}).csv\"):\n",
    "        p = get_stock_data(stock, start = start, end = end)\n",
    "        p.to_csv(f\"./Data/{stock}_Prices({start} - {end}).csv\")\n",
    "    else:\n",
    "        p = pd.read_csv(f\"./Data/{stock}_Prices({start} - {end}).csv\")\n",
    "\n",
    "    t['Date'] = pd.to_datetime(t['Date'])\n",
    "    p['Date'] = pd.to_datetime(p['Date'])\n",
    "\n",
    "    full_data = pd.merge(p, t, on='Date')\n",
    "\n",
    "    full_data['log_returns'] = np.log(full_data.Close / full_data.Close.shift(1))\n",
    "    full_data['Volatility'] = full_data['log_returns'].rolling(window=days).std() * np.sqrt(days)\n",
    "\n",
    "    full_data[f'Delay_{delay}'] = full_data.Trend.shift(7)\n",
    "\n",
    "    # Determine the color based on the stock category\n",
    "    if stock in general_stocks:\n",
    "        color = color_map['general']\n",
    "        name = general_stocks_names[general_stocks.index(stock)]\n",
    "    elif stock in tech_stocks:\n",
    "        color = color_map['tech']\n",
    "        name = tech_stocks_names[tech_stocks.index(stock)]\n",
    "    elif stock in finance_stocks:\n",
    "        color = color_map['finance']\n",
    "        name = finance_stocks_names[finance_stocks.index(stock)]\n",
    "    else:\n",
    "        color = color_map['crypto']\n",
    "        name = decentralized_currencies_names[decentralized_currencies.index(stock)]\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "    # Plot Close price\n",
    "    axes[0].plot(full_data['Date'], full_data['Close'], label = 'Close Price', color = color)\n",
    "    axes[0].set_xlabel('Date')\n",
    "    axes[0].set_ylabel('Close Price')\n",
    "    axes[0].set_title(f'{stock}: Close Price')\n",
    "    legend = axes[0].legend(loc='upper left')\n",
    "    legend.get_frame().set_alpha(0.3)\n",
    "\n",
    "    # Plot 7-days delay trend\n",
    "    axes[1].plot(full_data['Date'], full_data[f'Delay_{delay}'], label = f'{delay}-Days Delayed Trend', color = 'black')\n",
    "    axes[1].set_xlabel('Date')\n",
    "    axes[1].set_ylabel(f'{delay}-Days Delayed Trend')\n",
    "    axes[1].set_title(f'{stock}: {delay}-Days Delayed Trend')\n",
    "    legend = axes[1].legend(loc='upper right')\n",
    "    legend.get_frame().set_alpha(0.3)\n",
    "\n",
    "    fig.suptitle(f'{name} ({stock})', fontsize=20, verticalalignment = 'bottom', fontweight = 'bold')\n",
    "    plt.tight_layout(pad=2.0)\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    if download:\n",
    "        plt.savefig(f\"./Plots/{stock}_plot({start} - {end}).png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time_jump\n",
    "adds time in days to a given date that was accepted as string, returns as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the date in string format after a certain number of days\n",
    "def time_jump(start, days = 7 * 38):\n",
    "    return (datetime.datetime.strptime(start, '%Y-%m-%d') + datetime.timedelta(days = days)).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_breakpoints\n",
    "calculates the breakpoints needed for the multiple requests of data to make the data as long as possible for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_breakpoints(start, end, days = 7 * 38):\n",
    "    breakpoints = [start]\n",
    "    while datetime.datetime.strptime(breakpoints[-1], '%Y-%m-%d') < datetime.datetime.strptime(end, '%Y-%m-%d'):\n",
    "        temp = time_jump(breakpoints[-1], days)\n",
    "        if datetime.datetime.strptime(temp, '%Y-%m-%d') < datetime.datetime.strptime(end, '%Y-%m-%d'):\n",
    "            breakpoints.append(temp)\n",
    "        else:\n",
    "            breakpoints.append(end)\n",
    "    return breakpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## connectNnormalizeTrends\n",
    "gets the data of the trends and prices daily in the whole time frame and in parts, and connects them by estimating an eproximation of their absolute amount of searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connectNnormalizeTrends(Dfs, stock):\n",
    "    # setting up the data for step 1\n",
    "    if not os.path.exists(f'./Data/glimpse_{stock}_5Y.csv'):\n",
    "        raise Exception(f'Failed to retrieve Glimpse Data of {stock}.')\n",
    "    glmpsDf = pd.read_csv(f'./Data/glimpse_{stock}_5Y.csv')\n",
    "    glmpsDf.rename(columns={'Time (week of)': 'Date', 'Absolute Google Search Volume': 'Absolute_Volume'}, inplace=True)\n",
    "    glmpsDf['Date'] = pd.to_datetime(glmpsDf['Date'])\n",
    "\n",
    "    df_concat = pd.concat(Dfs).reset_index(drop = True)\n",
    "    df_concat['Date'] = pd.to_datetime(df_concat['Date'])\n",
    "\n",
    "    # calculating the mean trend for each week and merging it into glmpsDf\n",
    "    df_concat['MeanTrend'] = df_concat['Trend'].rolling(window=7, min_periods=1).mean().shift(-6)\n",
    "    glmpsDf = pd.merge(glmpsDf, df_concat, on='Date', how='left').drop(columns = ['Trend'])\n",
    "\n",
    "    # setting up the data for step 2\n",
    "    glmpsDf.rename(columns={'Date': 'Date_Week'}, inplace=True)\n",
    "    df_concat = df_concat.drop(columns = ['MeanTrend'])\n",
    "    df_concat['Date_Week'] = (df_concat['Date'] - pd.to_timedelta((df_concat['Date'].dt.weekday + 1) % 7, unit='d')).dt.strftime('%Y-%m-%d')\n",
    "    df_concat['Date_Week'] = pd.to_datetime(df_concat['Date_Week'])\n",
    "\n",
    "    # calculating the ratio and search volume for each week\n",
    "    df_concat = pd.merge(df_concat, glmpsDf[['Date_Week', 'MeanTrend', 'Absolute_Volume']], on='Date_Week', how='left')\n",
    "    df_concat['Ratio'] = df_concat['Trend'] / (df_concat['MeanTrend'] * 7)\n",
    "    df_concat['Search_Volume'] = df_concat['Ratio'] * df_concat['Absolute_Volume']\n",
    "\n",
    "    # adding to df_concat the check ratio to check validity of the data\n",
    "    df_concat['check_ratio'] = df_concat['Search_Volume'] / df_concat['Trend']\n",
    "\n",
    "    # renormalizing the data\n",
    "    df_concat['Normalized_Searches'] = ((df_concat['Search_Volume'] - df_concat['Search_Volume'].min()) / (df_concat['Search_Volume'].max() - df_concat['Search_Volume'].min())) * 100\n",
    "\n",
    "    # cleaning out unnecessary columns\n",
    "    df_concat = df_concat.drop(columns = ['Trend', 'Date_Week', 'MeanTrend', 'Absolute_Volume', 'Ratio'])\n",
    "\n",
    "    df_concat['Date'] = df_concat['Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getNormalizedData\n",
    "gets the data normalized, and performs validity checks (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNormalizedData(stock, ticker, start = '2019-06-23', end = '2024-06-01', weeks = 38, do_double = True, verbose = False):\n",
    "    if weeks > 38:\n",
    "        wrn.warn('The maximum number of weeks is 38. \\nThe number of weeks will be set to 38.', category=Warning)\n",
    "        weeks = 38\n",
    "    if do_double & (weeks % 2 != 0):\n",
    "        wrn.warn('The number of weeks must be even to use the double method. \\nThe number of weeks will be rounded down to the nearest even number.', category=Warning)\n",
    "        weeks -= 1\n",
    "    breakpoints = get_breakpoints(start, end, days = 7 * weeks)\n",
    "    if do_double:\n",
    "        breakpoints2 = get_breakpoints(start, end, days = 7 * weeks / 2)\n",
    "        breakpoints2 = [x for x in breakpoints2 if x not in breakpoints[1:-1]]\n",
    "    \n",
    "    # initializing the list to store the dataframes\n",
    "    Dfs = []\n",
    "\n",
    "    # extracting the data for each time period\n",
    "    for i in range(len(breakpoints) - 1):\n",
    "        startTemp = breakpoints[i]\n",
    "        endTemp = time_jump(breakpoints[i + 1], days=-1)\n",
    "        # checking if the data is already extracted\n",
    "        if not os.path.exists(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\"):\n",
    "            # extracting the data\n",
    "            t = get_trends_data([stock], timeframe = f\"{startTemp} {endTemp}\", verbose = verbose).drop(columns = ['isPartial'])\n",
    "            if t is None:\n",
    "                raise Exception(f'Failed to retrieve Trend Data of {stock}.')\n",
    "            t.to_csv(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\")\n",
    "        else:\n",
    "            t = pd.read_csv(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\").drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "        Dfs.append(t)\n",
    "\n",
    "    df_concat = connectNnormalizeTrends(Dfs, stock)\n",
    "\n",
    "    if do_double:\n",
    "        # initializing the list to store the dataframes\n",
    "        Dfs2 = []\n",
    "\n",
    "        # extracting the data for each time period\n",
    "        for i in range(len(breakpoints2) - 1):\n",
    "            startTemp = breakpoints2[i]\n",
    "            endTemp = time_jump(breakpoints2[i + 1], days=-1)\n",
    "            # checking if the data is already extracted\n",
    "            if not os.path.exists(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\"):\n",
    "                # extracting the data\n",
    "                t = get_trends_data([stock], timeframe = f\"{startTemp} {endTemp}\", verbose = verbose).drop(columns = ['isPartial'])\n",
    "                if t is None:\n",
    "                    raise Exception(f'Failed to retrieve Trend Data of {stock}.')\n",
    "                t.to_csv(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\")\n",
    "            else:\n",
    "                t = pd.read_csv(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\").drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "            Dfs2.append(t)\n",
    "        \n",
    "        df_concat2 = connectNnormalizeTrends(Dfs2, stock)\n",
    "    \n",
    "        df_concat[\"Normalized_Searches\"] = (df_concat[\"Normalized_Searches\"] + df_concat2[\"Normalized_Searches\"]) / 2\n",
    "    \n",
    "    if not os.path.exists(f\"./Data/{stock}_Prices({start}-{end}).csv\"):\n",
    "        stockData = get_binance_historical_data(f'{ticker}USDT', start_date=start, end_date=end)\n",
    "        stockData.to_csv(f\"./Data/{stock}_Prices({start}-{end}).csv\")\n",
    "    else:\n",
    "        stockData = pd.read_csv(f\"./Data/{stock}_Prices({start}-{end}).csv\").drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "    df_concat[df_concat.Normalized_Searches == 0] = 0.1\n",
    "\n",
    "    df_concat['log_searches'] = np.log(df_concat.Normalized_Searches / df_concat.Normalized_Searches.shift(1))\n",
    "    stockData['log_returns'] = np.log(stockData.Close / stockData.Close.shift(1))\n",
    "\n",
    "    try:\n",
    "        df_concat['Date'] = df_concat['Date'].dt.strftime('%Y-%m-%d')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        stockData['Date'] = stockData['Date'].dt.strftime('%Y-%m-%d')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    finalDf = pd.merge(stockData, df_concat, on='Date', how='left').dropna()\n",
    "\n",
    "    if verbose:\n",
    "        # data normalization validity check\n",
    "        top = 0\n",
    "        bottom = 0\n",
    "        for i in range(len(Dfs)):\n",
    "            top += Dfs[i].shape[0]\n",
    "            print(f'period {i + 1}:\\n=========\\nmean: {df_concat.iloc[bottom:top]['check_ratio'].mean():.4f}\\nsd: {df_concat.iloc[bottom:top][\"check_ratio\"].std():.4f}\\n\\nmean/sd: {df_concat.iloc[bottom:top]['check_ratio'].mean()/df_concat.iloc[bottom:top][\"check_ratio\"].std():.4f}\\n')\n",
    "            bottom += Dfs[i].shape[0]\n",
    "\n",
    "        if do_double:\n",
    "            top = 0\n",
    "            bottom = 0\n",
    "            for i in range(len(Dfs2)):\n",
    "                top += Dfs2[i].shape[0]\n",
    "                print(f'period {i + len(Dfs) + 1}:\\n=========\\nmean: {df_concat.iloc[bottom:top]['check_ratio'].mean():.4f}\\nsd: {df_concat.iloc[bottom:top][\"check_ratio\"].std():.4f}\\n\\nmean/sd: {df_concat.iloc[bottom:top]['check_ratio'].mean()/df_concat.iloc[bottom:top][\"check_ratio\"].std():.4f}\\n')\n",
    "                bottom += Dfs2[i].shape[0]\n",
    "    return finalDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate_strategy\n",
    "prints and returns the evaluation of the strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_total_return(portfolio_values: pd.Series):\n",
    "    return (portfolio_values.iloc[-1] / portfolio_values.iloc[0]) - 1.0\n",
    "\n",
    "def calc_annualized_return(portfolio_values):\n",
    "    portfolio_trading_years = portfolio_values.shape[0] / 365\n",
    "    return (portfolio_values.iloc[-1] / portfolio_values.iloc[0])**(1/portfolio_trading_years) - 1.0\n",
    "\n",
    "def calc_annualized_sharpe(portfolio_values: pd.Series, rf: float=0.0):\n",
    "    annualized_return = calc_annualized_return(portfolio_values)\n",
    "    annualized_std = portfolio_values.pct_change().std() * np.sqrt(365)\n",
    "    if annualized_std is None or annualized_std == 0:\n",
    "        return 0\n",
    "    return (annualized_return - rf) / annualized_std\n",
    "\n",
    "def calc_downside_deviation(portfolio_values):\n",
    "    porfolio_returns = portfolio_values.pct_change().dropna()\n",
    "    return porfolio_returns[porfolio_returns < 0].std()\n",
    "\n",
    "def calc_sortino(portfolio_values, rf=0.0):\n",
    "    down_deviation = calc_downside_deviation(portfolio_values) * np.sqrt(365)\n",
    "    annualized_return = calc_annualized_return(portfolio_values)\n",
    "    if down_deviation is None or down_deviation == 0:\n",
    "        return 0\n",
    "    return (annualized_return - rf) / down_deviation\n",
    "\n",
    "def calc_max_drawdown(portfolio_values):\n",
    "    cumulative_max = portfolio_values.cummax()\n",
    "    drawdown = (cumulative_max - portfolio_values) / cumulative_max\n",
    "    return drawdown.max()\n",
    "\n",
    "def calc_calmar(portfolio_values):\n",
    "    max_drawdown = calc_max_drawdown(portfolio_values)\n",
    "    annualized_return = calc_annualized_return(portfolio_values)\n",
    "    return annualized_return / max_drawdown\n",
    "\n",
    "def evaluate_strategy(df, strat_name, returns = True, verbose = True):\n",
    "    \n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    first_date = df['Date'].iloc[0]\n",
    "    df_rest = df[df['Date'] != first_date].groupby('Date').last().reset_index()\n",
    "\n",
    "    df = pd.concat([df[df['Date'] == first_date].head(1), df_rest]).reset_index(drop = True)\n",
    "    df = df.sort_values(by = 'Date').reset_index(drop = True)\n",
    "\n",
    "    total_return = calc_total_return(df['portfolio_value'])\n",
    "    annualized_return = calc_annualized_return(df['portfolio_value'])\n",
    "    annualized_sharpe = calc_annualized_sharpe(df['portfolio_value'])\n",
    "    sortino_ratio = calc_sortino(df['portfolio_value'])\n",
    "    max_drawdown = calc_max_drawdown(df['portfolio_value'])\n",
    "    calmar_ratio = calc_calmar(df['portfolio_value'])\n",
    "\n",
    "    if verbose:    \n",
    "        trString = f\"{total_return:.2%}\"\n",
    "        asString = f\"{annualized_return:.2f}\"\n",
    "        mdString = f\"{max_drawdown:.2%}\"\n",
    "        print(f\"Results for {strat_name}:\")\n",
    "        print(f\"  Total Return:            {trString:8s}  Annualized Return: {annualized_return:.2%}\")\n",
    "        print(f\"  Annualized Sharpe Ratio: {asString:8s}  Sortino Ratio:     {sortino_ratio:.2f}\")\n",
    "        print(f\"  Max Drawdown:            {mdString:8s}  Calmar Ratio:      {calmar_ratio:.2f}\")\n",
    "\n",
    "    if returns:\n",
    "        return total_return, annualized_return, annualized_sharpe, sortino_ratio, max_drawdown, calmar_ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting the dataframes\n",
    "so they could be interpeted as one complete investment plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfConnect(DFs: list) -> pd.DataFrame:\n",
    "    combinedDF = pd.concat(DFs).reset_index(drop = True, inplace = False)\n",
    "    combinedDF['TempDate'] = pd.to_datetime(combinedDF['Date'])\n",
    "    combinedDF = combinedDF.sort_values(by = 'TempDate').reset_index(drop = True, inplace = False)\n",
    "    try:\n",
    "        combinedDF = combinedDF.drop(columns = ['TempDate', 'index'])\n",
    "    except:\n",
    "        combinedDF = combinedDF.drop(columns = ['TempDate'])\n",
    "    return combinedDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enums\n",
    "class ActionType(Enum):\n",
    "    BUY = 1\n",
    "    SELL = -1\n",
    "    DONOTHING = 0\n",
    "\n",
    "# our strategy class\n",
    "class Strategy():\n",
    "    def __init__(self, bb_window: int, rsi_window: int, \n",
    "                 bb_window_min: int=None, bb_threshold: float=0.5, rsi_window_min: int=None, rsi_limits: Tuple[float,float]=[30.0, 70.0], sell_all: bool=False, qty_scale: float=0.1,\n",
    "                 commission_type: object=\"scalar\", slippage_factor=np.inf) -> None:\n",
    "        self.bb_window = bb_window\n",
    "        self.rsi_window = rsi_window\n",
    "        if bb_window_min is None:\n",
    "            bb_window_min = bb_window\n",
    "        else:\n",
    "            if bb_window_min > bb_window:\n",
    "                raise ValueError(\"bb_window_min should be less than or equal to bb_window\")\n",
    "            if bb_window_min <= 0:\n",
    "                raise ValueError(\"bb_window_min should be a value greater than 0\")\n",
    "            self.bb_window_min = bb_window_min\n",
    "        if bb_threshold <= 0:\n",
    "            raise ValueError(\"bb_threshold should be a value greater than 0\")\n",
    "        self.bb_threshold = bb_threshold\n",
    "        if rsi_window_min is None:\n",
    "            rsi_window_min = rsi_window\n",
    "        else:\n",
    "            if rsi_window_min > rsi_window:\n",
    "                raise ValueError(\"rsi_window_min should be less than or equal to rsi_window\")\n",
    "            if rsi_window_min <= 0:\n",
    "                raise ValueError(\"rsi_window_min should be a value greater than 0\")\n",
    "            self.rsi_window_min = rsi_window_min\n",
    "        if rsi_limits[0] <= 0 or rsi_limits[1] > 100 or rsi_limits[0] >= rsi_limits[1]:\n",
    "            raise ValueError(\"rsi_limit should be a tuple of two values between 0 and 100 where the first value is less than the second\")\n",
    "        self.rsi_limits = rsi_limits\n",
    "        if commission_type not in [\"scalar\", \"precentage\"]:\n",
    "            raise ValueError(\"commision_type should be either 'scalar' or 'precentage'\")\n",
    "        self.commission_type = commission_type\n",
    "        self.sell_all = sell_all\n",
    "        self.slippage_factor = slippage_factor\n",
    "        if qty_scale <= 0:\n",
    "            raise ValueError(\"qty_scale should be a value greater than 0\")\n",
    "        self.qty_scale = qty_scale\n",
    "\n",
    "    def calc_signal(self, data: pd.DataFrame) -> None:\n",
    "        norm_search = data['Normalized_Searches_delayed']\n",
    "        close = data['Close']\n",
    "\n",
    "        # MA of search volume as threshold for Bollinger Bands\n",
    "        norm_search_MA = norm_search.rolling(window=self.bb_window, min_periods=self.bb_window_min).mean()\n",
    "        norm_search_sd = norm_search.rolling(window=self.bb_window, min_periods=self.bb_window_min).std()\n",
    "        upper_band = norm_search_MA + self.bb_threshold * norm_search_sd\n",
    "        lower_band = norm_search_MA - self.bb_threshold * norm_search_sd\n",
    "\n",
    "        data['norm_search_MA_upper_band'] = upper_band\n",
    "        data['norm_search_MA_lower_band'] = lower_band\n",
    "\n",
    "        # RSI of close price as threshold limiting the Bollinger Bands signal\n",
    "        delta = close.diff()\n",
    "        gains = delta.where(delta > 0, 0)\n",
    "        losses = -delta.where(delta < 0, 0)\n",
    "        avg_gain = gains.rolling(window=self.rsi_window, min_periods=self.rsi_window_min).mean()\n",
    "        avg_loss = losses.rolling(window=self.rsi_window, min_periods=self.rsi_window_min).mean()\n",
    "        rs = avg_gain / avg_loss\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "        data['strategy'] = 0\n",
    "        data.loc[(norm_search < lower_band) & (rsi < self.rsi_limits[0]), 'strategy'] = -1 # Sell signal\n",
    "        data.loc[(norm_search > upper_band) & (rsi > self.rsi_limits[1]), 'strategy'] = 1 # Buy signal\n",
    "\n",
    "    def calc_realistic_price(self, row: pd.Series, action: ActionType) -> float:\n",
    "        slippage_rate = ((row['Close'] - row['Open']) / row['Open']) / self.slippage_factor\n",
    "        slippage_price = row['Open'] + row['Open'] * slippage_rate\n",
    "\n",
    "        if action == ActionType.BUY:\n",
    "            return max(slippage_price, row['Open'])\n",
    "        elif action == ActionType.SELL:\n",
    "            return min(slippage_price, row['Open'])\n",
    "        else:\n",
    "            return slippage_price\n",
    "\n",
    "# comparison strategy class\n",
    "class BuyAndHoldStrategy():\n",
    "    def __init__(self, sell_all: bool=False, qty_scale: float=1.0,\n",
    "                 commission_type: object=\"scalar\", slippage_factor=np.inf) -> None:\n",
    "        if commission_type not in [\"scalar\", \"precentage\"]:\n",
    "            raise ValueError(\"commision_type should be either 'scalar' or 'precentage'\")\n",
    "        self.commission_type = commission_type\n",
    "        self.sell_all = sell_all\n",
    "        self.slippage_factor = slippage_factor\n",
    "        if qty_scale <= 0:\n",
    "            raise ValueError(\"qty_scale should be a value greater than 0\")\n",
    "        self.qty_scale = qty_scale\n",
    "    \n",
    "    def calc_signal(self, data: DataFrame) -> None:\n",
    "        data['strategy'] = 0 # Do nothing\n",
    "        data.iloc[0, data.columns.get_loc('strategy')] = 1 # Buy signal\n",
    "        data.iloc[-1, data.columns.get_loc('strategy')] = -1 # Sell signal\n",
    "\n",
    "    def calc_realistic_price(self, row: Series, action: ActionType) -> float:\n",
    "        slippage_rate = ((row['Close'] - row['Open']) / row['Open']) / self.slippage_factor\n",
    "        slippage_price = row['Open'] + row['Open'] * slippage_rate\n",
    "\n",
    "        if action == ActionType.BUY:\n",
    "            return max(slippage_price, row['Open'])\n",
    "        elif action == ActionType.SELL:\n",
    "            return min(slippage_price, row['Open'])\n",
    "        else:\n",
    "            return slippage_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backtest\n",
    "backtesting function that can be applied on different strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(DFs: list, strategy: Strategy, starting_balance: float, include: list, commission: float=0.0, hoddle: bool=False) -> pd.DataFrame:\n",
    "    for i in range(len(DFs)):\n",
    "        strategy.calc_signal(DFs[i])\n",
    "        DFs[i]['coin'] = decentralized_currencies_names[include[i]]\n",
    "        DFs[i].reset_index(drop=True)\n",
    "\n",
    "    # memory dataframe\n",
    "    mem = pd.DataFrame({'coin': [decentralized_currencies_names[include[i]] for i in range(len(DFs))], \n",
    "                        'qty': 0.0,\n",
    "                        'price': 0.0})\n",
    "\n",
    "    data = dfConnect(DFs)\n",
    "    data['qty'] = 0.0\n",
    "    data['balance'] = 0.0\n",
    "    data['coins_value'] = 0.0\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        # Get the current balance and qty before the action\n",
    "        curr_qty = mem.loc[mem.coin == data.loc[i, 'coin'], 'qty'].values[0]\n",
    "        prev_price = mem.loc[mem.coin == data.loc[i, 'coin'], 'price'].values[0]\n",
    "        new_price = strategy.calc_realistic_price(row, ActionType.BUY if row['strategy'] == 1 else ActionType.DONOTHING if ((row['strategy'] != -1) | (row['Date'] == data.Date.iloc[-1])) else ActionType.SELL)\n",
    "        mem.loc[mem.coin == data.loc[i, 'coin'], 'price'] = new_price\n",
    "        curr_balance = data.loc[i - 1, 'balance'] + (new_price - prev_price) * curr_qty if i > 0 else starting_balance\n",
    "        if curr_balance < 0:\n",
    "            new_price = strategy.calc_realistic_price(row, ActionType.SELL)\n",
    "            curr_balance = data.loc[i - 1, 'balance'] + (new_price - prev_price) * curr_qty if i > 0 else starting_balance\n",
    "        curr_coin_value = data.loc[i - 1, 'coins_value'] + (new_price - prev_price) * curr_qty if i > 0 else 0.0\n",
    "\n",
    "        # Sell signal when strategy says so or at the end of trade and holding any stock\n",
    "        if (curr_qty > 0) & ((row['Date'] == data.Date.iloc[-1]) | (row['strategy'] == -1) | (curr_balance < 0)):\n",
    "            if hoddle:\n",
    "                sell_qty = curr_qty\n",
    "            else:\n",
    "                sell_qty = curr_qty if (strategy.sell_all | (row['Date'] == data.Date.iloc[-1])) else min((row['norm_search_MA_lower_band'] / row['Normalized_Searches_delayed'] - 1) * strategy.qty_scale, curr_qty)\n",
    "            data.loc[i, 'balance'] = curr_balance + (new_price * sell_qty - commission if strategy.commission_type == \"scalar\" else new_price * sell_qty * (1 - commission))\n",
    "            data.loc[i, 'qty'] = 0.0 if (strategy.sell_all | (row['Date'] == data.Date.iloc[-1])) else curr_qty - sell_qty\n",
    "            mem.loc[mem.coin == data.loc[i, 'coin'], 'qty'] = 0.0 if (strategy.sell_all | (row['Date'] == data.Date.iloc[-1])) else curr_qty - sell_qty\n",
    "            data.loc[i, 'coins_value'] = curr_coin_value - ((new_price - prev_price) * sell_qty)\n",
    "\n",
    "        # Buy signal when strategy says so as long not in the end of trade data and not holding any stock\n",
    "        elif (row['Date'] != data.Date.iloc[-1]) & (data.loc[i, 'strategy'] == 1) & (curr_balance > 0):\n",
    "            if hoddle:\n",
    "                buy_qty = (starting_balance / len(DFs)) / new_price\n",
    "            else:\n",
    "                buy_qty = min((row['Normalized_Searches_delayed'] / row['norm_search_MA_upper_band'] - 1) * strategy.qty_scale, (curr_balance - commission) / new_price if strategy.commission_type == \"scalar\" else curr_balance / (new_price * (1 + commission)))\n",
    "            data.loc[i, 'balance'] = curr_balance - (new_price * buy_qty + commission if strategy.commission_type == \"scalar\" else new_price * buy_qty * (1 + commission))\n",
    "            data.loc[i, 'qty'] = curr_qty + buy_qty\n",
    "            mem.loc[mem.coin == data.loc[i, 'coin'], 'qty'] = curr_qty + buy_qty\n",
    "            data.loc[i, 'coins_value'] = curr_coin_value + ((new_price - prev_price) * buy_qty)\n",
    "\n",
    "        # Do nothing\n",
    "        else:\n",
    "            data.loc[i, 'balance'] = curr_balance\n",
    "            data.loc[i, 'qty'] = curr_qty\n",
    "            data.loc[i, 'coins_value'] = curr_coin_value\n",
    "        \n",
    "    data['portfolio_value'] = data['balance'] + data['coins_value']\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrainTestLog(file = './log.txt'):\n",
    "    total_retruns, annualized_returns, annualized_sharpes, sortino_ratios, max_drawdowns, calmar_ratios = [], [], [], [], [], []\n",
    "    first = True\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            if first:\n",
    "                first = False\n",
    "                continue\n",
    "            total_retruns.append(float(line.split()[1][-2]) / 100)\n",
    "            annualized_returns.append(float(line.split()[3][-2]) / 100)\n",
    "            annualized_sharpes.append(float(line.split()[5][-1]))\n",
    "            sortino_ratios.append(float(line.split()[7][-1]))\n",
    "            max_drawdowns.append(float(line.split()[9][-2]) / 100)\n",
    "            calmar_ratios.append(float(line.split()[11][-1]))\n",
    "    return total_retruns, annualized_returns, annualized_sharpes, sortino_ratios, max_drawdowns, calmar_ratios\n",
    "\n",
    "def writeTrainTestLog(total_return, annualized_return, annualized_sharpe, sortino_ratio, max_drawdown, calmar_ratio, filePath = './log.txt'):\n",
    "    if not os.path.exists(filePath):\n",
    "        with open(filePath, 'a') as file:\n",
    "            file.write('total_return annualized_return annualized_sharpe sortino_ratio max_drawdown calmar_ratio\\n')\n",
    "    with open(filePath, 'a') as file:\n",
    "        file.write(f'tr {total_return:.2%} ar {annualized_return:.2%} as {annualized_sharpe:.2f} sr {sortino_ratio:.2f} md {max_drawdown:.2%} cr {calmar_ratio:.2f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Granger Causality Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_granger(DFs: list, max_lags: int = 7, verbose: bool = True) -> Tuple[list, list]:\n",
    "    wrn.filterwarnings('ignore', category=UserWarning)\n",
    "    wrn.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "    lags = []\n",
    "    p_values = []\n",
    "\n",
    "    for i in range(len(DFs)):\n",
    "        try:\n",
    "            cause = grangercausalitytests(DFs[i][['log_returns', 'log_searches']], \n",
    "                                          maxlag = max_lags, \n",
    "                                          verbose = False)\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Failed to perform Granger Causality Test on {decentralized_currencies_names[i]}: {e}')\n",
    "\n",
    "        min_p_value = float('inf')\n",
    "        min_p_lag = None\n",
    "\n",
    "        for lag, result in cause.items():\n",
    "            p_value = result[0]['ssr_ftest'][1]\n",
    "            if p_value < min_p_value:\n",
    "                min_p_value = p_value\n",
    "                min_p_lag = lag\n",
    "\n",
    "        lags.append(min_p_lag)\n",
    "        p_values.append(min_p_value)\n",
    "\n",
    "        if verbose:\n",
    "            print(f'{decentralized_currencies_names[i]}: {min_p_lag} lags, p-value = {min_p_value:.4f}')\n",
    "    \n",
    "    return lags, p_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting & Connecting The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrn.filterwarnings('ignore', category=UserWarning)\n",
    "wrn.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "coinsDFs = []\n",
    "\n",
    "for i in range(len(decentralized_currencies)):\n",
    "    coinsDFs.append(getNormalizedData(decentralized_currencies_names[i], decentralized_currencies[i], \n",
    "                                      start = start, end = end, \n",
    "                                      do_double = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtesting\n",
    "## description\n",
    "### Data Split\n",
    "* 4 years train\n",
    "* 1 year test\n",
    "### hyper-parameters:\n",
    "* bb_window: size of the window of MA of the normalized searches and the bollinger bands on that MA\n",
    "* bb_window_min: minimum size of the window of MA of the normalized searches and the bollinger bands on that MA\n",
    "* bb_threshold: how many sds should be added to/subtracted from the bollinger bands\n",
    "* rsi_window: size of the window of the RSI on the Close\n",
    "* rsi_window_min: minimum size of the window of RSI on the Close\n",
    "* rsi_limits: how sensitive the rsi is, \n",
    "    - the higher the lower value (the first value of rsi_limits) the less the RSI blocks a BUY signal\n",
    "    - the lower the higher value (the second value of rsi_limits) the less the RSI blocks a SELL signal\n",
    "* sell_all: a boolian value that indicates whether the strategy says to sell all of the quantity of the asset when it gives a SELL signal or the strategy says to sell some relative amount of the quantity when it gives a SELL signal\n",
    "* qty_scale: represents the scale of the quantity that is calculated by the strategy\n",
    "### additional parameters:\n",
    "* starting_balance: the starting balance\n",
    "* commission: the commission for every market dealing\n",
    "* commission_type: whether the commission is a set amount or some precentage from the price\n",
    "* slippage_factor: how much slippage exists in the market \n",
    "## Defining Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance = 10000\n",
    "\n",
    "# hyperparameters\n",
    "bb_window = [10, 14, 18]\n",
    "rsi_window = [21, 28, 35]\n",
    "bb_threshold = [0.5, 1, 1.4, 1.7, 2]\n",
    "rsi_limits = [[20.0, 80.0], [25.0, 75.0], [30.0, 70.0]]\n",
    "qty_scale = [0.1, 0.7, 1.2, 1.6]\n",
    "max_p_value = [0.1, 0.15, 0.2]\n",
    "\n",
    "# perm parameters\n",
    "bb_window_min = 1\n",
    "rsi_window_min = 1\n",
    "sell_all = True\n",
    "commission = 0.0001\n",
    "commission_type = \"precentage\"\n",
    "slippage_factor = 1.0\n",
    "max_lags = 7\n",
    "cutoff = '2023-07-01'\n",
    "\n",
    "# permute the hyperparameters\n",
    "params = list(itertools.product(bb_window, rsi_window, bb_threshold, rsi_limits, qty_scale, max_p_value))\n",
    "\n",
    "amntOparams = len(params)\n",
    "\n",
    "fill_stages = [' ', '\\u2591', '\\u2592', '\\u2593', '\\u2588']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrn.filterwarnings('ignore')\n",
    "\n",
    "start_time = time.time()\n",
    "prev_time = start_time\n",
    "\n",
    "if os.path.exists('./trainLog.txt'):\n",
    "    train_total_returns, train_annualized_returns, train_annualized_sharpes, train_sortino_ratios, train_max_drawdowns, train_calmar_ratios = readTrainTestLog('./trainLog.txt')\n",
    "    test_total_returns, test_annualized_returns, test_annualized_sharpes, test_sortino_ratios, test_max_drawdowns, test_calmar_ratios = readTrainTestLog('./testLog.txt')\n",
    "else:\n",
    "    train_total_returns, train_annualized_returns, train_annualized_sharpes, train_sortino_ratios, train_max_drawdowns, train_calmar_ratios = [], [], [], [], [], []\n",
    "    test_total_returns, test_annualized_returns, test_annualized_sharpes, test_sortino_ratios, test_max_drawdowns, test_calmar_ratios = [], [], [], [], [], []\n",
    "\n",
    "starting_pos = len(train_total_returns)\n",
    "\n",
    "runTimeList = []\n",
    "\n",
    "for i, param in enumerate(params[starting_pos:]):\n",
    "    bb_window, rsi_window, bb_threshold, rsi_limits, qty_scale, max_p_value = param\n",
    "\n",
    "    coinsTrainDfs = []\n",
    "    coinsTestDfs = []\n",
    "\n",
    "    for j, df in enumerate(coinsDFs):\n",
    "        df['temp_date'] = pd.to_datetime(df['Date'])\n",
    "        coinsTrainDfs.append(df[df['temp_date'] <= pd.to_datetime(cutoff)])\n",
    "        coinsTestDfs.append(df[df['temp_date'] > pd.to_datetime(cutoff)])\n",
    "        df.drop(columns = ['temp_date'], inplace = True)\n",
    "\n",
    "    lags, p_values = do_granger(coinsTrainDfs.copy(), max_lags=max_lags, verbose=False)\n",
    "\n",
    "    for j in range(len(coinsTrainDfs)):\n",
    "        coinsTrainDfs[j]['Normalized_Searches_delayed'] = coinsTrainDfs[j]['Normalized_Searches'].shift(lags[j])\n",
    "        coinsTrainDfs[j].dropna(inplace = True)\n",
    "        coinsTestDfs[j]['Normalized_Searches_delayed'] = coinsTestDfs[j]['Normalized_Searches'].shift(lags[j])\n",
    "        coinsTestDfs[j].dropna(inplace = True)\n",
    "\n",
    "    include = [j for j, r in enumerate(np.array(p_values) < max_p_value)if r]\n",
    "    coinsIncluded = [decentralized_currencies_names[j] for j in include]\n",
    "    coinsTrainDfs = [coinsTrainDfs[j] for j in include]\n",
    "    coinsTestDfs = [coinsTestDfs[j] for j in include]\n",
    "\n",
    "    strat = Strategy(bb_window=bb_window, rsi_window=rsi_window, bb_window_min=bb_window_min, bb_threshold=bb_threshold, rsi_window_min=rsi_window_min, rsi_limits=rsi_limits, sell_all=sell_all, qty_scale=qty_scale, commission_type=commission_type, slippage_factor=slippage_factor)\n",
    "    stratTrainDf = backtest(coinsTrainDfs.copy(), strat, balance, include, commission)\n",
    "    stratTestDf = backtest(coinsTestDfs.copy(), strat, balance, include, commission)\n",
    "\n",
    "    print(f'==================================================================\\nStrategy hyperparameters:\\n  bb_window:   {f'{bb_window}':13s}  rsi_window: {f'{rsi_window}':5s}  bb_threshold: {bb_threshold}\\n  rsi_limits:  {f'{rsi_limits}':13s}  sell_all:   {f'{sell_all}':5s}  qty_scale:    {qty_scale}\\n  max_p_value:  {f'{max_p_value}':13s}\\n==================================================================\\n')\n",
    "\n",
    "    train_total_return, train_annualized_return, train_annualized_sharpe, train_sortino_ratio, train_max_drawdown, train_calmar_ratio = evaluate_strategy(stratTrainDf.copy(), 'Train')\n",
    "    print()\n",
    "    test_total_return, test_annualized_return, test_annualized_sharpe, test_sortino_ratio, test_max_drawdown, test_calmar_ratio = evaluate_strategy(stratTestDf.copy(), 'Test')\n",
    "    print()\n",
    "\n",
    "    train_total_returns.append(train_total_return)\n",
    "    train_annualized_returns.append(train_annualized_return)\n",
    "    train_annualized_sharpes.append(train_annualized_sharpe)\n",
    "    train_sortino_ratios.append(train_sortino_ratio)\n",
    "    train_max_drawdowns.append(train_max_drawdown)\n",
    "    train_calmar_ratios.append(train_calmar_ratio)\n",
    "\n",
    "    test_total_returns.append(test_total_return)\n",
    "    test_annualized_returns.append(test_annualized_return)\n",
    "    test_annualized_sharpes.append(test_annualized_sharpe)\n",
    "    test_sortino_ratios.append(test_sortino_ratio)\n",
    "    test_max_drawdowns.append(test_max_drawdown)\n",
    "    test_calmar_ratios.append(test_calmar_ratio)\n",
    "\n",
    "    writeTrainTestLog(train_total_return, train_annualized_return, train_annualized_sharpe, train_sortino_ratio, train_max_drawdown, train_calmar_ratio, filePath = './trainLog.txt')\n",
    "    writeTrainTestLog(test_total_return, test_annualized_return, test_annualized_sharpe, test_sortino_ratio, test_max_drawdown, test_calmar_ratio, filePath = './testLog.txt')\n",
    "\n",
    "    print(f'Coins included ({len(coinsIncluded)}):\\n=============================================')\n",
    "    for j in range(len(coinsIncluded)):\n",
    "        print(f'  \\u2022 {coinsIncluded[j]:10s}', end = '\\n' if (j + 1) % 3 == 0 else '  ')\n",
    "\n",
    "    if len(coinsIncluded) % 3 != 0:\n",
    "        print()\n",
    "\n",
    "    curr_time = time.time()\n",
    "    elapsed = round(curr_time - start_time, 2)\n",
    "    if elapsed > 60:\n",
    "        elapsed_mins = elapsed // 60\n",
    "        elapsed_secs = elapsed % 60\n",
    "        if elapsed_mins > 60:\n",
    "            elapsed_hours = elapsed_mins // 60\n",
    "            elapsed_mins = elapsed_mins % 60\n",
    "            time_elapsed = f'{elapsed_hours:.0f}h {elapsed_mins:.0f}m {elapsed_secs:.2f}s'\n",
    "        else:\n",
    "            time_elapsed = f'{elapsed_mins:.0f}m {elapsed_secs:.2f}s'\n",
    "    else:\n",
    "        time_elapsed = f'{elapsed:.2f}s'\n",
    "    from_last = round(curr_time - prev_time, 2)\n",
    "\n",
    "    runTimeList.append(from_last)\n",
    "\n",
    "    avgRunTime = np.array(runTimeList).mean().round(2)\n",
    "\n",
    "    estimated = round(avgRunTime * (amntOparams - i - 1 - starting_pos), 2)\n",
    "\n",
    "    if estimated > 60:\n",
    "        estimated_mins = estimated // 60\n",
    "        estimated_secs = estimated % 60\n",
    "        if estimated_mins > 60:\n",
    "            estimated_hours = estimated_mins // 60\n",
    "            estimated_mins = estimated_mins % 60\n",
    "            estimatedTime = f'{estimated_hours:.0f}h {estimated_mins:.0f}m {estimated_secs:.2f}s'\n",
    "        else:\n",
    "            estimatedTime = f'{estimated_mins:.0f}m {estimated_secs:.2f}s'\n",
    "    else:\n",
    "        estimatedTime = f'{estimated:.2f}s'\n",
    "\n",
    "    print(f'\\nTime elapsed: {time_elapsed}, Time from previous run: {from_last}s\\nAverage run time: {avgRunTime}s, Estimated time to finish: {estimatedTime}\\n{i + starting_pos + 1}/{amntOparams} = {((i + starting_pos + 1) / amntOparams):.2%}\\n[{fill_stages[4] * (math.floor(((i + starting_pos + 1) / amntOparams) * 100) // 4)}{fill_stages[(math.ceil(((i + starting_pos + 1) / amntOparams) * 100) - 1) % 4]}{' ' * (25 - math.ceil(((i + starting_pos + 1) / amntOparams) * 100) // 4)}]\\n')\n",
    "    prev_time = curr_time\n",
    "\n",
    "bb_windows, rsi_windows, bb_thresholds, rsi_limitss, qty_scales, max_p_values = zip(*params)\n",
    "trainResultsDf = pd.DataFrame({'total_return': train_total_returns, 'annualized_return': train_annualized_returns, 'annualized_sharpe': train_annualized_sharpes, 'sortino_ratio': train_sortino_ratios, 'max_drawdown': train_max_drawdowns, 'calmar_ratio': train_calmar_ratios, 'bb_window': bb_windows, 'rsi_window': rsi_windows, 'bb_threshold': bb_thresholds, 'rsi_limits': rsi_limitss, 'qty_scale': qty_scales, 'max_p_value': max_p_values})\n",
    "testResultsDf = pd.DataFrame({'total_return': test_total_returns, 'annualized_return': test_annualized_returns, 'annualized_sharpe': test_annualized_sharpes, 'sortino_ratio': test_sortino_ratios, 'max_drawdown': test_max_drawdowns, 'calmar_ratio': test_calmar_ratios, 'bb_window': bb_windows, 'rsi_window': rsi_windows, 'bb_threshold': bb_thresholds, 'rsi_limits': rsi_limitss, 'qty_scale': qty_scales, 'max_p_value': max_p_values})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtesting End & it's Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison over the total_return and annual return\n",
      "\n",
      "==================================================================\n",
      "Our Strategy hyperparameters:\n",
      "  bb_window:   10             rsi_window: 21     bb_threshold: 0.5\n",
      "  rsi_limits:  [30.0, 70.0]   sell_all:   True   qty_scale:    1.6\n",
      "  max_p_value:  0.1          \n",
      "==================================================================\n",
      "\n",
      "Coins included (2):\n",
      "=============================================\n",
      "   Bitcoin        Ethereum    \n",
      "\n",
      "Our Strategy:\n",
      "Results for Train:\n",
      "  Total Return:            1116.68%  Annualized Return: 100.51%\n",
      "  Annualized Sharpe Ratio: 1.01      Sortino Ratio:     0.89\n",
      "  Max Drawdown:            84.40%    Calmar Ratio:      1.19\n",
      "Results for Test:\n",
      "  Total Return:            207.09%   Annualized Return: 209.00%\n",
      "  Annualized Sharpe Ratio: 2.09      Sortino Ratio:     2.05\n",
      "  Max Drawdown:            72.97%    Calmar Ratio:      2.86\n",
      "\n",
      "Comparison Strategy:\n",
      "Results for Train:\n",
      "  Total Return:            1776.94%  Annualized Return: 126.23%\n",
      "  Annualized Sharpe Ratio: 1.26      Sortino Ratio:     1.78\n",
      "  Max Drawdown:            79.23%    Calmar Ratio:      1.59\n",
      "Results for Test:\n",
      "  Total Return:            141.39%   Annualized Return: 142.57%\n",
      "  Annualized Sharpe Ratio: 1.43      Sortino Ratio:     4.01\n",
      "  Max Drawdown:            44.47%    Calmar Ratio:      3.21\n"
     ]
    }
   ],
   "source": [
    "bestIdx = np.argmax(train_total_returns)\n",
    "bb_window, rsi_window, bb_threshold, rsi_limits, qty_scale, max_p_value = params[bestIdx]\n",
    "# print(f'The best hyper-parameters for the strategy out of the options available are: \\n  bb_window: {bb_window}\\n  rsi_window: {rsi_window}\\n  bb_threshold: {bb_threshold}\\n  rsi_limits: {rsi_limits}\\n  qty_scale: {qty_scale}\\n\\nThe best hyper-parameters yield the following returns:\\n  Train Period ({stratTrainDf.Date.iloc[0]} - {stratTrainDf.Date.iloc[-1]}): {train_results[bestIdx]}%\\n  Test Period  ({stratTestDf.Date.iloc[0]} - {stratTestDf.Date.iloc[-1]}): {test_results[bestIdx]:.2f}%')\n",
    "\n",
    "if os.path.exists('./trainLog.txt'):\n",
    "    os.remove('./trainLog.txt')\n",
    "    os.remove('./testLog.txt')\n",
    "\n",
    "coins2InvestDfs = [coinsDFs[i] for i, r in enumerate(np.array(p_values) < max_p_value) if r]\n",
    "\n",
    "coinsTrainDfs = []\n",
    "coinsTestDfs = []\n",
    "\n",
    "for i, df in enumerate(coins2InvestDfs):\n",
    "    df['temp_date'] = pd.to_datetime(df['Date'])\n",
    "    coinsTrainDfs.append(df[df['temp_date'] <= pd.to_datetime(cutoff)])\n",
    "    coinsTestDfs.append(df[df['temp_date'] > pd.to_datetime(cutoff)])\n",
    "    df.drop(columns = ['temp_date'], inplace = True)\n",
    "\n",
    "lags, p_values = do_granger(coinsTrainDfs.copy(), max_lags=max_lags, verbose=False)\n",
    "\n",
    "for j in range(len(coinsTrainDfs)):\n",
    "    coinsTrainDfs[j]['Normalized_Searches_delayed'] = coinsTrainDfs[j]['Normalized_Searches'].shift(lags[j])\n",
    "    coinsTrainDfs[j].dropna(inplace = True)\n",
    "    coinsTestDfs[j]['Normalized_Searches_delayed'] = coinsTestDfs[j]['Normalized_Searches'].shift(lags[j])\n",
    "    coinsTestDfs[j].dropna(inplace = True)\n",
    "\n",
    "include = [j for j, r in enumerate(np.array(p_values) < max_p_value)if r]\n",
    "coinsIncluded = [decentralized_currencies_names[j] for j in include]\n",
    "coinsTrainDfs = [coinsTrainDfs[i] for i in include]\n",
    "coinsTestDfs = [coinsTestDfs[i] for i in include]\n",
    "\n",
    "strat = Strategy(bb_window=bb_window, rsi_window=rsi_window, bb_window_min=bb_window_min, bb_threshold=bb_threshold, rsi_window_min=rsi_window_min, rsi_limits=rsi_limits, sell_all=sell_all, qty_scale=qty_scale, commission_type=commission_type, slippage_factor=slippage_factor)\n",
    "stratTrainDf = backtest(coinsTrainDfs.copy(), strat, balance, include, commission)\n",
    "stratTestDf = backtest(coinsTestDfs.copy(), strat, balance, include, commission)\n",
    "\n",
    "comparisonStrat = BuyAndHoldStrategy(sell_all=sell_all, qty_scale=1.0, commission_type=commission_type, slippage_factor=slippage_factor)\n",
    "stratComparisonTrainDf = backtest(coinsTrainDfs.copy(), comparisonStrat, balance, include, commission, hoddle=True)\n",
    "stratComparisonTestDf = backtest(coinsTestDfs.copy(), comparisonStrat, balance, include, commission, hoddle=True)\n",
    "\n",
    "print(f'Comparison over the total_return and annual return\\n\\n==================================================================\\nOur Strategy hyperparameters:\\n  bb_window:   {f'{bb_window}':13s}  rsi_window: {f'{rsi_window}':5s}  bb_threshold: {bb_threshold}\\n  rsi_limits:  {f'{rsi_limits}':13s}  sell_all:   {f'{sell_all}':5s}  qty_scale:    {qty_scale}\\n  max_p_value:  {f'{max_p_value}':13s}\\n==================================================================\\n')\n",
    "\n",
    "print(f'Coins included ({len(coinsIncluded)}):\\n=============================================')\n",
    "for j in range(len(coinsIncluded)):\n",
    "    print(f'  \\u2022 {coinsIncluded[j]:10s}', end = '\\n' if (j + 1) % 3 == 0 else '  ')\n",
    "\n",
    "if len(coinsIncluded) % 3 != 0:\n",
    "    print()\n",
    "\n",
    "print('\\nOur Strategy:')\n",
    "evaluate_strategy(stratTrainDf, 'Train', returns=False)\n",
    "evaluate_strategy(stratTestDf, 'Test', returns=False)\n",
    "print('\\nComparison Strategy:')\n",
    "evaluate_strategy(stratComparisonTrainDf, 'Train', returns=False)\n",
    "evaluate_strategy(stratComparisonTestDf, 'Test', returns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratTrainDf.to_csv('./strat_train_data.csv', index=False)\n",
    "stratTestDf.to_csv('./strat_test_data.csv', index=False)\n",
    "stratComparisonTrainDf.to_csv('./comparison_strat_train_data.csv', index=False)\n",
    "stratComparisonTestDf.to_csv('./comparison_strat_test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples for checks code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see data validity checks of a certain stock\n",
    "stock_name = 'Bitcoin' # change to any stock name that is in the list at the top\n",
    "stock_ticker = 'BTC' # change to the corresponding ticker of the stock\n",
    "\n",
    "getNormalizedData(stock_name, stock_ticker, \n",
    "                  start = start, end = end, \n",
    "                  do_double = True, verbose = True)\n",
    "\n",
    "# see the granger causality test results of a certain stock\n",
    "df = getNormalizedData(stock_name, stock_ticker, start = start, end = end, do_double = True)\n",
    "\n",
    "grangercausalitytests(df[['log_returns', 'log_searches']], \n",
    "                      maxlag = max_lags, \n",
    "                      verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previously used chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trend Data for ['Shiba Inu'] retrieved successfully.\n",
      "Stock Data for SHIB-USD retrieved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Calculate correlations for each category\n",
    "volt_del_corr_general = [trend_corr(stock)[0] for stock in general_stocks]\n",
    "volt_del_corr_tech = [trend_corr(stock)[0] for stock in tech_stocks]\n",
    "volt_del_corr_finance = [trend_corr(stock)[0] for stock in finance_stocks]\n",
    "volt_del_corr_crypto = [trend_corr(crypto)[0] for crypto in decentralized_currencies]\n",
    "\n",
    "# Combine the results\n",
    "volt_del_corr = volt_del_corr_general + volt_del_corr_tech + volt_del_corr_finance + volt_del_corr_crypto\n",
    "\n",
    "# Create labels for the scatter plot\n",
    "labels = general_stocks + tech_stocks + finance_stocks + decentralized_currencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scatter plot\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.scatter(range(len(general_stocks)), volt_del_corr_general, color = color_map['general'], label = 'General Stocks')\n",
    "for i in range(len(general_stocks)):\n",
    "    plt.axvline(x = i, color = color_map['general'], linestyle = ':', alpha = 0.3)  # Add vertical lines to separate the stocks\n",
    "plt.scatter(range(len(general_stocks), len(general_stocks) + len(tech_stocks)), volt_del_corr_tech, color = color_map['tech'], label = 'Tech Stocks')\n",
    "for i in range(len(general_stocks), len(general_stocks) + len(tech_stocks)):\n",
    "    plt.axvline(x = i, color = color_map['tech'], linestyle = ':', alpha = 0.3)  # Add vertical lines to separate the stocks\n",
    "plt.scatter(range(len(general_stocks) + len(tech_stocks), len(general_stocks) + len(tech_stocks) + len(finance_stocks)), volt_del_corr_finance, color = color_map['finance'], label = 'Finance Stocks')\n",
    "for i in range(len(general_stocks) + len(tech_stocks), len(general_stocks) + len(tech_stocks) + len(finance_stocks)):\n",
    "    plt.axvline(x = i, color = color_map['finance'], linestyle = ':', alpha = 0.3)  # Add vertical lines to separate the stocks\n",
    "plt.scatter(range(len(general_stocks) + len(tech_stocks) + len(finance_stocks), len(general_stocks) + len(tech_stocks) + len(finance_stocks) + len(decentralized_currencies)), volt_del_corr_crypto, color = color_map['crypto'], label = 'Decentralized Currencies')\n",
    "for i in range(len(general_stocks) + len(tech_stocks) + len(finance_stocks), len(general_stocks) + len(tech_stocks) + len(finance_stocks) + len(decentralized_currencies)):\n",
    "    plt.axvline(x = i, color = color_map['crypto'], linestyle = ':', alpha = 0.3)  # Add vertical lines to separate the stocks\n",
    "plt.axhline(y = 0, color = 'black', linestyle = '--')  # Add a horizontal line at y = 0\n",
    "plt.xlabel('Assets')\n",
    "plt.ylabel('Correlation with 7-Day Delayed Trend')\n",
    "plt.title('Correlation of Close Price and 7-Day Delayed Trend')\n",
    "legend = plt.legend()\n",
    "legend.get_frame().set_alpha(0.3)\n",
    "plt.xticks(range(len(labels)), labels, rotation = 60)\n",
    "plt.tight_layout(pad = 2)\n",
    "plt.savefig('Correlation_Scatter_Plot.png')\n",
    "plt.show()\n",
    "\n",
    "# Print the correlation values and their mean\n",
    "print(volt_del_corr, np.mean(volt_del_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for each stock and decentralized currency\n",
    "for stock in general_stocks + tech_stocks + finance_stocks + decentralized_currencies:\n",
    "    plot_stock_data(stock, download = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
