{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd   \n",
    "import matplotlib.pyplot as plt\n",
    "from pytrends.request import TrendReq\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import warnings as wrn\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_stocks = ['KO', 'PFE', 'WMT', 'PG', 'JNJ', 'DIS', 'PEP', 'MCD', 'T', 'VZ']\n",
    "tech_stocks = ['AAPL', 'AMZN', 'MSFT', 'GOOGL', 'NVDA', 'TSLA', 'META', 'INTC', 'IBM', 'AMD']\n",
    "finance_stocks = ['GS', 'BAC', 'WFC', 'USB', 'JPM', 'MA', 'V', 'AXP', 'C', 'BLK']\n",
    "decentralized_currencies = ['BTC', 'ETH', 'ADA', 'SOL', 'XRP', 'XMR', 'LTC', 'DOT', 'LINK', 'XTZ']\n",
    "\n",
    "general_stocks_names = ['Coca-Cola', 'Pfizer', 'Walmart', 'Procter & Gamble', 'Johnson & Johnson', 'Disney', 'Pepsi', 'McDonalds', 'AT&T', 'Verizon']\n",
    "tech_stocks_names = ['Apple', 'Amazon', 'Microsoft', 'Google', 'Nvidia', 'Tesla', 'Meta', 'Intel', 'IBM', 'AMD']\n",
    "finance_stocks_names = ['Goldman Sachs', 'Bank of America', 'Wells Fargo', 'US Bancorp', 'JPMorgan Chase', 'Mastercard', 'Visa', 'American Express', 'Citigroup', 'BlackRock']\n",
    "decentralized_currencies_names = ['Bitcoin', 'Ethereum', 'Cardano', 'Solana', 'Ripple', 'Monero', 'Litecoin', 'Polkadot', 'Chainlink', 'Tezos']\n",
    "\n",
    "color_map = {\n",
    "    'general': 'deepskyblue',\n",
    "    'tech': 'limegreen',\n",
    "    'finance': 'darkorchid',\n",
    "    'crypto': 'red'\n",
    "}\n",
    "\n",
    "start = '2019-06-30'\n",
    "end = '2024-07-01'\n",
    "stock = 'Bitcoin'\n",
    "ticker = 'BTC'\n",
    "max_lags = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "## get_trends_data\n",
    "gets the trend data using pytrends, given a certain timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trends_data(keyword, \n",
    "                    timeframe=datetime.date.today().strftime('%Y-%m-%d') + ' ' + (datetime.date.today() - datetime.timedelta(days = 269)).strftime('%Y-%m-%d'),\n",
    "                    retries=5, \n",
    "                    backoff_factor=1.0,\n",
    "                    verbose=True):\n",
    "    pytrends = TrendReq(hl='en-US', tz=360, timeout=(10,25), )\n",
    "    pytrends.build_payload(keyword, cat = 0, timeframe = timeframe, geo='')\n",
    "    \n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            df = pytrends.interest_over_time()\n",
    "            if df is not None and not df.empty:\n",
    "                if verbose:\n",
    "                    print(f\"Trend Data for {keyword[0]} at timeframe {timeframe} retrieved successfully.\")\n",
    "                df.reset_index(inplace = True)\n",
    "                df.rename(columns = {'date': 'Date', keyword[0]: 'Trend'}, inplace = True)\n",
    "                df['Date'] = pd.to_datetime(df['Date'].dt.strftime('%m/%d/%Y'))\n",
    "                return df\n",
    "            else:\n",
    "                print(\"No data retrieved or DataFrame is empty.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            if \"429\" in str(e):\n",
    "                sleep_time = backoff_factor * (2 ** i)\n",
    "                if verbose:\n",
    "                    print(f\"Rate limit exceeded. Retrying in {sleep_time} seconds...\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                raise(f\"An error occurred: {e}\")\n",
    "    print(\"Failed to retrieve data after several retries.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_stock_data\n",
    "gets the prices of a certain stock in a certain timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(ticker, start, end, verbose = True):\n",
    "    currTicker = yf.Ticker(ticker)\n",
    "    tickerDF = currTicker.history(repair = True, start = start, end = end, auto_adjust = False).drop(columns = ['Dividends', 'Stock Splits', 'Repaired?']).reset_index()\n",
    "    if verbose:\n",
    "        print(f\"Stock Data for {ticker} retrieved successfully.\")\n",
    "    tickerDF['Date'] = pd.to_datetime(tickerDF['Date'].dt.strftime('%m/%d/%Y'))\n",
    "    return tickerDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trend_corr\n",
    "gets the data of the trends and prices of the stock given to it in a certain timeframe and calculates the correlation between the log_returns and the trends delayed by certain delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_corr(stock, days = 60, start = '2023-10-01', end = '2024-06-01', delay = 7):\n",
    "    if not os.path.exists(f\"./Data/{stock}_trends({start} - {end}).csv\"):\n",
    "        if stock in general_stocks:\n",
    "            name = general_stocks_names[general_stocks.index(stock)]\n",
    "        elif stock in tech_stocks:\n",
    "            name = tech_stocks_names[tech_stocks.index(stock)]\n",
    "        elif stock in finance_stocks:\n",
    "            name = finance_stocks_names[finance_stocks.index(stock)]\n",
    "        else:\n",
    "            name = decentralized_currencies_names[decentralized_currencies.index(stock)]\n",
    "        t = get_trends_data([name], timeframe = f\"{start} {end}\")\n",
    "        if t is None:\n",
    "            raise Exception(f'Failed to retrieve Trend Data of {stock}.')\n",
    "        t.to_csv(f\"./Data/{stock}_trends({start} - {end}).csv\")\n",
    "    else:\n",
    "        t = pd.read_csv(f\"./Data/{stock}_trends({start} - {end}).csv\")\n",
    "    if not os.path.exists(f\"./Data/{stock}_Prices({start} - {end}).csv\"):\n",
    "        if stock in decentralized_currencies:\n",
    "            p = get_stock_data(f'{stock}-USD', start = start, end = end)\n",
    "        else:\n",
    "            p = get_stock_data(stock, start = start, end = end)\n",
    "        p.to_csv(f\"./Data/{stock}_Prices({start} - {end}).csv\")\n",
    "    else:\n",
    "        p = pd.read_csv(f\"./Data/{stock}_Prices({start} - {end}).csv\")\n",
    "\n",
    "    t['Date'] = pd.to_datetime(t['Date'])\n",
    "    p['Date'] = pd.to_datetime(p['Date'])\n",
    "\n",
    "    full_data = pd.merge(p, t, on='Date')\n",
    "\n",
    "    full_data['log_returns'] = np.log(full_data.Close / full_data.Close.shift(1))\n",
    "    full_data['Volatility'] = full_data['log_returns'].rolling(window=days).std() * np.sqrt(days)\n",
    "\n",
    "    for i in range(1, 8):\n",
    "        full_data[f'Delay_{i}'] = full_data['Trend'].shift(i)\n",
    "\n",
    "    rho = full_data.corr()\n",
    "    rho_c = rho['Close'][f'Delay_{delay}']\n",
    "    return rho_c, full_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot_stock_data\n",
    "gets the data of the trends and the prices of the stock given to it in a certain timeframe and plots its close and its delayed trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stock_data(stock, days = 60, start = '2023-10-01', end = '2024-06-01', delay = 7, download = False):\n",
    "    if not os.path.exists(f\"./Data/{stock}_trends({start} - {end}).csv\"):\n",
    "        if stock in general_stocks:\n",
    "            name = general_stocks_names[general_stocks.index(stock)]\n",
    "        elif stock in tech_stocks:\n",
    "            name = tech_stocks_names[tech_stocks.index(stock)]\n",
    "        elif stock in finance_stocks:\n",
    "            name = finance_stocks_names[finance_stocks.index(stock)]\n",
    "        else:\n",
    "            name = decentralized_currencies_names[decentralized_currencies.index(stock)]\n",
    "        t = get_trends_data([name], timeframe = f\"{start} {end}\")\n",
    "        t.to_csv(f\"./Data/{stock}_trends({start} - {end}).csv\")\n",
    "    else:\n",
    "        t = pd.read_csv(f\"./Data/{stock}_trends({start} - {end}).csv\")\n",
    "    if not os.path.exists(f\"./Data/{stock}_Prices({start} - {end}).csv\"):\n",
    "        p = get_stock_data(stock, start = start, end = end)\n",
    "        p.to_csv(f\"./Data/{stock}_Prices({start} - {end}).csv\")\n",
    "    else:\n",
    "        p = pd.read_csv(f\"./Data/{stock}_Prices({start} - {end}).csv\")\n",
    "\n",
    "    t['Date'] = pd.to_datetime(t['Date'])\n",
    "    p['Date'] = pd.to_datetime(p['Date'])\n",
    "\n",
    "    full_data = pd.merge(p, t, on='Date')\n",
    "\n",
    "    full_data['log_returns'] = np.log(full_data.Close / full_data.Close.shift(1))\n",
    "    full_data['Volatility'] = full_data['log_returns'].rolling(window=days).std() * np.sqrt(days)\n",
    "\n",
    "    full_data[f'Delay_{delay}'] = full_data.Trend.shift(7)\n",
    "\n",
    "    # Determine the color based on the stock category\n",
    "    if stock in general_stocks:\n",
    "        color = color_map['general']\n",
    "        name = general_stocks_names[general_stocks.index(stock)]\n",
    "    elif stock in tech_stocks:\n",
    "        color = color_map['tech']\n",
    "        name = tech_stocks_names[tech_stocks.index(stock)]\n",
    "    elif stock in finance_stocks:\n",
    "        color = color_map['finance']\n",
    "        name = finance_stocks_names[finance_stocks.index(stock)]\n",
    "    else:\n",
    "        color = color_map['crypto']\n",
    "        name = decentralized_currencies_names[decentralized_currencies.index(stock)]\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "    # Plot Close price\n",
    "    axes[0].plot(full_data['Date'], full_data['Close'], label = 'Close Price', color = color)\n",
    "    axes[0].set_xlabel('Date')\n",
    "    axes[0].set_ylabel('Close Price')\n",
    "    axes[0].set_title(f'{stock}: Close Price')\n",
    "    legend = axes[0].legend(loc='upper left')\n",
    "    legend.get_frame().set_alpha(0.3)\n",
    "\n",
    "    # Plot 7-days delay trend\n",
    "    axes[1].plot(full_data['Date'], full_data[f'Delay_{delay}'], label = f'{delay}-Days Delayed Trend', color = 'black')\n",
    "    axes[1].set_xlabel('Date')\n",
    "    axes[1].set_ylabel(f'{delay}-Days Delayed Trend')\n",
    "    axes[1].set_title(f'{stock}: {delay}-Days Delayed Trend')\n",
    "    legend = axes[1].legend(loc='upper right')\n",
    "    legend.get_frame().set_alpha(0.3)\n",
    "\n",
    "    fig.suptitle(f'{name} ({stock})', fontsize=20, verticalalignment = 'bottom', fontweight = 'bold')\n",
    "    plt.tight_layout(pad=2.0)\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    if download:\n",
    "        plt.savefig(f\"./Plots/{stock}_plot({start} - {end}).png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time_jump\n",
    "adds time in days to a given date that was accepted as string, returns as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the date in string format after a certain number of days\n",
    "def time_jump(start, days = 7 * 38):\n",
    "    return (datetime.datetime.strptime(start, '%Y-%m-%d') + datetime.timedelta(days = days)).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_breakpoints\n",
    "calculates the breakpoints needed for the multiple requests of data to make the data as long as possible for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_breakpoints(start, end, days = 7 * 38):\n",
    "    breakpoints = [start]\n",
    "    while datetime.datetime.strptime(breakpoints[-1], '%Y-%m-%d') < datetime.datetime.strptime(end, '%Y-%m-%d'):\n",
    "        temp = time_jump(breakpoints[-1], days)\n",
    "        if datetime.datetime.strptime(temp, '%Y-%m-%d') < datetime.datetime.strptime(end, '%Y-%m-%d'):\n",
    "            breakpoints.append(temp)\n",
    "        else:\n",
    "            breakpoints.append(end)\n",
    "    return breakpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## connectNnormalizeTrends\n",
    "gets the data of the trends and prices daily in the whole time frame and in parts, and connects them by estimating an eproximation of their absolute amount of searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connectNnormalizeTrends(Dfs, stock):\n",
    "    # setting up the data for step 1\n",
    "    if not os.path.exists(f'./Data/glimpse_{stock}_5Y.csv'):\n",
    "        raise Exception(f'Failed to retrieve Glimpse Data of {stock}.')\n",
    "    glmpsDf = pd.read_csv(f'./Data/glimpse_{stock}_5Y.csv')\n",
    "    glmpsDf.rename(columns={'Time (week of)': 'Date', 'Absolute Google Search Volume': 'Absolute_Volume'}, inplace=True)\n",
    "    glmpsDf['Date'] = pd.to_datetime(glmpsDf['Date'])\n",
    "\n",
    "    df_concat = pd.concat(Dfs).reset_index(drop = True)\n",
    "    df_concat['Date'] = pd.to_datetime(df_concat['Date'])\n",
    "\n",
    "    # calculating the mean trend for each week and merging it into glmpsDf\n",
    "    df_concat['MeanTrend'] = df_concat['Trend'].rolling(window=7, min_periods=1).mean().shift(-6)\n",
    "    glmpsDf = pd.merge(glmpsDf, df_concat, on='Date', how='left').drop(columns = ['Trend'])\n",
    "\n",
    "    # setting up the data for step 2\n",
    "    glmpsDf.rename(columns={'Date': 'Date_Week'}, inplace=True)\n",
    "    df_concat = df_concat.drop(columns = ['MeanTrend'])\n",
    "    df_concat['Date_Week'] = (df_concat['Date'] - pd.to_timedelta((df_concat['Date'].dt.weekday + 1) % 7, unit='d')).dt.strftime('%Y-%m-%d')\n",
    "    df_concat['Date_Week'] = pd.to_datetime(df_concat['Date_Week'])\n",
    "\n",
    "    # calculating the ratio and search volume for each week\n",
    "    df_concat = pd.merge(df_concat, glmpsDf[['Date_Week', 'MeanTrend', 'Absolute_Volume']], on='Date_Week', how='left')\n",
    "    df_concat['Ratio'] = df_concat['Trend'] / (df_concat['MeanTrend'] * 7)\n",
    "    df_concat['Search_Volume'] = df_concat['Ratio'] * df_concat['Absolute_Volume']\n",
    "\n",
    "    # adding to df_concat the check ratio to check validity of the data\n",
    "    df_concat['check_ratio'] = df_concat['Search_Volume'] / df_concat['Trend']\n",
    "\n",
    "    # renormalizing the data\n",
    "    df_concat['Normalized_Searches'] = ((df_concat['Search_Volume'] - df_concat['Search_Volume'].min()) / (df_concat['Search_Volume'].max() - df_concat['Search_Volume'].min())) * 100\n",
    "\n",
    "    # cleaning out unnecessary columns\n",
    "    df_concat = df_concat.drop(columns = ['Trend', 'Date_Week', 'MeanTrend', 'Absolute_Volume', 'Ratio'])\n",
    "\n",
    "    df_concat['Date'] = df_concat['Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getNormalizedData\n",
    "gets the data normalized, and performs validity checks (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNormalizedData(stock, ticker, start = '2019-06-23', end = '2024-06-01', weeks = 38, do_double = True, verbose = False):\n",
    "    if weeks > 38:\n",
    "        wrn.warn('The maximum number of weeks is 38. \\nThe number of weeks will be set to 38.', category=Warning)\n",
    "        weeks = 38\n",
    "    if do_double & (weeks % 2 != 0):\n",
    "        wrn.warn('The number of weeks must be even to use the double method. \\nThe number of weeks will be rounded down to the nearest even number.', category=Warning)\n",
    "        weeks -= 1\n",
    "    breakpoints = get_breakpoints(start, end, days = 7 * weeks)\n",
    "    if do_double:\n",
    "        breakpoints2 = get_breakpoints(start, end, days = 7 * weeks / 2)\n",
    "        breakpoints2 = [x for x in breakpoints2 if x not in breakpoints[1:-1]]\n",
    "    \n",
    "    # initializing the list to store the dataframes\n",
    "    Dfs = []\n",
    "\n",
    "    # extracting the data for each time period\n",
    "    for i in range(len(breakpoints) - 1):\n",
    "        startTemp = breakpoints[i]\n",
    "        endTemp = time_jump(breakpoints[i + 1], days=-1)\n",
    "        # checking if the data is already extracted\n",
    "        if not os.path.exists(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\"):\n",
    "            # extracting the data\n",
    "            t = get_trends_data([stock], timeframe = f\"{startTemp} {endTemp}\", verbose = verbose).drop(columns = ['isPartial'])\n",
    "            if t is None:\n",
    "                raise Exception(f'Failed to retrieve Trend Data of {stock}.')\n",
    "            t.to_csv(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\")\n",
    "        else:\n",
    "            t = pd.read_csv(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\").drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "        Dfs.append(t)\n",
    "\n",
    "    df_concat = connectNnormalizeTrends(Dfs, stock)\n",
    "\n",
    "    if do_double:\n",
    "        # initializing the list to store the dataframes\n",
    "        Dfs2 = []\n",
    "\n",
    "        # extracting the data for each time period\n",
    "        for i in range(len(breakpoints2) - 1):\n",
    "            startTemp = breakpoints2[i]\n",
    "            endTemp = time_jump(breakpoints2[i + 1], days=-1)\n",
    "            # checking if the data is already extracted\n",
    "            if not os.path.exists(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\"):\n",
    "                # extracting the data\n",
    "                t = get_trends_data([stock], timeframe = f\"{startTemp} {endTemp}\", verbose = verbose).drop(columns = ['isPartial'])\n",
    "                if t is None:\n",
    "                    raise Exception(f'Failed to retrieve Trend Data of {stock}.')\n",
    "                t.to_csv(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\")\n",
    "            else:\n",
    "                t = pd.read_csv(f\"./Data/{stock}_trends({startTemp} - {endTemp}).csv\").drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "            Dfs2.append(t)\n",
    "        \n",
    "        df_concat2 = connectNnormalizeTrends(Dfs2, stock)\n",
    "    \n",
    "        df_concat[\"Normalized_Searches\"] = (df_concat[\"Normalized_Searches\"] + df_concat2[\"Normalized_Searches\"]) / 2\n",
    "    \n",
    "    if not os.path.exists(f\"./Data/{stock}_Prices({start}-{end}).csv\"):\n",
    "        stockData = get_stock_data(f'{ticker}-USD', start = start, end = end, verbose = verbose)\n",
    "        stockData.to_csv(f\"./Data/{stock}_Prices({start}-{end}).csv\")\n",
    "    else:\n",
    "        stockData = pd.read_csv(f\"./Data/{stock}_Prices({start}-{end}).csv\").drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "    df_concat[df_concat.Normalized_Searches == 0] = 0.1\n",
    "\n",
    "    df_concat['log_searches'] = np.log(df_concat.Normalized_Searches / df_concat.Normalized_Searches.shift(1))\n",
    "    stockData['log_returns'] = np.log(stockData.Close / stockData.Close.shift(1))\n",
    "\n",
    "    try:\n",
    "        df_concat['Date'] = df_concat['Date'].dt.strftime('%Y-%m-%d')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        stockData['Date'] = stockData['Date'].dt.strftime('%Y-%m-%d')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    finalDf = pd.merge(stockData, df_concat, on='Date', how='left').dropna()\n",
    "\n",
    "    if verbose:\n",
    "        # data normalization validity check\n",
    "        top = 0\n",
    "        bottom = 0\n",
    "        for i in range(len(Dfs)):\n",
    "            top += Dfs[i].shape[0]\n",
    "            print(f'period {i + 1}:\\n=========\\nmean: {df_concat.iloc[bottom:top]['check_ratio'].mean():.4f}\\nsd: {df_concat.iloc[bottom:top][\"check_ratio\"].std():.4f}\\n\\nmean/sd: {df_concat.iloc[bottom:top]['check_ratio'].mean()/df_concat.iloc[bottom:top][\"check_ratio\"].std():.4f}\\n')\n",
    "            bottom += Dfs[i].shape[0]\n",
    "\n",
    "        if do_double:\n",
    "            top = 0\n",
    "            bottom = 0\n",
    "            for i in range(len(Dfs2)):\n",
    "                top += Dfs2[i].shape[0]\n",
    "                print(f'period {i + len(Dfs) + 1}:\\n=========\\nmean: {df_concat.iloc[bottom:top]['check_ratio'].mean():.4f}\\nsd: {df_concat.iloc[bottom:top][\"check_ratio\"].std():.4f}\\n\\nmean/sd: {df_concat.iloc[bottom:top]['check_ratio'].mean()/df_concat.iloc[bottom:top][\"check_ratio\"].std():.4f}\\n')\n",
    "                bottom += Dfs2[i].shape[0]\n",
    "    return finalDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitcoin: 21 lags, p-value = 0.0466\n",
      "Ethereum: 1 lags, p-value = 0.1807\n",
      "Cardano: 2 lags, p-value = 0.0776\n",
      "Solana: 57 lags, p-value = 0.0712\n",
      "Ripple: 60 lags, p-value = 0.1876\n",
      "Monero: 1 lags, p-value = 0.1025\n",
      "Litecoin: 2 lags, p-value = 0.2933\n",
      "Polkadot: 3 lags, p-value = 0.0056\n",
      "Chainlink: 24 lags, p-value = 0.0076\n",
      "Tezos: 3 lags, p-value = 0.4993\n"
     ]
    }
   ],
   "source": [
    "wrn.filterwarnings('ignore', category=UserWarning)\n",
    "wrn.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "coinsDFs = []\n",
    "lags = []\n",
    "\n",
    "for i in range(len(decentralized_currencies)):\n",
    "    coinsDFs.append(getNormalizedData(decentralized_currencies_names[i], decentralized_currencies[i], \n",
    "                                      start = start, end = end, \n",
    "                                      do_double = True))\n",
    "    cause = grangercausalitytests(coinsDFs[-1][['log_returns', 'log_searches']], \n",
    "                                  maxlag = max_lags, \n",
    "                                  verbose = False)\n",
    "\n",
    "    min_p_value = float('inf')\n",
    "    min_p_lag = None\n",
    "\n",
    "    for lag, result in cause.items():\n",
    "        p_value = result[0]['ssr_ftest'][1]\n",
    "        if p_value < min_p_value:\n",
    "            min_p_value = p_value\n",
    "            min_p_lag = lag\n",
    "\n",
    "    lags.append(min_p_lag)\n",
    "\n",
    "    print(f'{decentralized_currencies_names[i]}: {min_p_lag} lags, p-value = {min_p_value:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples for checks code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see data validity checks of a certain stock\n",
    "stock_name = 'Bitcoin' # change to any stock name that is in the list at the top\n",
    "stock_ticker = 'BTC' # change to the corresponding ticker of the stock\n",
    "\n",
    "getNormalizedData(stock_name, stock_ticker, \n",
    "                  start = start, end = end, \n",
    "                  do_double = True, verbose = True)\n",
    "\n",
    "# see the granger causality test results of a certain stock\n",
    "df = getNormalizedData(stock_name, stock_ticker, start = start, end = end, do_double = True)\n",
    "\n",
    "grangercausalitytests(df[['log_returns', 'log_searches']], \n",
    "                      maxlag = max_lags, \n",
    "                      verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previously used chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trend Data for ['Shiba Inu'] retrieved successfully.\n",
      "Stock Data for SHIB-USD retrieved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Calculate correlations for each category\n",
    "volt_del_corr_general = [trend_corr(stock)[0] for stock in general_stocks]\n",
    "volt_del_corr_tech = [trend_corr(stock)[0] for stock in tech_stocks]\n",
    "volt_del_corr_finance = [trend_corr(stock)[0] for stock in finance_stocks]\n",
    "volt_del_corr_crypto = [trend_corr(crypto)[0] for crypto in decentralized_currencies]\n",
    "\n",
    "# Combine the results\n",
    "volt_del_corr = volt_del_corr_general + volt_del_corr_tech + volt_del_corr_finance + volt_del_corr_crypto\n",
    "\n",
    "# Create labels for the scatter plot\n",
    "labels = general_stocks + tech_stocks + finance_stocks + decentralized_currencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scatter plot\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.scatter(range(len(general_stocks)), volt_del_corr_general, color = color_map['general'], label = 'General Stocks')\n",
    "for i in range(len(general_stocks)):\n",
    "    plt.axvline(x = i, color = color_map['general'], linestyle = ':', alpha = 0.3)  # Add vertical lines to separate the stocks\n",
    "plt.scatter(range(len(general_stocks), len(general_stocks) + len(tech_stocks)), volt_del_corr_tech, color = color_map['tech'], label = 'Tech Stocks')\n",
    "for i in range(len(general_stocks), len(general_stocks) + len(tech_stocks)):\n",
    "    plt.axvline(x = i, color = color_map['tech'], linestyle = ':', alpha = 0.3)  # Add vertical lines to separate the stocks\n",
    "plt.scatter(range(len(general_stocks) + len(tech_stocks), len(general_stocks) + len(tech_stocks) + len(finance_stocks)), volt_del_corr_finance, color = color_map['finance'], label = 'Finance Stocks')\n",
    "for i in range(len(general_stocks) + len(tech_stocks), len(general_stocks) + len(tech_stocks) + len(finance_stocks)):\n",
    "    plt.axvline(x = i, color = color_map['finance'], linestyle = ':', alpha = 0.3)  # Add vertical lines to separate the stocks\n",
    "plt.scatter(range(len(general_stocks) + len(tech_stocks) + len(finance_stocks), len(general_stocks) + len(tech_stocks) + len(finance_stocks) + len(decentralized_currencies)), volt_del_corr_crypto, color = color_map['crypto'], label = 'Decentralized Currencies')\n",
    "for i in range(len(general_stocks) + len(tech_stocks) + len(finance_stocks), len(general_stocks) + len(tech_stocks) + len(finance_stocks) + len(decentralized_currencies)):\n",
    "    plt.axvline(x = i, color = color_map['crypto'], linestyle = ':', alpha = 0.3)  # Add vertical lines to separate the stocks\n",
    "plt.axhline(y = 0, color = 'black', linestyle = '--')  # Add a horizontal line at y = 0\n",
    "plt.xlabel('Assets')\n",
    "plt.ylabel('Correlation with 7-Day Delayed Trend')\n",
    "plt.title('Correlation of Close Price and 7-Day Delayed Trend')\n",
    "legend = plt.legend()\n",
    "legend.get_frame().set_alpha(0.3)\n",
    "plt.xticks(range(len(labels)), labels, rotation = 60)\n",
    "plt.tight_layout(pad = 2)\n",
    "plt.savefig('Correlation_Scatter_Plot.png')\n",
    "plt.show()\n",
    "\n",
    "# Print the correlation values and their mean\n",
    "print(volt_del_corr, np.mean(volt_del_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for each stock and decentralized currency\n",
    "for stock in general_stocks + tech_stocks + finance_stocks + decentralized_currencies:\n",
    "    plot_stock_data(stock, download = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
